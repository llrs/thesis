# Materials and methods

Some sections of these methods has been already published and copied or adapted to the thesis from [@revilla2021].

## Sample collection

#### Ethical considerations

All samples were collected with approval from the Ethical Committee and under express authorization of the patients.
The minimum number of samples needed for the study were collected.
Data is pseudonymisated to prevent identification of the patients.

### HSCT dataset

<!--COPIED!!! from article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0246367#sec005 -->

Samples from the HSCT dataset used in this thesis were from a cohort of patients with severe refractory CD undergoing hematopoietic stem cell transplant.
Patients were treated in the Department of Gastroenterology (Hospital Clínic de Barcelona --Spain--).
The protocol was approved by the Catalan Transplantation Organization and by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number 2012/7244).
All patients provided written consent following extensive counselling.
Colonic and ileal biopsies were obtained at several time points during ileo-colonoscopy.
Patients were followed-up for 4 years and biopsies were collected every six or twelve months after HSCT.
Samples were obtained whenever possible from both uninvolved and involved areas.
In addition, biopsies were taken from the ileum and colon regions of 19 non-IBD controls consisting of individuals with no history of IBD and who presented no significant pathological findings following endoscopic examination for colon cancer surveillance (Hospital Univesitari Mútua de Terrassa--Spain--).
The protocol was approved by the Institutional Ethics Committee of the Hospital Univesitari Mútua de Terrassa (Study Number NA1651).
At least one biopsy was collected and fresh-frozen at -80°C for microbial DNA extraction.
The remaining biopsies were placed in RNAlater RNA Stabilization Reagent (Qiagen, Hilde, Germany) and stored at -80°C until total RNA extraction.

#### Transcriptome sequencing

Total RNA from mucosal samples (HSCT cohort) was isolated using the RNeasy kit (Qiagen, Hilde, Germany).
RNA sequencing libraries were prepared for paired-end sequencing using HighSeq-4000 platform.
Later, cutadapt (version 1.7.1) [@martin2011] was used for quality filtering and the libraries were mapped against the human reference genome using the STAR aligner (2.5.2a) with Ensembl annotation (release GRCh38.10) [@dobin2013].
Read counts per gene were obtained with RSEM (version 1.2.31) [@li2011] as previously described [@corralizaDifferencesPeripheralTissue].
Data was normalized using the trimmed mean of M-values and log transformed into counts per millions using edgeR (version 3.28) [@mccarthy2012].

#### Microbial DNA extraction from mucosal samples

Biopsies from the HSCT CD cohort were resuspended in 180 μl TET (TrisHCl 0.02M, EDTA 0.002M, Triton 1X) buffer and 20mg/ml lysozyme (Carl Roth, Quimivita, S.A.). Samples were incubated for 1h at 37°C and vortexed with 25 μl Proteinase K before incubating at 56°C for 3h.
Buffer B3 (NucleoSpin Tissue Kit--Macherey-Nagel) was added followed by a heat treatment for 10 min at 70°C.
After adding 100% ethanol, samples were centrifuged at 11000 x g for 1 min.
Two washing steps were performed before eluting DNA.
Concentrations and purity were checked using NanoDrop One (Thermo Fisher Scientific).
Samples were immediately used or placed at -20°C for long-term storage.

#### DNA sequencing

Library preparation and sequencing were performed at the Technische Universität München.
Briefly, volumes of 600μL DNA stabilization solution (STRATEC biomedical) and 400μL Phenol:choloform:isoamyl alcohol (25:24:1, Sigma-Aldrich) were added to the aliquots.
Microbial cells were disrupted by mechanical lysis using FastPrep-24.
Heat treatment and centrifugation were conducted after adding a cooling adaptor.
Supernatants were treated with RNase to eliminate RNA.
Total DNA was purified using gDNA columns as described in detail previously [@berry2011].
Briefly, the V3-V4 regions of 16S rRNA gene were amplified (15x15 cycles) following a previously described two-step protocol [@klindworth2013] using forward and reverse primers 341F-785R [@edgar2013a].
Purification of amplicons was performed by using the AMPure XP system (Beckmann).
Next, sequencing was performed with pooled samples in paired-end modus (PE275) using an MiSeq system (Illumina, Inc.) according to the manufacturer's instructions and 25% (v/v) PhiX standard library.

#### Microbial profiling

Processing of raw-reads was performed by using the IMNGS (version 1.0 Build 2007) [@lagkouvardos2016] pipeline based on the UPARSE approach [@edgar2013a] .
Sequences were demultiplexed, trimmed to the first base with a quality score \<3 and then paired.
Sequences with less than 300 and more than 600 nucleotides and paired reads with an expected error \>3 were excluded from the analysis.
Trimming of the remaining reads was done by trimming 5 nucleotides from each end to avoid GC bias and non-random base composition.
Operational taxonomic units (OTUs) were clustered at 97% sequence similarity.
Taxonomy assignment was performed at 80% confidence level using the RDP classifier and the SILVA ribosomal RNA gene database project.
Later the data was normalized using the same method as for RNA-seq described above.
The microbiome was visually inspected for batch effects in PCA; none were found.
The resulting OTUs table was normalized using edgeR (Version 3.28) [@mccarthy2012].

Another method used to analyze microbiome data is dada2 [@callahan2016] which creates amplicon sequencing variants from the 16S sequencing data.
Together with Qiime2 [@bolyen2019] it is an alternative to OTUs which allows to be able to compare results between studies and provides more resolution.

<!--Finished the copied section-->

## Statistics

A false discovery rate (FDR) correction for multiple testing was performed when necessary following the method of Benjamini, Hochberg and Yekutieli.
FDR values below 0.05 were considered significant.
If not considered necessary the alpha level of 0.05 was used.

Besides correcting for multiple testing several methods were used on this thesis of different types.
Functional enrichment related methods stand out from the other methods used.

### Functional enrichment

Functional enrichment methods are those methods that aim to provide with more information about the variables besides their numerical value measured.
They can be very different in nature but they all use the numeric values of the variables and other information, being it from the same experiment data collection or from external data sources.

To test if some variables show an unexpected importance according to a statistic like fold change or value fgsea [@korotkevich2021] methods was used.
fgsea is a software used to see if groups of variables present in an ordered list present some skewed distribution.
Pathways of genes were tested on the weight of different models.

GSVA is a method related to fgsea that summarize the variables' numerical value so that they can be later used by other methods [@hänzelmann2013].
GSVA was used to estimate the expression of the pathways and try to find the relationships between the pathways and the microbiome at different taxonomic levels.

clusterProfiler: test genes enrichment for functionality based on information on pathway databases [@wu2021].
It uses several methods like fisher test to check the enrichment of features of a given group on the list provided.

### Other methods used

vegan package provides the adonis method used to test if microbiome data variance is due to other variables [@oksanen2020].
This allows to test if the variables are related to the variance of the data.
This is particularly important on the microbiome data, which is highly variable.
It was used to detect which variables were important to include in the models of the disease.

globaltest is a package with methods for testing complex hypothesis and help decide if the variables were influencing [@goeman2006].
It provides a general test statistic to test a hypothesis against a high dimensional dataset.
It was used to test which variables, (sex, age, location, time since diagnostic) are important on the datasets.

Microbiome diversity was measured using vegan and phyloseq methods [@oksanen2020].
$\alpha$-diversity is a measure of how much a given microbiome at a taxonomic level is present on a sample.
Several measures exists, on the thesis I used the effective Simpson or Shannon diversity index to compare diversity between samples and conditions.
$\beta$-diversity was calculated using the phyloseq package for exploratory analysis.

We looked to see if using some co-expression measure between the the microbiome and the RNAseq would help identify relationships.
We used weighted gene co-expression network analysis as implemented on WGCNA [@langfelder2008].

Multiple co-inertia analysis, also known as MCIA, is a method to examine covariant gene expression patterns between two data sources, simple integration, two blocks [@meng2014].
It was used as a baseline method to find relationships between microorganisms and genes.

STATegRa, is a framework for integrating datasets with two data types using parametric and non-parametric methods [@planell2021].
It was used to explore the datasets and see what do they had in common.

BaseSet is a package that implements fuzzy set logic for easy calculation of probability to belong to a group.
It can be used to calculate the size of the sets given a probability of having each element on the sets.
When evaluating a model using bootstrap, it was used to find which variables were really involved on the interaction and how likely were to be together.

Regularized generalized canonical correlation analysis, RGCCA, has been the main method used [@tenenhaus2017a].
It will be explained in detail on the next section.

## Regularized generalized canonical correlation analysis

The canonical correlation is a method that uses data from the same sample but from different datasets.
The regularized generalized canonical correlation analysis is an extension to the canonical correlation analysis [@jordan1875; @hotelling1936] which includes a regularization step to select which are the variables more relevant.

### Description

Over several years of progress [@tenenhaus_component-based_2008; @esposito_vinzi_bridge_2010; @tenenhaus_regularized_2011; @tenenhaus_regularized_2014; @tenenhaus_variable_2014; @tenenhaus_kernel_2015] on the field of canonical correlations RGCCA is a robust method with a usable implementation [@R-RGCCA].

The required data is a numeric matrix, as big as you want, as it is designed for datasets with more variables than samples ($n\gg p$).
It needs to have a complete case with no missing values and time is not considered as a specially variable.
Data should be from the same samples and it is not specific to a certain technology or method used to obtain the data.

It uses a dimensional reduction approach to relate the different data sets between them and produce specific factors for each dataset.
The interpretation of the RGCCA results are tricky and require expertise and familiarity with the technique used, but is highly dependent on the model and options used.

There have been some improvements to generalize the method when $p \gg n$.
Current practices include using a pre selected model of relations between blocks.
However this model might not be accurate and several models might need to be fitted.
To help find the fitting model for the data I created an R package, named [inteRmodel](https://llrs.github.io/inteRmodel/), which helps finding the right model and how fit it is for your data.

This method applied to an existing dataset of an autologous haematopoietic stem cell transplantation [@corralizaDifferencesPeripheralTissue].
From this dataset there is data about the human transcriptome and the 16S DNA present at biopsies from colonoscopy.

### Relationships' models

Most multi-omics and integration tools assume one block for each type of data.
However, RGCCA uses linear relationships between blocks of data.
But there is no formal definition of what constitutes a block of data.
Usually a block of data is just a source of data, such as an essay a survey or an experiment.
We decided to split the block with data about the samples to separate independent variables from the same block.
The hypothesis we made was that more blocks with highly related variables but independent from the other blocks would fit better the data.

Instead of a big metadata block we have a block for time related variables, another one for location and so on.
This allows to design a model with an expected relationships between these dimensions and help make more interpretable the relationships.

We looked to several models and searched for the model that fit better with the data.
The first model just accounted the transcriptomics and the microbiome data, then in another family of models we added the information we know about those samples.
In a further family of models we split the information we know about these samples into three different blocks grouping them according to how are they related between them.
We fitted the best model for all the three family of models and we found that the most fitting model was from the family that had the data split on several blocks of independent variables.

However, the dimension reduction methods, as well as, generalized canonical correlation analysis, can't identify causal relationships.
Furthermore, there is no test to decide if a variable selected has some meaningful variable.
