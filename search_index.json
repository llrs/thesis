[["index.html", "Data integration on inflammatory bowel disease Preface", " Data integration on inflammatory bowel disease Llus Revilla Sancho 2022-03-27 Preface The main topic of the thesis is data integration applied in the inflammatory bowel disease (IBD) research. This disease is complex, for instance it is not known if the cause behind Crohns disease and ulcerative colitis is the same. There are hypothesis pointing that the microbiome is a major factor in the disease, which together with an aberrant immune response is the dominant theory. In order to find robust relationships between the microbiome and the immune system it is important to consider all the relevant variables that influence a disease. On this thesis I seek these relationships using data from different sequencing technologies and the observed or reported phenotype of the patients. The thesis was carried out on the Institut dInvestigacions Biomdiques August Pi i Sunyer (IDIBAPS) research institute and funded by Centro de Investigacin Biomdica en Red (CIBER). The thesis was done on the IBD unit which is a translational team of biologist, microbiologists, veterinaries, bioinformaticians, doctors and nurses (at Hospital Clnic) in a multidisciplinary team. The leading doctor of the unit was Julian Pans whose interest on the disease lead to receiving a grant that made possible this thesis. The thesis is on the doctoral programme in biomedicine of the University of Barcelona (UB). My thesis directors were Juanjo Lozano and Azucena Salas, my boss, who helped as bioinformatician and disease expert respectively. They provided advice and guidance on how to analyse the data and where to focus on the different experiments/analysis. This thesis, available on https://thesis.llrs.dev is licensed under the Creative Commons Attribution 4.0 International License (CC-BY). "],["abstracts.html", "Abstracts English Spanish Catalan", " Abstracts English Introduction: Inflammatory bowel disease is a complex disease with several factors that affect each other. The origin and process that maintains the disease is not fully understood, and current treatments focus on early detection of the disease and prevent the accumulation of damage on the intestine. Finding those relationships will help to advance treatments, diagnosis or prevent it. Methods: To identify these relationships we based our analysis on intestinal bacterial 16S data and human transcriptome from biopsies. We used canonical correlation analysis to find models that are coherent with previous knowledge on the biologiy of the disease and provide insights on the relationships between bacteria and genes. Results: We compared our own methods with previously published methods, mainly MCIA, to assess the advantages and disadvantages of the different proposed relationships. Furthermore, the classification of samples was used to find which methods provide are more accurate. Discussion: Biological relevance, difficult. Conclusion: Chips or further tests. Spanish Introduccin: Inflammatory bowel disease is a complex disease with several factors that affect each other. The origin and process that maintains the disease is not fully understood, and current treatments focus on early detection of the disease and prevent the accumulation of damage on the intestine. Finding those relationships will help to advance treatments, diagnosis or prevent it. Mtodos: To identify these relationships we based our analysis on intestinal bacterial 16S data and human transcriptome from biopsies. We used canonical correlation analysis to find models that are coherent with previous knowledge on the biologiy of the disease and provide insights on the relationships between bacteria and genes. Resultados: We compared our own methods with previously published methods, mainly MCIA, to assess the advantages and disadvantages of the different proposed relationships. Furthermore, the classification of samples was used to find which methods provide are more accurate. Discussin: Biological relevance, difficult. Conclusin: Chips or further tests. Catalan Introducci: Inflammatory bowel disease is a complex disease with several factors that affect each other. The origin and process that maintains the disease is not fully understood, and current treatments focus on early detection of the disease and prevent the accumulation of damage on the intestine. Finding those relationships will help to advance treatments, diagnosis or prevent it. Mtodos: To identify these relationships we based our analysis on intestinal bacterial 16S data and human transcriptome from biopsies. We used canonical correlation analysis to find models that are coherent with previous knowledge on the biologiy of the disease and provide insights on the relationships between bacteria and genes. Resultats: We compared our own methods with previously published methods, mainly MCIA, to assess the advantages and disadvantages of the different proposed relationships. Furthermore, the classification of samples was used to find which methods provide are more accurate. Discussi: Biological relevance, difficult. Conclusi: Chips or further tests. "],["introduction.html", "Chapter 1 Introduction 1.1 Inflammatory bowel disease 1.2 Integration studies on IBD 1.3 Integration", " Chapter 1 Introduction Inflammatory bowel disease (IBD) involves Crohns disease (CD) and ulcerative colitis (UC). It generally affects the terminal ileum and the colon but it can involve any segment of the gastrointestinal tract. UC is a recurrent, chronic and continuous inflammation of the colon and rectum while the CD is not a continuous inflammation and affects the whole gastrointestinal tract causing transmural inflammation. IBD etiology is unknown. However, once it has initiated the most prevalent hypothesis of its chronicity suggests an aberrant immunological response to antigens of the commensal microbiome. To diagnose IBD doctors use endoscopy and/or magnetic resonance imaging and histologies. Treatments provided for IBD include, noninflammatory drugs, suppressors and biologics, i.e, anti TNF- anti-IL2, 23, anti-integrin \\(\\alpha4\\beta7\\). The therapeutic options can induce remission in some patients, but they often need continuous treatment to avoid recurrence. Nevertheless, many patients are refractory or intolerant to those therapies and need to undergo surgery or other strategies like dietetic and psychological support [1] . 1.1 Inflammatory bowel disease IBD includes the CD and UC which are characterized by alternating periods of remission and clinical relapse that mainly affect the gastrointestinal tract. CD is a progressive relapsing disease that can affect all the gastrointestinal tract but shows mostly on both terminal ileum and colon with a discontinuous inflammation. UC is a colonic relapsing disease characterized by continuous inflammation of the colon. Both of them have different risk factors, clinical, endoscopic an histological characteristics (see sections 1.1.2 and 1.1.3 ). Around 3.5 million individuals have IBD in Europe and North America combined [2]. IBD is more commonly found in industrialized and developed regions, suggesting that environmental factors might greatly influence IBD occurrence. In addition, the incidence of IBD is increasing in areas, such as Asia or Eastern Europe, where the number of cases was relatively low hitherto [3]. The dysregulation of the inflammatory response observed in IBD requires interplay between host genetic factors and the intestinal microbiome. Several studies support the concept that IBD arise from an exacerbate immune response against commensal gut microorganisms. Nonetheless, the disease could result from an imbalanced microbial composition leading to generalized or localized dysbiosis1. The role of the gut microbiome in IBD is an active ongoing field of research. Several authors are currently studying the alterations reported in IBD of the intestinal microbiome. However, it is still unclear the cause-effect relation between dysbiosis and IBD. Partly due to the multiple variables already identified that have been linked to IBD; for instance, age, diet, usage of antibiotic, tobacco, and socioeconomic status [4]. The relationship between host and microbiome has been proposed to play a fundamental role in maintaining disease. For instance, some Proteobacteria species which have adherent and invasive properties might exploit host defenses and promote a proinflammatory environment, altering the intestinal microbiota in favor of dysbiosis [5]. The epithelium is often damaged and might present ulcers or other inflammation symptoms. A segment of the gastrointestinal tract might recover if the patient receives treatment or due the natural cycles of the disease. But once a segment is affected by the disease it can be considered as involved, as some damage remains even if the tissues is no longer inflammed. 1.1.1 Etiology and pathogenesis Several mechanisms have been proposed to drive IBD pathogenesis [6, 7]. Some of them are based on a relationship between the immune system and the microbiome [8, 9]. It is also unclear if CD and the UC share the same origin considering their different symptoms. There is also evidence of some genetic component on the onset of the disease, specially if the disease appears very early (less than 2 years old patients) [10, 11]. Disease can be classified based on age at onset as very early, early or adult on-set disease [11]. Genome-wide association studies (GWAS) have linked IBD to over 100 genetic loci, including a NOD2 gene, but so far there is not any known mechanism how polymorfism on this genes are driving the disease [12]. On early pedriatic and adult disease the genetic component is lower than on very early on set and it is thought that the environmental factors are the main cause of the disease at those ages. On the following sections I will explore the role of several of the possible factors involved on the pahtogenesis, starting with the genetics. 1.1.1.1 Genetics IBD is not an heritable disease, except for very early onset IBD, but it has some genetic influence that predisposes people to have it. This has lead to look for genetic factors on IBD both on general population and on the early cases. Genome wide association studies (GWAS) are one of the most common genetic studies performed, together with methylation studies. To discover through linkage desequilibrium genetic variations linked to phenotypes and regulatory transcription changes, respectively. With GWAS several allels on protein coding loci have been found, rising to around 300 genetic variants [13]. Particularly, the NOD2 gene is highly relevant for the disease on European patients, as it is a risk alleles for CD loci but show significant protective effects in UC [14, 15]. The mechanism of how this gene protects from UC has not been confirmed yet [12]. Many of the relevant genetic loci related to IBD are not on protein coding fragments of the genome. Recently expression quantitative trait loci (eQTL) particularly showed [10] that locis are on enhancers or promoters like e.g. H3K27Ac or promoter e.g. H3K4me1 marks as found by chromatin immunoprecipitation sequencing (ChIP-Seq). 1.1.1.2 Microbiome The human intestine is a large reservoir of co-existing microorganisms (bacteria, fungi, viruses, and unicellular eukaryotes) . This microbiome community exerts different functions in the human body influencing nutrients metabolism, the maturation of the immune system while suppressing the growth of harmful microorganisms [16]. The role of the gut microbiota has been proposed to play a role in IBD pathogenesis. IBD has been characterized by a breakdown in the balance between beneficial and harmful bacteria that are present in the human gut compared to healthy individuals [17, 18]. Indeed, many studies show that patients with IBD have less biodiversity. Biodiversity is measured on \\(\\alpha\\) (alfa) and \\(\\beta\\) (beta) diversity. \\(\\alpha\\)-diversity is a measure of the species present on a single sample and its abundance while \\(\\beta\\)-diversity compares the diversity between samples. There are some reports of taxonomic changes and increase on Enterobacteriaceae sp, Escherichia coli (specially the invasive strain) at the mucosal layer of IBD patients [19]. At the same time there is often a reduction of protective species like Bifidobacterium, Lactobacillus and Faecalibacterium which might be able to protect individuals from mucosal inflammation via several mechanism such as a downregulation of proinflammatory cytokines or the stimulation of IL-10 and antiinflammatory cytokines [19]. Specially Faecalibacterium prausnitzii is one microorganism of interest [20, 21]. In fact, it has been recently proposed that several unique microbial species can distinguish healthy controls from UC and CD patients [22, 23]. Figure 1.1: The microbial composition in a healthy and IBD gut. One of the proposed mechanism of crosstalk between bacteria and host is through bacterial metabolites. They interact with the cells and modulate the state of the intestine. One example of such metabolite is butyrate which has been linked to microorganisms presents on healthy intestines and shown to interact with intestine cells and help regulate some genes [24]. As previously mentioned, adherent invasive Escherichia coli, a proteobacteria specie, has been associated with IBD [25]. Adherent invasive strains are mainly found in ileal and colonic samples of CD patients and their presence in UC is less clear. These adherent invasive cells enter through the epithelium of the more permeable cells and live on their cytosol. The metabolic cocktail composed of soluble factors secreted by life probiotic bacteria, living microorganisms which, when administered in adequate amounts, confer health benefits on the host [2630] or any bacterial-released molecule capable of providing health benefits through a direct or indirect mechanism, has been collectively known as postbiotics since 2012 [26]. 1.1.1.3 Immune response As explained previously the immune system plays a role in IBD pathogenesis and pathophysiology. Loss of tolerance to commensal bacteria has been suggested as the underlying mechanism triggering the inflammation on the intestine. The immune response involves many different cells lines and regions, which are important to know how they organize for a better understanding of the disease. Figure 1.2: The intestinal epithelial barrier. From the luminal side of the intestine, the first layer is the mucosa (See sections 1.2 or 1.1). In the colon the mucus is organized in two layers: the inner layer, a firm mucus layer; and the outer, loose mucus layer [31]. The intestinal epithelium is a single layer of cells organized into crypts and villi (and circular folds on the large intestine) that carries out a diverse array of functions besides digestion performed by specialized cell lineages. Immune response in the intestinal mucous is mainly excreted by the gut associated lymphoid tissue [32]. Genetically predisposed patients when exposed to certain environmental factors activate immune responses against microbials or self-antigens which in turn, may impair the mucosal barrier of the intestinal mucosa, the first physical barrier on the mucosal surface. Both the adaptive and the innate immune cells are present in the intestine, right below the epithelium. On IBD due to antigen translocation into the lamina propria, the immune response leads the adaptive cells to generate immune response to harmless components of the intestinal microbiota. This initial response induces a local increase in the production of pro-inflammatory cytokines and mediators which damages the mucosa. Therefore, the loss of integrity on this barrier enables the intestinal luminal bacteria to access the intestinal epithelium and to interact with the immune system underneath it more directly [33]. The intestinal epithelium is another line of defense against bacterial invasion. Intestinal epithelial cells play a key role in controlling the integrity of the physical barrier to the intestinal microorganisms [33] not only physically but also secreting antimicrobial peptides and defensins, both of which are altered in IBD patients [34]. The intestinal epithelium also plays a key role on the intake and diffusion of metabolites from the intestinal lumen to the lamina propia. Damaging or increasing the permeability of the intestinal epithelium results on a response from the immune system. Detecting signals of any foreign particle can also trigger the immune system. On the intestine this starts with the identification of these signals by intestinal epithelial cells have pattern recognition receptors. There are two main pattern recognition receptors: toll-like receptors (TLR), which are present on the surface, and nucleotide-binding oligomerization domain-like receptors (NOD-like receptors), present on the cytoplasm of the cells. These receptors upon recognition of pathogen associated molecular patterns (PAMPs) start an amplifying signaling producing chemokines and cytokines which activates the transcription and translation of pro-inflammatory mediators to ensure an effective immune response. Initially the innate response is triggered but the cells also increase the antigen presentation to T cells and thus activate the immune adaptive response [35]. Other cell types, such as monocytes, macrophages and dendritic cells also present the pattern recognition receptors. From those, macrophages and dendritic cells are antigen presenting cells too and secrete several cytokines to activate other immune cells. Usually CD patients express higher amounts of TLR than healthy individuals, which might trigger a stronger immune response. This response is driven by CD4+ T cells proliferation in secondary lymphoid tissues to T helpers in the presence of the antigens and cytokines nearby. T helpers (Th) differentiate depending of the cytokines at which they are exposed. Th type 1 are driven by exposure to IL-12 secreted primarily by dentritic cells. Th2 are driven by cytokines secreted by macrophages. The imbalance between Th1 and Th2-promoting cytokines determines the intensity and duration of the inflammatory response in experimental colitis [36]. Th17-promoting cytokines are less well characterized in human. Treg cells differentiate after exposure to cytokines IL-10, IFN-\\(\\gamma\\) and TGF-\\(\\beta\\). Overall the presence of certain cytokines and the response to self-antigens are factors leads to an inflammation and damage that is related to the onset and establishment of IBD On these kind of diseases autologous hematopoietic stem cell transplantation has shown some benefits on IBD [37]. The benefit of hematopoietic stem cell transplant (HSCT) in autoimmunity is thought to originate from the depletion of auto-reactive cells regardless of their specificity. However, due to its associated risk this therapy is only given when patients are refractory to all available therapeutic options. 1.1.1.4 Environmental Chronic inflammatory disorders and neoplasms have become the main cause of morbidity and mortality in the countries with high standards of personal cleanliness. A decrease in human exposure to microbes or hygene which might affect the proper maturation of the immune system so that it provides less immune response or exacerbated response towards friendly microbes [38, 39]. From other environmental factors related to IBD such as tobacco, diet, certain drugs and stress; tobacco is the most influential environmental factor. Surprisingly, it has an opposite effect on UC and CD: in CD tobacco is a risk factor that increases the risk of relapse and/or surgical intervention. In UC, it has been observed that smoking cessation worsens the disease [40]. Pharmacological treatments such as oral contraceptives, non steroidal anti-inflammatory drugs are also related to develop or relapse the disease [41, 42]. The psychological welfare of people also plays an important role in the disease progression, stress, anxiety and depression might be important in relapse and deterioration of the disease [43]. Other environmental factors have been linked to IBD but without enough evidence to support a causative effect in the development of the disease. 1.1.2 CD physiology As previously introduced, CD is a chronic inflammatory disorder characterized by a discontinuous inflammation of the gastrointestinal tract. Inflammation on the gastrointestinal tract is transmural and can affect from the mouth to the anus, but mainly it manifests on the ileum and colon [9]. It is frequently associated with extraintestinal manifestation and/or concomitant immune-mediated diseases. The disease itself manifest an heterogeneous symptoms that can involve, diarrhea, weight loss, abdominal pain, fever, anorexia and malaise. Other less frequent co-occurring manifestations are arthritis, primary sclerosing cholangitis, skin disorders venous or arterial thromboembolism and/or pulmonary involvement [44] . These symptoms make it hard to correctly diagnose the disease by non-specialists, in addition there is not a non invasive easy procedure to diagnose it. All these can lead to delays on correct diagnosis of the disease. The detection of parasites or bacteria, such as Clostridium difficile, have been associated with CD [45]. The detection of fecal calprotectin, is generally a good marker of endoscopic activity with sensitivity above 70% and specificity above 80% [46, 47]. Usually the best diagnosis method is to perform a colonoscopy, whether there is inflammation on the gastrointestinal tract on discontinuous regions then is CD. This inflammation could also present ulceration with rectal sparing and histological lesions which also help to diagnose the patients [48]. Usually on CD a granulome, that is a region with big multinucleous cells, can appear on any intestinal layer. In addition to the inflamed location(s), mosaic zones (patches of inflamed and non-inflamed areas) are more characteristic of CD [49]. The Montreal classification aims to classify patients according to their age of disease onset, standardized anatomical disease location an disease behavior. This classification assumes that the location of CD remains stable over time after diagnosis but behavioral phenotypes change. Other scores consider area affected: Montreal classification allows for early onset of disease to be categorized separately those with age of diagnosis at 16 years or younger, diagnosis at 1740 years and &gt;40 years, respectively [11]. SES-CD: simple endoscopic score for CD [50]. Score based on size of ulcers, ulcerate surface percentage, affected surface and presence of narrowing on the bowel. CDAI: CD Activity Index takes into account weight, ideal body weight, sex, and events on the last week such as liquid stools, abdominal pain, general well-being and if anti-diarrhea drug usage, as well as knowing if there are fistulas, fever, and other complications [51] To some extent, there is a disassociation between clinical symptoms and the endoscopy finding. Often patients report feeling better despite lack of muscular healing [52]. To overcome this disassociation and be able to compare the well-being of patients several scores and thresholds are used on research and by physicians that will be described later. In the early stages of the disease the relapsing and remitting course is more frequent. Often relapses are accompanied by clinical symptoms, and few have prolonged clinical remission (without treatment) [53]. When there is clinical remission, there can still remain some other lesions and often subclinical inflammation persists. Frequently the damage caused by the disease evolves to fibrostenotic stricture or penetrating lesions (fistula and abscess). Damage of the disease might not be apparent to patients and might be only seen several years later than the first detection [52]. Mucosal healing is a first step towards the healing of deeper layers of the inflamed bowel wall on the CD. Patients might progress from an inflammatory phenotype to a stricturing or penetrating one [11]. Stricturing is a narrowing of a part of the intestine often because of scar tissue and fibrosis in its wall. Penetrating is when the epithelium has some holes or tubes. If these tubes result in an abnormal connection between two body parts it is a fistula, it might also result in an abcess, a collections of pus, often developed in the abdomen, pelvis, or around the anal area. 1.1.3 UC physiology As previously introduced, UC is a chronic inflammatory disorder characterized by a continuous inflammation of the colon. Depending on the inflamed segments of the intestine it is classified in several phenotypes. Around a third of the patients with UC suffer proctitis, the inflammation of the rectum. If the segments from rectum to the sigmoid colon are affected is a distal colitis, if it affects the left colon then it becomes a left colitis. If the inflammation continues to the descending colon it is then an extensive colitis until it affects the whole colon when it becomes a pancolitis. The extension of UC is inversely related to the frequency. However, the extension and severity of the disease correlates: the prognosis is worse the more extended it is [54]. In addition, the damage usually consists in many neutrophil in the lumen crypt [49]. The goal of the clinical care is to recover. As a first step, the symptoms of IBD have to lessen to the point that they are mostly absent, gone, or barely noticeable, this is known as clinical remission. However, this is not enough as the mucosa might be still inflamed and thus the reconstitution of the structure and function of the intestinal mucosa is not complete. Other lesions, might aid to the progression to other phenotypes such as fibrostenotic stricture or penetrating lesions or primary sclerosing cholangitis [55]. To prevent and avoid further damage several procedures are followed: When there is dysplasia, an abnormal development of cells within tissues or organs (which is considered a precedence before colorectal cancer growth [56]), or the damage on the colon has been too big a surgical procedure to remove part or all of the colon must be done. Patients that undergo a colectomy need to have their bowel reconnected with a procedure called ileoanal anastomosis (also know as J-pouch by the shape it takes) surgery. Often the lining of the pouch created during surgery becomes inflamed on what is known as pouchitis [57]. Many scores have been proposed for several purposes, from quality of life to disease severity or patient status. Among the scores most used are the following: Mayo: A score designed to be simple to calculate based on stool frequency, bleeding, mucosal appearance at endoscopy and physicians assessment [58]. IBDQ: A 32 questionnaire used to assess the quality of life grouped into four categories: bowel, systemic, social and emotional [59]. UCEDIS: An endoscopic score based on vascular pattern, internal bleeding and erosion and ulcers [60]. Other measured parameters include, weight, effective weight, fecal calprotectin, C reactive protein and hemoglobine. 1.1.4 Treatment Current treatment attempt to induce and keep the remission of patients and reduce secondary effects of the disease instead of revert the pathogenic mechanisms. As standard of care corticosteroids, aminosalicylates and immunosuppressor and some other drugs like antibiotics or metronidazol are util in some cases. Acid 5-aminosalicylic (5-ASA or mesalazina, pentasa) can be given in a topic way (either liquid, enemas, or suppository) or in oral form (pills or dilutions). In CU it helps in the clinic remission but it does not always mean that there is remission (is twice much likely than placebo to reach remission) [61]. On CD the effects are not so stark and generally it does not produce changes on the disease [62]. Antibiotics, such as metronidazol and ciprofloxacina, are effective to deal with secondary effects of IBD such as abscess and bacterian overgrowth in CD [63, 64], but they do not seem effective on UC [64]. Corticosteroids can be taken orally, such as prednisolona, prednisona and Budesonide; intravenous, hidrocortisona, metilprednisolona; or via enemas and suppositories. Budesonide is not absorved well and has a limited biodistribution but it has good therapeutic benefits with a reduced systemic toxicity in IBD [53, 65]. These drugs work very well as antiinflammatory for mild or severe IBD but do not work well as maintenance drug [66, 67]. Thiopurines (Azathioprine, mercaptopurina) are immunosuppressants drugs that deactivate key process of limphocytes T that might trigger the inflammation. As a side effect they are toxic due to their interaction with nucleic acids [68]. on CD they are useful to induce and keep remission [69], while on UC they are used to keep the remission [70]. In the last two decades IBD treatment has moved from aminosalicylates, corticosteroids and immunomodulators to anti-TNF\\(\\alpha\\). Anti-TNF\\(\\alpha\\) drugs has changed IBD treatment as it reduced the hospitalization associated with previous treatments, reducing medical costs and risk of surgery as well as induce a better mucosal healing and quality of life for patients [71]. However, 20-30% of patients have no response to this treatments and another 30-40% lose response in a year [72]. Recently a new wave of drugs has been developed targeting different molecules such as vedolizumab, targeting anti-integrin\\(\\alpha 4 \\beta 7\\), ustekinumab, targeting both IL-12 and IL-23, risankizumab an anti IL-23, tofacitinib an inhibitor of JAKs, infliximab an anti-TNF\\(\\alpha\\). Patients might become refractory to drug. Thus, drugs do not have the same effect as previously and the dose might need to be increased with the risk of more secondary effects [1]. Surgery resection might be needed on these patients. Close to 35% of patients with UC will need to have a surgery resection, either due to complications or because the inflammation can not be controlled. Surgery usually removes the inflamed segment of the colon. The most common procedure used is a colectomy (whole colon removal), with ileostomy [73]. CD patients usually require surgery associated to complications like stenosis, abscess, and fistulas ) between 70% and 90% at some point of their lives [74]. Usually the surgery is limited to removing the inflamed segment but occasionally an ileostomy is required [75]. If the drugs fail to contain the inflammation and heal the mucosa doctors might recommend a different procedure. In some cases HSCT is recommended which have shown to improve the life of the patients [76]. This is a new procedure given only to the most extreme cases to reset the immunological state of the patient. To reset or hugely modify the microbiota fecal microbiota transplantation between different people is currently being explored [77]. 1.1.5 Summary IBD is a complex disease that impacts the health of many people for long time and with lasting impact on their quality of life. Current clinical care in some cases is enough to have a sustained clinical and endoscopic remission but most often is not enough and relapse is expected. Several factors, such as becoming refractory to drugs, intermittent course of the disease and lack of validated predictors of disease course or response to therapy make the treatment complex. Lack of knowledge of what are the factors cause of the disease make those treatments and drugs to be addressed to block further inflammation and damage, but cannot prevent it and often they do not stop it completely. 1.2 Integration studies on IBD Many studies have looked up to the origin of the disease. As seen, one of the hypothesis behind the maintenance of the inflammation involves the microbiome and the host epithelium. This has been studied using several data sources, mostly from sequencing data. The technical methods used to obtain the data of the inflamed tissue differ between extracted from biopsied samples at colonoscopy or from surgical samples. Those samples are usually used later on to diagnose or for research purpose. To obtain research-quality data it usually imply using techniques such as immunohistochemistry, histopathology, immunohistochemistry, fluorescence in situ hybridization and polymerase chain reaction. These techniques allow to measure or visualize where are the cells expressing certain proteins or genes, thus helping with the analysis validation. Furthermore several studies have been carried out to discover links between microbiome and the inflammation, followed by those looking for some relationship between genetics and the disease and more recently the metabolome. These studies, known as integration, multi-omic or interaction studies, usually use multiple sequencing assays as the bases of the analysis [78]. However, confirming causal interactions of the variables of each essay is difficult. To find relationships some articles use correlation [79], there are others that use a combination of methods from correlations, partial correlations to integrative methods [8082] and network integrations [81]. Very rarely there is an experimental confirmation of the relationships between variables of the different assays because it is complicated to test an interaction and to set up the right conditions for the many variables that are accounted for on the integrations. One of the few methods published that shows an interaction between genes and microorganisms on IBD is to expose the ex-vivo sample or cell lines with microbiomes or supernatant of at their culture [83]. 1.2.1 Type of data used for integration analysis According to the data used, we can classify the studies: 1.2.1.1 Transcriptome Most of the integrations refer some other source of data to the transcriptomics of the patient. The transcriptome of patients derived samples has been extensively studied since the existence of microarrays. There are known marker gens of inflammation and many research focus on identifying prognosis predictors and treatment response prediction based on gene expression [8486]. Recently single-cell RNA-seq technology has enabled to estimate cell populations of the samples with better degree of success than bulk RNA-sequencing. Single cell technologies are starting to be used for integration. 1.2.1.2 Microbiome Many of the integration analysis on IBD are done between host transcriptome and the microbiome. These studies use datasets from IBD patients usually stratified by disease activity or severity of inflammation or location of the disease. Most of them are based on correlation analysis between the microbiome and RNA-seq [79]. Conclusions of these integrations range from finding differences on the correlation depending on the type of disease ([79]) to finding relationships with inflammatory genes [80]. 1.2.1.3 Genetics Genetics is the next most common data source used to integrate data on IBD. Most studies on genetics and IBD are genome-wide association studies. The genetic component is specially important on IBD that starts on children [13, 82, 87]. When using genetic data to integrate it with transcriptomics it is usually to understand how a genetic variant is affecting a gene expression. This has lead to the identification of expression quantitative trait loci (eQTL) [8891]. 1.2.1.4 Metabolome More recently there have been an increased interest on the study of the metabolic stat of IBD patients, given that microorganisms interact with the host also via their products and metabolites. There is evidence some of these metabolomic products come from the microbiome [24, 83]. Some studies have integrated the metabolome with the RNAseq and state of the epithelium [92, 93]. 1.3 Integration The term data integration is widely used with varying meanings. According to the dictionary integration is defined as: the process of combining two or more things into one  Cambridge Dictionary Other words used are integration, and if specific to data from sequencing technologies, multi-omics or pluri-omics. Here integration will be used as it is the more general one and not restricted to omics or sequencing technologies. Since the beginning of the integration methods there have been many methods proposed [94]. Some of the early methods were initially used for surveying the agreement of different evaluating systems [95], others were developed for agricultural sciences [96] or food industry [97]. Some of these methods are specific for one application or data type while others are more generally applicable. Lately, the access to bigger datasets with more variables and often from the same samples has increased the focus of the research community on the methodologies available on several disciplines but mainly on biological sciences. The explosion of data on biological sciences has been driven by the new sequencing technologies that allow to measure thousand of variables of many samples at the same time. If done with multiple sequencing technologies it is usually referred as multi-omics methods, which usually only uses omic data. It is crucial to classify, review and compare tools available, as well as, to benchmark these tools against the same dataset as a way to provide clear recommendations to anyone wishing to use them [98]. Part of these efforts use the methods strategies to classify them [99, 100]. Following this view I will review the available integration methods according to several axes: type of data used, aim of the method, relationships between variables, relationships between samples, relationship between variables and samples, input data, mathematical framework and results of the method. 1.3.1 Classification of integration methods Integration methods have very different properties that allow them to be classified and compared [101]. Here I classify those meant to be used with omics datasets, with references to concrete methodology and in occasionally to articles using them. Figure 1.3: Unsupervised data integration methodology. Figure 1 from Huang 2017. 1.3.1.1 Data type: numeric or categorical The most important distinction in integration methods is what kind of data are combined. In general data can be divided between categorical and numeric variables, which are usually found in several fields. Sometimes clinicians want to understand the relationship between a phenotype they observe and the underlying mechanism. Usually this involves looking how metabolites, gene expression, methylation, number of variants a gene has, and other numeric variables are related to the observed phenotype (i.e.pain). Depending on the methods aim it handles numeric and categorical data types or just one. Often they are used differently. The most common way to handle different types of data is converting the categorical values to a mock or dummy variable. For each categorical factor there is a new variable whose value is 1 if that sample had this factor and 0 otherwise. For instance, if the categorical variable has three values (A, B, C) it would be converted to A (1, 0, 0) B (0, 1, 0) and C (0, 0, 1). Often the number of variables created is one less than the number of factors that existed, on this example only A and B would be kept. This transformation allows to use categorical values as numeric variables. If the method only accepts categorical data but you want to provide numeric values usually those values are categorized. For example if a variable is (0.123, 0.25, 0.56, 0.78) one could make to categorical values like (&lt;0.5, &lt;0.5, &gt;0.5, &gt;0.5). The number of categories to use and how the numeric value splits depends on each case. Very rarely methods allow to use both data types as they are. If they allow so, it is usually for classification purposes only. 1.3.1.2 Objective The objective of any method is one of its most important defining properties. Data integration can be classified according to the (biological) question they try to answer. In general all of them aim to provide a better understanding of the relationships between the omics types. Often a single method is not enough and several methods are used on the same dataset. This is specially relevant when a potentially relationship between omics is discovered. For instance, checking that in a particular case or condition a given relationship is present, might require experimental confirmation. Most of the times one (or several) of the following results are expected from integration methods: Classification of the patients or samples One of the purposes of integration might be to use multiple sources of data to accurately describe how do samples or patients fit on a predefined possibles states. The objective here might be to accurately describe whether a patient has one or other related disease of a possible subset of diseases or phenotype [102]. An overview of the role of each individual omic in a biological system Sometimes the question is which omic method is the best describing a disease. This knowledge could prevent performing expensive tests and replace them by more affordable or easier technique that has enough predictive power or is sensitive enough and specific for the task. An example of this is the search of markers on blood to identify links between different cell populations [80, 103]. Finding a molecular signature A signature is usually a group of features that describe/are representative of a cell type, a process or a stage. Identifying a subset of variables from the omics that are related is often a desired goal because it reduces the amount of variables allowing to perform experiments on the bench on just those that might be important. In other fields, such as machine learning, selecting the important variables is known as feature selection. There are several methods that are used to do this [99]. An example of this is when performing eQTL analysis, where a locus is related to a change in gene expression [8891]. A predictive model Predictive models usually require a very good understanding of the current and/or past relationships, as well as, a good feature selection procedure. If a good model on previous data exists it might be used to predict future events. Sometimes, models are only built to predict events without being able to accurately understand the underlying mechanism. This kind of methods are used to improve treatment selection, diagnosis and prediction of prognosis [104]. Impute values Some methods aim to accurately guess the values of missing data given some other information. Missing values can happen for a variety of reasons from practical ones, like a sample not being available, to technical ones, such as laboratory mistake [105]. However, this is often a intermediate step to other goals. To complete these goals it is important to have enough statistical power to determine the significance of tests performed (if any) and to understand how complete are the data sources used on the integration [106]. Having more statistical power helps identifying the relationships one seeks when using this methods. 1.3.1.3 Relationship between variables and samples Depending on the amount of variables and samples used in studies can be classified in two types. Traditionally for each sample few variables are measured, for instance on a biopsy with RT-PCR only a few genes are measured, however with the new omics techniques (transcriptomics, metabolomics, methylomics, genomics), thousands of variables are measured for the same sample. This has lead to the following situation: More variables than samples For a single sample of RNA around 50k genome identifiers (genes, long non coding RNAs, iRNA, pseudogenes,) can be measured. Which leads to the case where there are many more variables than samples. Thus high-throughput data analysis typically falls into the category of \\(p \\gg n\\) problems ( big p, little n), where the number of genes or proteins, \\(p\\), is considerably larger than the number of samples, \\(n\\). With such high number of variables the identification of the relevant variables is hard because variables will co-variate. When many variables are tightly correlated, discovering which one is important using just numerical methods is challenging. This is even more difficult when looking for causal relationships. More samples than variables This was the usual case when for instance, from a cohort of patients the temperature is measured along the stage of a disease: two variables, time and temperature for each sample. If there are more than 2 patients, then the number of samples is greater than the number of variables studied. This is described in the literature as \\(n \\gg p\\) (or big n, little p). Nowadays this is less frequent on the bioscience world, and does not causes trouble analyzing it because the high number of samples allows to accurately estimate the dispersion of variables. There are several methods available to estimate the number of samples required [107]. Having just the enough samples for the desired statistical power, however, might not be enough in case some samples are not correctly processed. In addition, variables might be separated on different blocks of variables. These blocks might be just of the same source or from multiple methods. Depending on the method this blocks might have special meaning: when all the data is joined in a single block it is known as superblock. 1.3.1.4 Relationship between samples Depending on the relationship between the samples, the questions answerable and the methods that work on them differ. A sample can have multiple or one data source, for instance we could have RNA-seq and 16S data from the same sample. In a study if all the samples have all the data from multiple data sources it is a complete case. If some samples have data from some data sources but not from others the study is not a with a complete case. Sometimes because the sample is not enough, or there are some technical or organizational problems a source of data for a sample (which is known as an incomplete case) might be lost. This results in a new source of variation that has to be dealt with, which complicates the conclusion one can draw from the studies of these kind of data. Even when all the cases of a patient are complete the samples can come from several sites of the same individual or with different combinations of variables, which makes it relevant to understand the relationships between the different samples. There is no easy classification of this as each experiment might be designed differently. In general, experiments are designed to be as consistent as possible but in face of adverse events that become a variation of the design the analysis complicates. Either some data is imputed or some samples are omitted for the analysis. This can happen with samples taken at different timepoints as patients for instance if they miss a follow up visit. Time As mentioned above, time is one of the factors that sometimes cannot be controlled, despite having programmed visits every two weeks, for instance, some patients might come early or later due to multiple reasons. Sometimes, the objective of the study is precisely to analyze the relationships at different time, or to asses how the relationships change with time. To discover causality between two variables the cause must precede the consequence, which highlights the importance of time. Being aware of the time differences and time scales is crucial in most cases. During in vitro experiments, conditions can be reproduced even if they are at different time. However, when using patients material replicates can not usually be performed like in vitro experiments. This makes it harder to study time-related change on patients. Lastly, time between the collection of a fresh samples and its processing also influences the readings of the samples of the omics technology, specially RNA-seq [108, 109]. Some genes are more influenced by time than others but as they are measured at the same time in all samples this might distort the data. Keeping track of the time that it takes to process samples is also hard to due and requires a highly coordinated effort [110]. 1.3.1.5 Relationship between variables Once data is collected, the next step is to understand the relationship between the variables present. As mentioned earlier, some variables influence others which can affect the outcome in complex ways. With many variables present in a dataset it is important to be aware of known relationships between variables. Even in a simple dataset, like an RNA-seq dataset, it is important to be aware of the relationships between variables. Since the discovery of the lactose operon it is known how some genes regulate each other [111]. However, it is not know how other variables are related between them. For instance, how does the increase in expression of a gene affects the growth of a microorganism? Usually the relationships between variables are mediated by many factors or interactions. One of the best examples of such interactions is when some variables correlate. Their correlation can be used to reduce the number of variables being analyzed by ignoring the relationships between them and using the most representative variable (less widely correlated and with more variation). This step is usually done by dimension reduction methods. However, sometimes this is not desirable or feasible as the correlation does not explain the direction of the causality of the interaction between the variables (if there is any). Network approaches relate the variables to each other [112]. These approaches are fairly new and growing in popularity partly because they can address the direction of the interaction. In partial correlations the effect of other variables on the two being under study are taken into account [113]. They assumes a linear relationship between the co-occurring variables and those of interest. However, it is computationally expensive when there are thousands of variables. 1.3.1.6 Input data We have classified the studies according to the data they use (as seen previously ). But, some methods to account for relationships of variables only work when a dataset is complete while other do not: Data from the same samples: These methods do not handle well or at all missing data. They need complete cases/data of the samples in order to be able to integrate the results. These methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [114, 115], Multiple co-inertia analysis (MCIA) [116], Multi-Study Factor Analysis (MSFA) [117], Multi-Omics Factor Analysis (MOFA) [118] and STATegRa [119]. Data from different samples: These methods do not need data from the same sample. They draw their conclusions generalizing from the data available. Some of them handle missing data, while others do use the data at face value. These method includes MetaPhlAn2 [120], HUMAnN,[121] and LEfSe [122]. Furthermore, some methods are designed to integrate specific types of datasets, (usually because they make some assumptions that are only met on that kind of data). For instance, HCG, 16S rRNA-seq, RNA-seq and metabolomics do not share the same data distribution, and are different between them. Also even with the same data depending on the processing of the data they can have very different properties: OTUs (operational taxonomic unit) properties are not the same as ASV (amplicon sequence variants) when analyzing 16S rRNAseq data. 1.3.1.7 Mathematical framework Methods use different mathematical frameworks to process the data. Here I briefly describe some common mathematical frameworks, some of which have previously appeared: Networks Networks methods were mentioned because they use and find information about the interaction of variables. Multilayer networks, including the multiplex, Molti-C-DREAM [123], Random Walk with Restart on Multiplex and Heterogeneous Biological Networks RWR-MH, Random walk with Restart on Multiplex RWR-M [124]. Network embedding MultiVERSE are some of the methods using networks [125]. Bayesian approaches are also quite frequent, these methods use the Bayes theorem to see the relationships between variables. The Bayes theorem explains that the conditional probability of a variable is related to the prior knowledge of conditions that might be related to the event [126]. Some methods that use these approaches are Reconstructing Integrative Molecular Bayesian Network (RIMBANET) [127] and Bayesian Consensus Clustering (BCC) [128]. Dimensional Reduction These methods focus on finding just a few variables and summarizing them using a function that has some desired property such as the correlation between thre transformed variables is maximal while the components are orthogonal.s The selection of variables is usually done with L1 or Lasso Regression regularization technique or L2 also known as Ridge Regression. L1-regularization adds a penalty equal to the absolute value of the magnitude of coefficients which might leads to some coefficients becoming zero and the variable eliminated from the model. On the other hand, L2-regularization does not result in elimination of coefficients or sparse models and can only be used when there is multicollinearity as it works well to avoid over-fitting. Several tools use this approach, Momix [129], regularized canonical correlation analysis (RGCCA) [130], mixOmics [131] and STATegRa [132]. Other methods use bayesian approches like the Bayesian Group Factor Analysis [133]. Active module identification Multiomic objective genetic algorithm (scores based in two metrics; node score and density of interactions score). An example of a method using this approach is Multi-Objective Genetic Algorithm to Find Active Modules in Multiplex Biological Networks (MOGAMUN) [134] Usually depending on the mathematical framework used, these methods return similar outputs. 1.3.1.8 Output results According to the output the integration methods can be classified in several groups: For the network methods the following output is usually returned: Connections between the variables/nodes, a measure of how strong is the connection (or simply if there is a connection or not). For dimensional reduction methods there are three possible outputs: Shared factor across the data, specific factors for each data or mixed factors. Shared factors: Integration results in a vector of the samples in a lower dimensional space that is shared by all the data set used to integrate. Such methods include iCluster, Multi-Omics Factor Analysis (MOFA) [118]. Specific factors: Integration results in several vectors of the samples in a lower dimensional space of each data set used to integrate. Such methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [114, 115], Multiple co-inertia analysis (MCIA) [116] and Multi-Study Factor Analysis (MSFA) [117]. Mixed factors: Integration results in both shared and specific factors, to each dataset and common to all them. Such methods include Joint and Individual Variation Explained (JIVE) [135] and integrative Non-negative Matrix Factorization (iNMF) [136]. 1.3.2 Interpretation How to interpret the results of applying the different methods is highly linked to understanding the method and its output. On a correlation between two variables, the interpretation of the analysis is clear, if one variable increase, the other one too. The implications of this correlation can be far reaching but the principles to understand them are simple. However, on more complex methods the interpretation becomes less clear. The interpretation of a canonical correlation analysis is much harder [137]. Also on more complex methods the number of parameters required increases so the time and intellectual effort to understand the relationships between the parameters is also higher. The interpretation also helps to discuss the results and relate it to other previously know information. Individually: Here we study how each variable relates to another. In the correlation analysis, the relationship between two variables under study. Or if looking by patient: how do interpret that in these patient variable A and B is X and Y? Globally: In a principal component analysis for instance how do we interpret that some variables have the same loading? What happens in a more complex method like canonical correlation analysis? There are some articles about how to interpret those methods on real datasets [138]. Others, to benchmark and to learn how to interpret propose analyzing a simulated dataset [139, 140]. Which is used to compare the results of the integration with the dataset of interest and to compare different tools. These datasets are created with some relationships that the tools are expected to find. There exists several methods to create synthetic datasets like MOSim [141], metaSPARSim [142], CAMISIM [143], ballgown [144], polyester [145] and even edgeR [146]. These methods are useful to compare different setup but they can miss some subtle not previously reported relations on real data. 1.3.3 Reviews The comparison and review of methods independently from original authors have become a crucial step for selecting the right tool to apply a given dataset and research question [129]. Some of these reviews focus on a specific type of data integration: metabolomics [99], genomics [10], microbiomics Others focus on the disease and the challenges of each omics and the need of an integrative approach to provide better therapies [106, 147, 148]. On this regard there are several efforts to integrate data on IBD but no comprehensive review to date is known to the author. The most comprehensive article to date is a very recent review identifying problems and providing recommendations for future work [149]. 1.3.4 Summary The field of integration is large and complex, with increasing interest over the last few years, specially in the psychology and omics fields As a methodology they are quite complex and diverse but there is a growing interest on them to help answer complex questions without using other complex tools like deep neuronal networks or other machine learning approches (despite not being incompatible). Methods to integrate have many characteristics, depending on the objectives and data that available. Regardless of the method used, interpretation and reporting is usually the main challenge. References "],["hypothesis-and-objectives.html", "Chapter 2 Hypothesis and objectives 2.1 Hypothesis 2.2 Objectives", " Chapter 2 Hypothesis and objectives 2.1 Hypothesis I hypothesize that the presence of certain microorganisms are related to the patient well being. Furthermore, the expression state of the epithelium and the amount of bacterial presence might identify whether patients are suffering an IBD. Finally, I believe that environmental factors interact with the IBD and are responsible of the big variability on the IBD discourse. 2.2 Objectives The main objective of the thesis is finding the relationships between microbiome and gene expression in the intestinal mucosa. For this reason weIdentify microorganisms and genes related to the IBD. Identify how is the relationship between the intestine mucose gene expression and the microbiome present on the intestine. Find the influence of variables such as location (colon and ileum), the treatment received, IBD activity, sex, age, etc. in the microbiome and mucosa relationship. Identify possible mechanism of interaction between the microbiome and mucosa "],["materials-and-methods.html", "Chapter 3 Materials and methods 3.1 Datasets 3.2 Sample processing 3.3 Integration methods 3.4 Regularized generalized canonical correlation analysis 3.5 Statistics 3.6 Functional enrichment methods 3.7 Variance and diversity methods 3.8 Other methods", " Chapter 3 Materials and methods This chapter contains a brief description of the main characteristics of the different datasets used on this thesis . The complete processing protocol before obtaining the data of those dataset that were not generated at Hosptial Clnic can be found on their respective reference. Samples of the different cohorts collected in Hospital Clnic were collected similarly and described only once. Differences between protocols are noted on the respective datasets section. Here we describe all the methods used to analyze data from the multiple cohorts included in this thesis. The code used can be found on the links provided in the appendix. 3.1 Datasets 3.1.1 Pugets dataset The glioma dataset is the data provided as an example of biological data by the authors of RGCCA from a previous publication [150]. The data came from diffuse intrinsic pontine glioma patients whose transcriptome was analyzed with Agilent 44K Whole Human Genome Array G4410B and G4112F. The copy number variation of the samples was processed with the ADM-2 algorithm, and data from comparative genomic hybridization (CGH) analyzed using Mutation Surveyor software. In addition, this dataset contained information on age, localization of the tumor, sex and a numerical grading of the severity of the tumor [150]. Table 3.1: Characteristics of samples from Pugets dataset. Characteristic Pugets Samples 53 Sex (female/male) 28/25 Location (cort/dipg/midl) 20/22/11 3.1.2 HSCT dataset Samples from the HSCT dataset used in this thesis were from a cohort of patients with severe refractory CD undergoing HSCT. Patients were treated in the Department of Gastroenterology (Hospital Clnic de Barcelona Spain). The protocol was approved by the Catalan Transplantation Organization and by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7244). All patients provided written consent following extensive counselling about being included on the study and using their data on publications. Colonic and ileal biopsies were obtained at several time points during ileo-colonoscopy, at inclusion and every 6 or 12 months after HSCT up to 4 years after the start of the treatment. Samples were obtained when possible from both uninvolved and involved areas. In addition, biopsies were taken from the ileum and colon regions of 19 non-IBD controls consisting of individuals with no history of IBD and who presented no significant pathological findings following endoscopic examination for colon cancer surveillance (Hospital Univesitari Mtua de Terrassa Spain). The protocol was approved by the Institutional Ethics Committee of the Hospital Univesitari Mtua de Terrassa (Study Number NA1651). At least one biopsy was collected and fresh-frozen at -80C for microbial DNA extraction. The remaining biopsies were placed in RNAlater RNA Stabilization Reagent (Qiagen, Hilde, Germany) and stored at -80C until total RNA extraction. In total 158 samples with both RNA and DNA extraction of the same segment and time were available 3.2: Table 3.2: Characteristics of samples from HSCT dataset. Characteristic HSCT Sex (female/male) 22/15 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 7/11/0 Duration (in years): mean (min-max) 14 (8-28) Age: mean (min-max) 44 (23-70) Disease status (non-IBD/CD) 51/107 Sample location (ileum/colon/unknown) 48/108/2 Local simple endoscopic score for CD: mean (min-max) 2.15 (0-12) CDAI: mean (min-max) 120 (0-450 3.1.3 Hslers dataset An IBD-related dataset was obtained by Prof.Dr.Rosentiel and Prof.Dr.Robert Hsler [79]. Biopsies were obtained endoscopically during routine diagnosis RNA and DNA were extracted using standard procedures. DNA from the 16S rRNA gene was amplified with primers 319F and 806R. Both RNA and DNA was then sequenced on HiSeq 2000 and MiSeq respectively. These biopsies included samples from the terminal ileum and sigma from CD, UC, infectious disease-controls and healthy non-IBD. The dataset included information about gender, location, age, and the status (inflamed or non-inflamed) of the region from which the biopsy was taken. Table 3.3: Characteristics of samples from Hslers dataset. Characteristic Hslers Disease status (non-IBD/IBD) 33/26 Sex (female/male) 42/17 Sample location (ileum/colon) 30/29 3.1.4 Morgans dataset A previously published dataset from a pouchitis study was analyzed [151]. In this study patients having undergone proctocolectomy with ileal pouch-anal anastomosis for treatment of UC or familial adenomatous polyposis at least 1 year prior to enrollment were recruited at Mount Sinai Hospital (Toronto, Canada) excluding individuals with a diagnosis of CD. The dataset has a total of 255 samples from 203 patients, containing data for both host transcriptome and intestinal microbiome. On some cases several biopsyes were collected from the same patients. This dataset included anonymous identifiers for patients, whether the sample was from the pre-pouch ileum (PPI) or from the pouch, the sex, the outcome of the procedure and an inflammation score. The pouch ileum might be inflamed or not. Table 3.4: Characteristics of samples from Morgans dataset. Characteristic Morgans Samples (n) 255 Sex patients (female/male) 101/102 Sample location (Pouch/PPI) 59/196 3.1.5 Howells dataset This dataset included a cohort of 66 treatment-nave children at diagnosis of their IBD, along with 30 age- and sex-matched non-inflammatory control children, recruited at the Paediatric Gastroenterology unit at Addenbrookes Hospital (England) [152]. Data from 77 samples that had both RNAseq and 16S data was used. There are 10 non-IBD samples, 11 with CD and 11 with UC. Data has the following characteristics: disease, age at diagnostic, age at time of study, sex, sample location, and disease activity: Table 3.5: Characteristics of samples from Howells dataset. Characteristic Howells Disease (non-IBD/CD/UC) 11/10/11 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 32/0/0 Age: mean (min-max) 12 (6-15) Sex (female/male) 10/22 Segment (ileum/colon) 31/46 Clinical history (inflammation/no inflammation) 24/53 3.2 Sample processing 3.2.1 RNA sequencing Total RNA from mucosal samples (HSCT cohort) was isolated using the RNAeasy kit (Qiagen, Hilde, Germany). RNA sequencing libraries were prepared for paired-end sequencing using HighSeq-4000 platform. Samples with good enough quality as recommended by FastQC were processed with cutadapt (version 1.7.1 [153]) for quality filtering. Later, the libraries were mapped against the human reference genome using the STAR aligner (2.5.2a) with Ensembl annotation (release 26 of GENCODE, GRCh38.p10 or superior) [154]. Read counts per gene were obtained with RSEM (version 1.2.31) [155] as previously described [156]. 3.2.2 Microbial DNA sequencing Biopsies from the HSCT CD cohort were resuspended in 180 \\(\\mu\\)l TET (TrisHCl 0.02M, EDTA 0.002M, Triton 1X) buffer and 20mg/ml lysozyme (Carl Roth, Quimivita, S.A.). Samples were incubated for 1h at 37C and vortexed with 25 \\(\\mu\\)l Proteinase K before incubating at 56C for 3h. Buffer B3 (NucleoSpin Tissue KitMacherey-Nagel) was added followed by a heat treatment for 10 min at 70C. After adding 100% ethanol, samples were centrifuged at 11000 x g for 1 min. Two washing steps were performed before eluting DNA. Concentrations and purity were checked using NanoDrop One (Thermo Fisher Scientific). Samples were immediately used or placed at -20C for long-term storage until DNA sequencing. 3.2.2.1 DNA sequencing Microbial cells were disrupted by mechanical lysis using FastPrep-24. Heat treatment and centrifugation were conducted after adding a cooling adaptor. Supernatants were treated with RNase to eliminate RNA. Total DNA was purified using gDNA columns as described in detail previously [157]. Briefly, the V3-V4 regions of 16S rRNA gene were amplified (15x15 cycles) following a previously described two-step protocol [158] using forward and reverse primers 341F-ovh/785R-ovh [159]. Purification of amplicons was performed by using the AMPure XP system (Beckmann). Next, sequencing was performed with pooled samples in paired-end modus (PE275) using an MiSeq system (Illumina, Inc.) according to the manufacturers instructions and 25% (v/v) PhiX standard library. Library preparation and sequencing of the HSCT dataset were performed at the Technische Universitt Mnchen. Briefly, volumes of 600\\(\\mu\\)L DNA stabilization solution (STRATEC biomedical) and 400\\(\\mu\\)L Phenol:choloform:isoamyl alcohol (25:24:1, Sigma-Aldrich) were added to the aliquots. 3.2.2.2 Microbial profiling For the HSCT dataset the processing of raw-reads was performed by using the IMNGS (version 1.0 Build 2007) [160] pipeline based on the UPARSE approach [159]. Sequences were demultiplexed, trimmed to the first base with a quality score &lt;3 and then paired. Sequences with less than 300 and more than 600 nucleotides and paired reads with an expected error &gt;3 were excluded from the analysis. The 5 nucleotides from each end of the remaining reads were trimmed to avoid GC bias and non-random base composition. Operational taxonomic units (OTUs) were clustered at 97% sequence similarity. Taxonomy assignment was performed at 80% confidence level using the RDP classifier and the SILVA ribosomal RNA gene database project. Later the data was normalized using the same method as for RNA-seq described above. The microbiome was visually inspected for batch effects in PCA; none were found. The resulting OTUs table was normalized using edgeR (Version 3.28 or later) [146]. For all the other datasets dada2 [161] (Version 1.14 or later) was used to analyze microbiome data. It creates amplicon sequencing variants from the 16S sequencing data, without merging similar sequences at any threshold. It is an alternative to the use of OTUs which allows to compare results between studies and provides more resolution to identify differences on the fragment of 16S amplified. We used Silva v138.1 to annotate the 16S fragments whenever possible [162]. If we did not have access to the direct ASV we used the annotation provided by the original authors of the dataset. Figure 3.1: Workflow of the main analysis process of the thesis. Created with BioRender.com 3.3 Integration methods 3.3.1 MCIA Multiple co-inertia analysis, also known as MCIA, is a method to examine covariant gene expression patterns between two blocks [163]. It is implemented on the package omicade4. On its core MCIA maximizes the following formula: \\[ \\sum_{k=1}^K w_k {cov}^2 (X_k Q_k u_k, v) \\] where \\(K\\) is the total number of matrices, \\(X_k\\) the transformed matrices and \\(Q_k\\) is a square matrix with \\(r_{ij}\\) in diagonal elements indicating the hyperspace of features metrics, \\(u_k\\) are auxiliary axes, \\(v\\) the reference structure and w the weights of the matrices. This can be used to obtain a dimension of \\(P_k^d=u_k^d(u_k^d Q_k {u_k^d}^T)^{-1} u_k^d Q_k\\) given that for each dimension the residuals are obtained following \\(X_k^{d-1} = X_1^d- X_1^d P_1^{d-1}\\) where \\(d\\) are the dimensions needed. MCIA was used as a baseline method to compare the RGCCA integration. 3.3.2 STATegRa We used STATegRa To explore how much do different blocks of a dataset have in common [164]. It is a framework for integrating datasets with two data types using parametric and non-parametric methods. The methods used are omics component analysis based on singular value decomposition (SVD) of the data matrix. There are three different methods provided to this end: DISCO-SCA, JIVE and O2PLS. DISCO-SCA uses: \\[ X_k = TP_k ^ T + E_k \\] Where \\(T\\) is the \\(I \\times R\\) matrix of components scores that is shared between all blocks and \\(P_k\\) the \\(J_k \\times R\\) matrix of components loadings for block \\(k\\). Let \\(X_1,X_2, \\dots X_i\\) be blocks of data and \\(X=[X_1,X_2, \\dots X_i]\\) represent the joint data, then the JIVE decomposition is defined as: \\[X_i=J_i+A_i+\\epsilon_i \\text{, }i = 1,2, \\dots\\] where \\(J=[J_1,J_2, \\dots J_i]\\) is the \\(p \\times n\\) matrix of rank \\(r&lt;rank(X)\\) representing the joint structure, \\(A_i\\) is the \\(p_i\\times n\\) matrix of rank \\(r_i &lt; rank(X_i)\\) representing the individual structure of \\(X_i\\) and \\(\\epsilon_i\\) are \\(p_i \\times n\\) error matrices of independent entries. Finally, the O2PLS approach uses multiple linear regression to estimate the pure constituent profiles and divides the systematic part into two, one common to both blocks and one not. The O2PLS model can be written as a factor analysis where some factors are common between both blocks. \\[ \\begin{aligned} &amp; \\text{X model : } X = TW ^ T + T_{\\text{Y-ortho}} P ^ T_{\\text{Y-ortho}} + E \\\\ &amp; \\text{Y model : } Y = UC ^ T + U_{\\text{X-ortho}} P ^ T_{\\text{X-ortho}} + F \\\\ &amp; \\text{Inner relation : } U = T + H \\end{aligned} \\] Each model is built similarly by adding the subtraction of the projected values of the other component keeping the relationship between them as stated on the third line. 3.3.3 RGCCA The main method used on this thesis has been regularized generalized canonical correlation analysis (RGCCA) a method derived from the canonical correlation. Canonical correlation is a method that uses data about the same unit but from different origins to find how much the different sources agree. Regularized generalized canonical correlation analysis is implemented on the homonymous package RGCCA [130] which was used here. The method and implementation will be explained in detail in the next section. Figure 3.2: Multiomic relationships on different datasets. Integration methods focus on relationships within datasets. Common relationships between datasets are used as confirmation/validation. Created with BioRender.com 3.4 Regularized generalized canonical correlation analysis To understand the regularized canonical correlation analysis I first provide a brief description of principal component analysis. Principal component analysis defines a new orthogonal coordinate system that optimally describes variance in a single dataset. It does so by decomposing the numerical matrix into the eigenvalues and eigenvectors with decreasing variance. Its results include new variables (the eigenvectors) and a vector of loadings or weight indicating the importance of the original variables for these new variables. Typically, data is normalized and standardized to avoid that a variable with higher variance and different scale dominate the results. The canonical correlation analysis [165, 166] is a method to find agreement between two, or more, scorers (as it was first introduced on the literature) or sources. It extends the PCA to a two sources of data. It provides a similar output, new variables and weights of the existing variables indicating their importance. The function it maximizes is: \\[ \\rho = \\underset{a, b}{\\text{argmax}} (\\text{cor}(a^T X, b^T Y)) \\] On this equation \\(a\\) and \\(b\\) are random variables that given \\(X\\) and \\(Y\\), the two data sources, maximize the correlation between \\(a^T X\\) and \\(b^T Y\\), the first canonical variables, the new variables. \\(a\\) and \\(b\\) are also known as weights or loadings of the variables. Over several years of progress on the field of canonical correlations [114, 115, 167171], regularized generalized canonical correlation analysis (RGCCA) emerged with a generalization from canonical correlations extending the procedure to more than two sources of data [R-RGCCA?] and being made more flexible generalizing from other proposed methods. 3.4.1 Description RGCCA works with numeric matrices that can be as big as needed, as it is designed for datasets with more variables than samples ( \\(p \\gg n\\)). However, for each sample it needs to have a complete case with no missing values (at the time of writing this thesis there is an in-development version, not released on CRAN2 yet, that replaces any missing value by a 0) and in which time is not considered as a special variable. It uses a dimensional reduction approach to relate the different blocks of data between them and produce specific factors for each dataset. The objective function is: \\[ \\underset{a_1,a_2, \\dots,a_J}{\\text{maximize}} \\sum_{j, k = 1}^J c_{jk} g( \\text{cov}(X_j a_j, X_k a_k)) \\text{ s.t. } (1-\\tau_j)var(X_j a_j)+\\tau_j \\Vert a_j \\Vert^2 = 1, j=1, \\ldots, J \\] Being \\(X_j\\) the values from sample \\(j\\), the weights of the variables of said sample are represented by \\(a\\). While \\(g\\) is a function that can take the form of \\(x\\), also known as Horst method, \\(|x|\\) known as centroid method, \\(x^2\\) known as factorial method, or any user-supplied function. \\(C\\) is a symmetric matrix describing the network between blocks. The shrinkage parameter is defined as: \\[ \\widehat{\\lambda}^{\\star} = \\dfrac{\\sum_{i\\neq j}\\widehat{Var}(r_{ij})}{\\sum_{i \\neq j}r_{ij}^2} \\] Where the \\(r_{ij}\\) are the correlation coefficients of the matrix between variables \\(i\\) and \\(j\\) . Where the variance is defined as: \\[ \\widehat{Var}(S_{ij}) = \\dfrac{n^2}{{(n-1)}} \\widehat{Var}({w}_{ij}) = \\dfrac{n}{{(n-1)}^3} \\sum_{k=1}^n ( w_{kij} - \\overline{w_{ij}})^2 \\] And its components are: \\(w_{kij}=(x_{ki}-\\overline{x}_i)(x_{kj}-\\overline{x}_j)\\) and \\(\\overline{w}_{ij}=\\frac{1}{n}\\sum_{k=1}^nw_{kij}\\) representing \\(x_{ij}\\) the values of a sample \\(j\\) on a variable \\(i\\). The authors realized that there is a special problem due to sparsity on biological data which could be handled using first another normalization to improve the stability and success of the canonical correlation methodology. The method to perform the dimensional reduction using the sparse method consists on maximizing the same equation but with a different constraint: Specifically, RGCCA with all \\(\\tau_j = 1\\), combined with an L1-penalty that gives rise to SGCCA: \\[ \\underset{a_1,a_2, \\dots,a_J}{\\text{maximize}} \\sum_{j, k = 1}^J c_{jk}g( \\text{cov}(X_j a_j, X_k a_k)) \\text{~~s.t.~~} \\Vert a_j \\Vert_2 = 1 \\text{ and } \\Vert a_j \\Vert_1 \\le s_j, j=1,\\ldots,J \\] The \\(s_j\\) controls the sparsity estimated of the data, the smaller it is, the higher the sparsity of \\(X_j\\) is. As \\(s_j\\) is closer to 0, more features are selected as it looks to optimize covariance; while if it is closer to 1, less features are selected and the function resembles the correlation. The values of \\(s_j\\) were estimated using the Schfer method [172] when the block included 16S data or RNAseq, otherwise 1 was used. As previously mentioned, there are different \\(g\\) functions that could be used; but the centroid method was chosen to detect both positive and negative relationships. Categorical data was encoded as binary (dummy) variables for each factor except one to keep degrees of freedom, where 0 indicates not present and 1 indicates present. One level was omitted to avoid overfitting the data. Each block, regardless if it had continuous numeric variables or dummy variables was standardized to zero mean and unit variance. Later, it was divided by the square root of the number of variables of the block for an unbiased estimation. 3.4.2 Output RGCCA as other dimensional reduction techniques provides specific weights for each variable on each dimension and a sample score on each dimension, together with quality scores. The most important output are the canonical component of each block. These canonical components To measure the quality of the model, the implementation provides indicators based on the Average Variance Explained (AVE). RGCCA returns an AVE score for each block, which measures how the variables of the block correlate with the dimension component of the block. It also provides two AVE scores of the whole model: the inner, which measures how each dimension accounts for the variance, and the outer, which measures how variables correlate with the dimension components. The closer the inner AVE is to 1, the better the model adjusts to the data. However, that mathematically fits better to the data does not guarantee that the model provides more insights into the biology. It can also generate results as other related methods based on the maximization of a function of correlations: SUMCOR (sum of correlations method) [173], SSQCOR (sum of squared correlations method) [174], SABSCOR (sum of absolute values of the correlations method) [175]. Others methods are based on the maximization of a function of covariances: SUMCOV (sum of covariances method), SSQCOV (sum of squared covariances method), SABSCOV (sum of absolute values of the covariances method). The following table summarizes the equivalent parameters needed on RGCCA to work: Table 3.6: Equivalences of RGCCA for multi-block data analysis to other methods Method Scheme Normalization Shrinkage SUMCOR Horst \\(Var(X_j a_j) = 1\\) 0 SSQCOR Factorial \\(Var(X_j a_j) = 1\\) 0 SABSCOR Centroid \\(Var(X_j a_j) = 1\\) 0 SUMCOV Horst \\(\\Vert a_j \\Vert = 1\\) 1 SSQCOV Factorial \\(\\Vert a_j \\Vert = 1\\) 1 SABSCOV Centroid \\(\\Vert a_j \\Vert = 1\\) 1 There are other methods that can be performed with RGCCA, for example: The classical canonical correlation analysis would be equivalent to data variance equal to 1 and shrinkage of 0. Partial least squares (PLS) regression, which maximizes covariance, would be equal to data variance of 1 and shrinkage of 1 in RGCCA. Finally, redundancy analysis could be performed where one blocks weight normalized value is 1 and the variance of the other equals to 1. 3.4.3 Models There is no formal definition of what constitutes a block of data on multi-omics tools. Most multi-omics and integration tools assume one block for each type of data, such as an essay a survey or an experiment. We decided to split the block with the variables about the samples to separate independent variables. The hypothesis we made was that more blocks with highly related variables but independent from the other blocks would fit better the data and thus help to identify causal or dependent variables. To model what might be the relationships within datasets current practices include using a pre-selected model of relations between blocks (See figure 3.2). However, this model might not be an accurate representation of the relations between blocks and several models might need to be fitted. To help find the fitting model for the data I created an R package, named inteRmodel, which helps finding the right model for the dataset via a bootstrapping procedure. This method was applied to the previously described datasets to find the relationship between microorganisms and the disease. Following this method; to provide a ground truth, a model with only the relationships between the two experimental obtained data is analyzed, on what it is called the model 0. The next models analyzed consisted on relationships between the two experimental blocks and a block with all the metadata of the samples. These models are denoted by 1.Y, where 1 denotes the family 1 and Y is used to label some of the models of this family. Later instead of a big metadata block, following our theory we split this metadata block on several ones, having a block for time related variables, another one for location and the other about the people on the study. This allows to design a model with an expected relationships between these blocks and makes more interpretable the relationships. These models are from the family 2 denoted by the name 2.Y, where 2 denotes the family of the model and Y change for particular models with different relationships between the blocks change. For each family of models we tested all possible relationships with weights between 0 and 1 by 0.1 intervals to find the best model on each datasets according to the AVE score. The final models were further validated using a bootstrap approach to measure their accuracy and likelihood on the data available. 3.5 Statistics Differential expression analysis was performed with the limma-trend method [176, 177] and edgeR [146] (Bioconductor version 3.10 or superior) packages. Data was normalized using the trimmed mean of M-values and log-2 transformed into counts per millions following the workflow previously described using voom [178]. To correct for multiple testing, the false discovery rate was estimated using the method of Benjamini and Hochberg [179]. A gene was considered differentially expressed when it was significant at 5% FDR. Special attention was given to those genes that showed a fold-change higher than |1.5|. 3.6 Functional enrichment methods Functional enrichment methods are those methods that aim to provide with more information about the variables besides their numerical value measured. They can be very different in nature but they all use the numeric values of the variables and other information, being it from the same experiment data collection or from external data sources. Many functional enrichment methods are based on an over representation analysis, where a group of elements is tested for their mesure in other groups. This can be done with clusterProfiler which tests genes enrichment for functionality based on information on pathway databases [180] that it is used in several publications [181]. clusterProfiler checks the enrichment of features of a given group on the (background) list provided. \\[ \\begin{aligned} H_0 : P_{subset} \\leq P_{overall} \\\\ H_1 : P_{subset} &gt; P_{overall} \\end{aligned} \\] The statistical test used is usually the fisher test, the hypergeometric test or the proportion test. I describe the hypergeometric test and the proportion test below. 3.6.1 Fisher test The fisher test calculates how independently are two categories. Say if genes in category X are independent of category Y, if we use the contingency table Table 3.7: Fisher contingency table In category X Not in category X Row total In category Y a b a+b Not in category Y c d c+d Column total a+c b+d a+b+c+d = n Given this contingency table 3.7, the probability of X and Y being independent can be calculated with: \\[ \\begin{aligned} P &amp; = \\dfrac{ \\binom{a+b}{a} \\binom{c+d}{c} }{\\binom{n}{a+c}} \\\\ &amp; = \\frac{(a+b)!~(c+d)!~(a+c)!~(b+d)!}{a!~~b!~~c!~~d!~~n!} \\end{aligned} \\] 3.6.2 Hypergeometric test The hypergeometric distribution describes the probability of \\(k\\) successes (when the object drawn has a specified feature) in \\(n\\) draws3, from a finite population of size \\(N\\) that contains exactly \\(K\\) objects with that feature, wherein each draw is either a success or a failure. In this context \\(N\\) is the number of genes being used and \\(n\\) the number of genes on a pathway. It can be used to compare the genes found on a pathway (\\(k\\) genes) compared to the expected \\(K\\) numbers of the distribution using the following equation: \\[ P_X(k) = P(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}} \\] This looks like the fisher test, because hypergeometric test assesses the extremeness of observing \\(k\\) or more of that overlap (\\(a\\) on ) and thus it is the same as a one-sided Fishers exact test. 3.6.3 Proportion test The overrepresentation of a given group of elements can also be tested with the proportion test, which is sometimes also used on clusterProfiler. The proportion test uses the \\(\\chi^2\\) distribution to test if the observed frequency (\\(O_i\\)) is close to the expected frequency (\\(E_i\\)): \\[ \\chi^2 = \\sum_{i =1}^n \\dfrac{(O_i - E_i)^2}{E_i} \\simeq \\chi_{n-p}^2 \\] As this is usually done on a 2x2 contingency table it is equivalent to the Z-test of proportion. Sometimes, the expected frequency is so low that a correction must be done to the estimation: \\[ \\chi_{\\text{Yates}}^2 = \\sum_{i =1}^n \\dfrac{(|O_i - E_i | - 0.5)^2}{E_i} \\] This increases the \\(p\\)-value as it raises the Chi-square statistic. 3.6.4 Gene Set Enrichment Analysis There are other methods that test if some variables show an unexpected importance according to a statistic like fold change or value, such as gene set enrichment analysis (GSEA) [182]. GSEA is a computational method originally developed to determine whether a priori defined set of genes shows statistically significant and concordant differences between two biological states. This methods check if a group of variables present in an ordered list shows a skewed distribution and it compares it against a random group of similar size. It has been widely used since its original publication, also on IBD [183]. This method calculates the rank of genes \\(rank(g_j)=r_j\\) where each \\(g\\) is a gene, and then it calculates the following functions: \\[ \\begin{aligned} &amp; P_{hit}(S, i) = \\sum_{g_j \\in S, j \\leq i}\\dfrac{|r_j|^p}{N_R} \\text{, where } N_R = \\sum_{g_j \\in S}|r_j|^p \\\\ &amp; P_{miss}(S, i) = \\sum_{g_j \\not \\in S, j \\leq i}\\dfrac{1}{N - N_H} \\end{aligned} \\] With these values the enrichment score (ES) defined as: \\(ES=max(|P_{hit}(S, i)-P_{miss}(S, i)\\vert)\\) is calculated from the walk. At least 1000 permutations are usually used but a high number of permutations are required for an accurate estimation of the enrichment score. However, when more than one pathway (\\(S\\)) is evaluated in order to compare between their enrichment scores, they must be normalized by dividing the scores by the mean of all the ES. When power \\(p\\) is 0 it is equivalent to the standard KolmogorovSmirnov statistic, though it is usually set to 1. For testing GSEA I used fgsea [184] implementation for its speed and integration with other methods used in this thesis. Gene pathways from the REACTOME database were tested on the weight of different models or on the comparisons performed [185]. 3.6.5 GSVA To estimate the expression of the pathways and compare their expression levels between conditions gene set variation analysis as implemented on GSVA was used [186]. It is a method that summarizes the variables numerical value changing the space of \\(\\text{variable x sample}\\) to \\(\\text{group x sample}\\) . This enables other methods to use this new space instead of the original variables, which provides a successful way to look into data [187]. GSVA was used (again from the REACTOME database) to find the relationships between the pathways and the microbiome at different taxonomic levels. This is done via an estimation and a comparison with a discrete Poisson kernel: \\(i\\) indicates the gene from a total of \\(k\\) genes, and samples are indicated by \\(j\\) from \\(n\\) number of samples. \\[ z_{ij}=\\hat{F_r}(x_{ij}) = \\frac{1}{n}\\sum_{k=1}^n\\sum_{y=0}^{x_{ij}}\\dfrac{e^{-(x_{ik}+r)}(x_{ik} + r)^y }{y!} \\] \\(r = 0.5\\) is used to set the mode of the Poisson kernel at each \\(x_{ik}\\), that is, similar to the expression of a gene for a given sample. Later this is converted to ranks \\(z_{(i)j}\\) for each sample and normalized: \\(r_{ij}=|\\frac{p}{2}-z_{(i)j}|\\) to make the distribution of ranks symmetric around zero to later compare with a normal distribution using a Kolmogorov-Smirnov-like random walk statistic: \\[ v_{jk}(l)=\\dfrac{\\sum_{i=1}^l|r_{ij}|^{\\tau}I(g_{(i)}\\in \\gamma_k)}{\\sum_{i=1}^p|r_{ij}|^{\\tau}I(g_{(i)} \\in \\gamma_k)} - \\dfrac{\\sum_{i=1}^lI(g_{(i)} \\not \\in\\gamma_k)}{p-|\\gamma_k|} \\] Here \\(\\tau\\) describes the weight of the tail in the random walk (default is set to 1). \\(\\gamma_k\\) is the k-th gene set and \\(I(g_{(i)}\\in \\gamma_k)\\) is the indicator function whether the gene ranked i-th belongs to the gene set \\(\\gamma_k\\) . \\(|\\gamma_k|\\) indicates the ordination of the gene set, the number of genes of the gene set and \\(p\\) the number of genes in the dataset. This difference is later converted to enrichment score for each gene set for each sample, similar to GSEA. This score can be calculated as a difference of hits and misses or the maximum deviation from zero of the random walk (which allows to detect gene sets that have genes with different opposing expression patterns). 3.7 Variance and diversity methods 3.7.1 PERMANOVA The PERMANOVA method [188, 189], provided by the vegan package on the adonis function, was used to test if microbiome data variance is due to other variables when using distances metrics. It uses the residual sum of squares such as: \\[ SS_W = \\frac{1}{n} \\sum_{i=1}^{N-1}\\sum_{j=i+1}^N d_{ij}^2\\epsilon{ij} \\] When using euclidian distances (\\(d\\)) it is equivalent to MANOVA. Here \\(\\epsilon_{ij}\\) takes the value of 1 if the observation \\(i\\) and the observation \\(j\\) are in the same group, otherwise it takes the value of 0. This can be later used to test which variance is bigger, inter-groups or intra-groups by using the following formula: \\[ F = \\dfrac{SS_A/(\\alpha -1)}{SS_W/(N-\\alpha)} \\] Where \\(SS_A\\) is the among group sum of squares, representing the intra-group variance. \\(N\\) is the number of samples and \\(\\alpha\\) the number of different groups. This allows to test if the variables are related to the variance of the data as it can be compared with the \\(F\\) statistic after a high number of permutations. 3.7.2 globaltest It is a method for testing complex hypothesis and calculate the influence of each variable on a given outcome [190]. I tested which variables, (sex, age, location, time since diagnosis, treatment) are important on the datasets with globaltest (Version 5.40 or later). This method provides a general statistic to test hypothesis against a high dimensional dataset. \\[ S = \\sum_{i=1}^p x_i^{&#39;} x_i g(t_i^2) \\] The global test performs a test statistic on the transformed t-test, where if \\(p\\), the number of variables, is large the test is more powerful on average over all possible sparse alternatives of general functions \\(g\\). It was performed with variables individually and also with interactions between the different variables. 3.7.3 Diversity indices Microbiome diversity was measured using vegan and phyloseq methods [191]. \\(\\alpha\\)-diversity is a measure of how much a given microbiome at a taxonomic level is present on a sample. Several measures exists, on the thesis I used the effective Simpson or effective Shannon diversity index to compare diversity between samples and conditions. \\(\\beta\\)-diversity was calculated using the phyloseq package for exploratory analysis. The effective Simpson (also known as inverse Simpson) and the effective Shannon are: \\[ \\begin{aligned} &amp; D_{\\text{effective Simpson}} = \\dfrac{1}{\\sum_{1=1}^Sp_i^2} \\\\ &amp; D_{\\text{effective Shannon}} = \\dfrac{1}{-\\sum_{i =1}^S p_i \\log_e{p_i}} \\end{aligned} \\] Where \\(p_i\\) is the proportion of species \\(i\\) and \\(S\\) is the number of species. 3.8 Other methods 3.8.1 WGCNA To look for relationships between the microbiome and the RNAseq we used weighted gene co-expression network analysis. We used weighted gene co-expression network analysis as implemented on WGCNA [192] as well as correlations. The Spearman rank correlation coefficient is: \\[ R_s(X,Y) = \\dfrac{\\sum_{i=1}^n (R_i - \\bar{R}) (S_i - \\bar{S} )}{\\sqrt{\\sum_{i=1}^n (R_i - \\bar{R})^2}\\sqrt{\\sum_{i=1}^n (S_i - \\bar{S})^2}} \\] Being \\((X_1 , Y_1 ),\\dots, (X_n , Y_n)\\), assign a rank where \\((R_1 , S_1 ), \\dots , (R_n , S_n )\\) for \\(n\\) being all the variables where \\(\\bar{R}=\\dfrac{1}{n}\\sum_{i=1}^n R_i\\) and likewise \\(\\bar{S}=\\dfrac{1}{n}\\sum_{i=1}^n S_i\\). The distribution of the Spearman correlation coefficient is symmetric around 0 and can be approximated to a normal distribution as \\(\\sqrt{n-1}R_{s(X,Y)} \\sim N(0,1)\\) which can be used to calculate the p-value of a given estimation. 3.8.2 BaseSet BaseSet was developed and used to find which variables are really involved on the interaction and how likely they were to be together. It is a package that uses fuzzy set logic to calculate the probability to belong to a group, in this case, those genes and bacteria selected by the model that interact with the other. Under the standard fuzzy set logic a set \\(S\\) is a group of elements for which each element \\(e\\) has an \\(\\alpha\\) membership to that set [193, 194]. \\(\\alpha\\) is usually bounded between 0 and 1: \\(\\alpha \\in [0, 1]\\) . A given element \\(e\\) can belong to more than one set. Assimilating the membership function to probability we can calculate the probability of a given element \\(e\\) to belong to a set \\(S\\) and not any other set: \\(P(e \\in S|e \\not \\in S^c)\\). Which applied to the data and the case at hand, it is the probability that a given variable is associated with a given outcome and not with any other outcome. The membership function was derived from the bootstraps used for each model on the thousand iterations of the integrative method applied to give an estimation of how probable is a given gene and bacteria to be selected as relevant for the model. The bootstraps of the models where used to calculate the probability of a variable to be selected by RGCCA. This probability was used to calculate (via set_size) the genes and bacteria that are specific of the model that allows to separate the transcriptome by its location and the microbiome by the disease status. 3.8.3 experDesign experDesign was developed [195] to prevent and quantify if a given experiment has batch effect due to the batches used to measure the values or other known variable. It might help to detect a bad design of the experiment. On pseudo code the core of the program can be described as: for each index: for each batch pick size(batch) samples if samples are in another batch pick other samples for each batch calculate some summary statistics compare with the summary statistics of all the samples keep the index with less differences between the index and all the samples Summary statistics taken into account are the median, the variance, the range, the number of missing values, and the entropy of the categorical variables. It can take into account spatial distribution and, given the number of samples that fit on a batch, provide which technical replicates4 are best to use. 3.8.4 ROC- AUC To estimate if the selected features (genes or microorganism) by the integration methods have some biological meaningful contribution I measured if they can classify features, such as, which gastrointestinal segment is each sample from, or which type of disease does each patient have. To compare between different models the area under the curve (AUC) of the receiver operating characteristic (ROC) was calculated with the pROC package [196]. It is based on the following formulas, where \\(\\text{FP}\\) is false positive, \\(N\\) is a negative, \\(P\\) is positive, \\(\\text{TP}\\) is true positive and \\(\\text{FN}\\) is false negative: \\[ \\begin{aligned} FPR = \\frac{FP}{N} = \\dfrac{FP}{FP+TN} \\\\ TPR = \\frac{TP}{P} = \\dfrac{FP}{FP+FN} \\end{aligned} \\] The ROC curve is that where the true positive rate (TPR) or sensitivity, recall or hit rate is represented against the false positive rate (FPR) on the abscissa. The area under this curve is a measure of how good such classifications performs overall, being 0.5 as good as a random selection. The closer it is to 1 the better as it classifies incorrectly less samples and accurately classify more. References "],["results.html", "Chapter 4 Results 4.1 Packages/methods 4.2 Analysis", " Chapter 4 Results 4.1 Packages/methods 4.1.1 experDesign experDesign package built in R was released for the first time on CRAN on 2020-09-08 after nearly a year after the initial release made on github. After peer review it was published on a journal on 2021-11 [195]. The package uses functional programming to create and modify objects and the features used. The package bases its performance on the large body of work made by the R core team. It adds the information to the introduced data.frame or returns an vector with the appropriate information. experDesign functions are divided into several categories: Helper functions to aid on deciding how many batches are or how many samples per batch. There are some also that report how good a given distribution of the samples felt for a given dataset. Functions generating indexes. Functions distributing the samples on indexes Figure 4.1: experDesign flowchart for users of the package showing which functions can be used depending on the experiment design they have. Regarding time related variables experDesign will use them as factors, while issuing a warning to the user. Since its development it has been used on a couple of RNA sequencing experiments that required a batch design, one of organoids bulk RNA-seq (data not related to this thesis) and another one of biopsies bulk RNA-seq from the BARCELONA cohort (See appendix section D.1). It was also used to check if there is any observable batch effect on the datasets analyzed. On the designed datasets experDesign avoided batch effects from the sequencing process. However, on the organoids dataset, a change on the matrigel used to produce them introduced a batch effect that made it impossible to compare samples before and after that change (there were not any shared sample before and after the change of matrigel). On the BARCELONA cohort there were other problems described on the appendices (section D.1). Since its release on CRAN it has had a median of ~400 downloads each month from RStudio repository mirror, showing the interest the community have on solutions like this. 4.1.2 BaseSet BaseSet package, built in R, was released for the first time on CRAN on 2020-11-11, nearly two years after the initial work started on github. The package uses both functional programming and object oriented program to create and modify the TidySet S4 object defined5. Mixing it with S3 generic functions it provides a powerful interface compatible with the tidyverse principles, a group of packages following the same design. The package provides a new class to handle fuzzy sets and the associate information. BaseSet methods are divided into several categories: General functions to create sets of the TidySet class or convert from it to a list or about the package. Set operations like adjacency cartesian product, cardinality, complement, incidence, independence, intersection, union, subtract, power set or size. Functions to work with TidySets to add relationships, sets, elements or some complimentary data about them. Remove the same or simply move around data or calculate the number of elements, relations and sets. Functions to read files from formats where sets are usually stored in the bioinformatician field: GAF, GMT and OBO formats. Last, some utility functions to use set name conventions and other auxiliary functions. The package had a long development process with initial iterations based on GSEABase package which was later abandoned (GSEAdv) to also include some uncertainty on the relationship of a gene with a given gene set. The package formed part of an exploration of the Bioconductor community (project to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assay) for more modern and faster handling of sets than GSEABase. There were three different packages created as part of this process, BaseSet, BiocSet released on Bioconductor and unisets, available on github. The three different approaches were presented at a birds of feather on BioC2019, the annual conference of Bioconductor on 2019. The package passed the review on the rOpenSci organization (See review) and is now part of the packages hosted there too. Since its release on CRAN it has had a median close to ~400 downloads each month from RStudio package manager. 4.1.3 inteRmodel The package was build once the method used to find accurate models of the relationships of the data available of a dataset using RGCCA was established. Using the package on github simplifies the process and makes easier to redo the model optimization used on this thesis. The package has functions that can be grouped in three categories: Look for models and evaluate them: To search for a model given some conditions, such as that all the blocks are connected, and check the models via bootstraping or leave one out procedures. Reporting: To make better reports by improving handling of names or simplifying the objects or how to calculate scores. Building: To easier build correct models on RGCCA, simplifying the process to create a symmetric matrix. Currently it is only available on github, so the number of downloads and usage is unknown but since its release a user has contacted to keep it up to date with development versions of RGCCA. Currently, it is compatible with the next release of RGCCA being prepared6. The functions analyze helps to analyze code of a single integration, providing the results on a tidy format for further processing. To create the connections between blocks the function weight_design is available. It creates all the possible matrices given a number of blocks and a number of weights. Optionally it can create just a subset of those based on a numeric vector. However, it does not provide a way to have the design named. If the user wants to create their own design matrices, they can use symm and modify the design of the model with subSymm. symm, takes an initial matrix to pick up the row and column names. It is recommended that the user checks the design matrix is fully connected, which the package facilitates with the function correct. This is also recommended even if the design matrices are created with weight_design. To search models search_models starts with a initial connectivity of the blocks and creates all the combinations of connections given. For the bootstrap procedure there is the function boot_index to create the bootstrapped index of samples to be used by boot_index_sgcca. boot_index randomly selects as much samples as specified by the arguments to create as much indices as the required by the second argument. If the bootstrapped samples used is not important one can use boot_samples_sgcca. If the users want to perform a leave-one-out procedure they can use looIndex. Figure 4.2: Functions provided by the inteRmodel package to search and validate models of relationships using RGCCA. Created with BioRender.com For more information, you can access the manual online or once it is installed. The package cannot choose which variables use from the block with information to split into several blocks. However, it provides the model_RGCCA function to make it easy to prepare such variables for RGCCA input. The inteRmodel procedure will only be useful if the user should decide which variables are independent from others and split them into different blocks. To asses this the user can use the methods we used, as described on the above section 3.7. It is important to keep in mind the possible causal relationships on users data [198]. 4.2 Analysis On the following sections the main results of analyzing each dataset are presented. 4.2.1 Pugets dataset On this dataset the different parameters and capabilities of RGCCA were tested. The three different methods, centroid, factorial or horst were tested and compared. The main result of this comparison was that the differences of the selection of the variables mattered more than the number of variables selected with each method. The models were tested with different weights on all three schemes: horst, centroid and factorial. The horst and the centroid scheme were similar while the factorial resulted in the most different AVE values (see S1 Data of [199]). The centroid scheme was selected because it takes into account all the relationship regardless of the sign of the canonical correlation between the blocks. It is similarity to horst scheme. The effect of the sparsity value was measured by its effect on the inner AVE scores and the combination of the different values for each block as can be seen on Figure 4.3. Figure 4.3: Effect of tau on the inner AVE on Pugets dataset. The suggested tau value is the column between the regular grid, on the ordinate axis the ys tau values and on the abscissa the gene expression (GE) on the left and the comparative genomic hybridization (CGH) on the right. The highest inner AVE is with high tau values for y and middle to upper values for GE and CGH. The first model of the family of models 1 can be seen on Table 4.1: Table 4.1: Model 1 for Pugets dataset. Relationships between the different blocks on the Puget dataset for model 1. 0 indicates no relationship and 1 indicates a strong relationship. Model 1 GE CGH Localization GE 0 0 1 CGH 0 0 1 Localization 1 1 0 When looking for the model that adjust better following this structure we arrived to model 1.2, described below (Table 4.2) : Table 4.2: Model 1.2 for Pugets dataset. Relationships between the different blocks on the Puget dataset for model 1. 0 indicates no relationship and 1 indicates a strong relationship. Model.1.2 GE CGH Localization GE 0 0.0 1.0 CGH 0 0.0 0.1 Localization 1 0.1 0.0 On model 2 we split the invariable variables from those related to the location (Table 4.3): Table 4.3: Model 2 for Pugets dataset. Relationships between the different blocks on the Puget dataset for model 1. 0 indicates no relationship and 1 indicates a strong relationship. Model.2 GE CGH Invariable Localization GE 0 1 1 1 CGH 1 0 1 1 Invariable 1 1 0 0 Localization 1 1 0 0 Following this split, the model that has higher inner AVE for these blocks is the following (Table 4.4): Table 4.4: Model 2.2 for Pugets dataset. Relationships between the different blocks on the Puget dataset for model 1. 0 indicates no relationship and 1 indicates a strong relationship. Model.2.2 GE CGH Invariable Localization GE 1 1/3 0 1 CGH 1/3 0 1/3 0 Invariable 0 1/3 0 0 Localization 1 0 0 0 If we added a superblock with all the data of the different blocks from model 1 we started with the standard relationship between blocks (Table 4.5): Table 4.5: Relationships between the different blocks on the Pugets dataset for model superblock. 0 indicates no relationship and 1 indicates a strong relationship. Model.superblock GE CGH Superblock Localization GE 0 0 1 0 CGH 0 0 1 0 Superblock 1 1 0 1 Localization 0 0 1 0 But when the best model with the superblock that had highest inner AVE is quite different (Table 4.6): Table 4.6: Relationships between the different blocks on the Pugets dataset for model superblock.2. 0 indicates no relationship and 1 indicates a strong relationship. Model.superblock GE CGH Superblock Localization GE 1 1/3 0 1 CGH 1/3 0 1 0 Superblock 0 1 0 0 Localization 1 0 1 0 Exploratory analysis with the superblock model was done. The first two components of the superblock did not help to explain the biology or classify the tumors (See 4.4): Figure 4.4: First components of the superblock which has all the data of the samples on the Pugets dataset. The same data was used to look for a good model from the data itself including a model with a superblock but looking at the first component of the CGH and transcriptome block. This allowed to visually inspect if each models components helped to classify the samples (Figure 4.5): Figure 4.5: Different models tried with the same data showing the first components of the CGH data and the transcriptome. Model 1 and 1.2 with transcriptomics, CGH data and all the data about the samples together. Model 2 and 2.2 with transcriptomics, CGH data and all the data bout the samples on different blocks. Model superblock and superblock.2 have all the data in different blokcs and one block with all the data. The first components of the CGH and the transcriptomics blocks of the superblock.2 model show better classification than that of the superblock. However, the other models show a better classification of the samples with much simpler models. To find these models the three blocks with the best tau and the centroid scheme were analyzed by changing the weights between 0 and 1 by 0.1 intervals. According to the inner AVE, the best model was the one in which the weights (1) between the host transcriptome and location, (2) the host transcriptome and the CGH, and (3) the CGH block were linked to variables related to the location with weights of 1, 0.1 and 0.1, respectively. When we added a superblock to the data, there was a slight increase of 0.01 on the inner AVE of the model (See Table 4.7). The model with the superblock that explained most of the variance was that in which the weights of the interaction within (1) the host transcriptome, (2) between the superblock and the CGH, (3) between the host transcriptome and the localization, and (4) between CGH and the host transcriptome were 1, 1, 1 and 1/3, respectively (See table 4.6. To see if the superblock could classify the sample by location, we plotted the first two components of the superblock. We can clearly see that they do not classify the samples according to the location of the tumor, which is known to affect the tumor phenotype [150]. Adding one block containing the age of the patient and the severity of the tumor to the model, decreased the inner AVE. The best model with these blocks, according to the inner AVE, was that in which the interactions (1) within the host transcriptome, (2) between the host transcriptome and the localization, (3) between the host transcriptome and(4) the CGH and between the CGH and the other variables were 1, 1, 1/3 and 1/3, respectively. The first components of each model can be seen in the figure: We can observe on the figure 4.5, the strong dependency between gene expression and location since the first model while the weaker relationship with the CGH assay [150]. On the other hand, the major difference is the dispersion on the CGH component on each model. The effect of the superblock and weights on different models to the inner AVE. There are significant differences between having the superblock and not having it. Figure 4.6: Effect of superblock and weights on the inner AVE on Pugets dataset. Designs with the superblock showed higher inner AVe scores than without it. Interaction yes/no indicates RNA and RNA interaction. The different models resulted on the following AVE values as reported on 4.7: Table 4.7: AVE values of RGCCA models on Pugets dataset. Values for both inner and outer AVE of the first canonical component of models 1, 1.2, 2 2.2 and superblock and superblock.2 are shown. Model inner AVE outer AVE 1 0.6333592 0.0692097 1.2 0.8512360 0.0692319 2 0.2791546 0.0738695 2.2 0.6902329 0.0692707 superblock 0.7055847 0.0734578 superblock.2 0.8047477 0.0695821 4.2.2 HSCT dataset The permanova analysis was performed on this dataset to estimate which were the variables that are more relevant. From the many variables the location, sex, patient id and others were found to be related to the variability of the microbiome or the transcriptome on this dataset. With the permanova analysis we found that more of the 50% of the variance of normalized RNA-seq data and microbiome data respectively is explained by the variables of location, disease, sex, and the interaction between disease and sex. On the transcriptome the most important factor is location which is more than 15% of the variance, while on the microbiome data the most important factor is the patient id followed by location of the sample. Table 4.8: Permanova analysis of transcriptome. The variables and their interactions (shown with :) and the \\(R^2\\) values and the associated p-value. The higher the \\(R^2\\) the more variance is explained by that factor. Factor R p-value Location 0.18057 0.00020 IBD 0.03120 0.00020 Sex 0.01306 0.00120 IBD:Sex 0.01279 0.11798 Location:IBD 0.02427 0.11458 Location:Sex 0.00816 0.03519 Location:IBD:Sex 0.00486 0.52190 Residuals 0.72508 Table 4.9: Permanova analysis of microbiome. The variables and their interactions (shown with :) and the \\(R^2\\) values and the associated p-value. The higher the \\(R^2\\) the more variance is explained by that factor. Factor R p-value Location 0.06061 0.0002000 IBD 0.04967 0.0002000 Sex 0.01712 0.0003999 IBD:Sex 0.01091 0.6604679 Location:IBD 0.02089 0.8476305 Location:Sex 0.01139 0.0075918 Location:IBD:Sex 0.00289 0.9994001 Residuals 0.82652 With globaltest the results were similar: sex, ibd, location, age and time since diagnosis were able to explain the SESCD score (p-value \\(5.7 \\cdot 10^{-21}\\) ). The resulting p-value was well below the 0,05 threshold defined for RNA-seq data on the models including the segment of the sample, sex and treatment. On the microbiome data the results were similar but the p-value was considerably higher but still below the threshold. Figure 4.7: Microbiome diversity on HSCT dataset. On the upper section the Shannon effective and on the lower row the Simpson effective diversity splitted by colon and ileum and controls and CD. Diversity indices of the samples were explored and compared for several subsets. Splitting by location of the sample and disease provided the highest differences and the diversity index along time did not change much. Weighted gene co-expression network analysis did not provide relevant links between bacteria and transcriptome as it failed to find an acceptable scale free degree. As can be seen on the Figure 4.8, the scale free topology does not reach the recommended threshold of 0.9 and the mean connectivity is also very low even for the first power. Figure 4.8: Power evaluation of WGCNA of the HSCT dataset. On the ordinate the power on the abscissa on the left the scale free topology model fit; on the right the mean connectivity. There is a low fit even on large power and the mean connectivity is below 100 from the very first value. STATegRa was used between stool 16S data and intestinal 16S data under the assumption that there is a shared common factor without influence of other categorical variables. However, it did not find a good agreement between these two data sources and 16S data source was not longer used on the analysis. In addition, the model is fixed, so it did not allow to find new or other relationships that are not one to one. With RGCCA we could select different models and use all the data available without much assumptions. The models with the highest inner AVE of the family 1 and the family 2 models were similar to those on the Hsler dataset. Figure 4.9: On the abscissa the transcriptome, on the ordinate the Microbiome. Each square represents a different model of the HSCT dataset. On panel A colored by disease status, on panel B colored by sample location. Model 0 has only transcriptome and microbiome data, models 1 to 1.2 with data about the samples and models 2.1 to 2.3 with data about the samples split in 3 blocks. The weights of these models can be observed here: Table 4.10: Relationships between the different blocks on the HSCT dataset for model 0. 0 indicates no relationship and 1 indicates a strong relationship. Model 0 Transcriptome Microbiome Transcriptome 0 1 Microbiome 1 0 If we include the information about the samples all together in a block called metadata we can start from this model on 4.11: Table 4.11: elationships between the different blocks on the HSCT dataset for model 1.1. 0 indicates no relationship and 1 indicates a strong relationship. Model 1.1 Transcriptome Microbiome metadata Transcriptome 0 0 1 Microbiome 0 0 1 metadata 1 1 0 When looking for the model that adjust better following this blocks we arrived to model 1.2 thanks to inteRmodel, described below on table 4.12: Table 4.12: Relationships between the different blocks on the HSCT dataset for model 1.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 1.2 Transcriptome Microbiome metadata Transcriptome 0 0.0 1.0 Microbiome 0 0.0 0.1 metadata 1 0.1 0.0 On model two we split the invariable variables from those related to the location: Table 4.13: Relationships between the different blocks on the HSCT dataset for model 2. 0 indicates no relationship and 1 indicates a strong relationship. Model 2 Transcriptome Microbiome Demographic Location Time Transcriptome 0 1 1 1 0 Microbiome 1 0 1 1 0 Demographic 1 1 0 0 1 Location 1 1 0 0 0 Time 0 0 1 0 0 Following this split, we used inteRmodel (See section above 4.1.3) to find the model that has higher inner AVE for these blocks is the one on table 4.14: Table 4.14: Relationships between the different blocks on the HSCT dataset for model 2.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 2.2 Transcriptome Microbiome Demographic Location Time Transcriptome 0 0.0 0.0 1.0 0.0 Microbiome 0 0.0 0.2 0.1 0.0 Demographic 0 0.2 0.0 0.0 0.6 Location 1 0.1 0.0 0.0 0.0 Time 0 0.0 0.6 0.0 0.0 We also tested specifically a model from the family 2.3, which can be seen on table 4.15: Table 4.15: Relationships between the different blocks on the HSCT dataset for model 2.3. 0 indicates no relationship and 1 indicates a strong relationship. Model 2.3 Transcriptome Microbiome Demographic Location Time Transcriptome 0.0 0.1 0.0 1.0 0 Microbiome 0.1 0.0 0.1 0.1 0 Demographic 0.0 0.1 0.0 0.0 1 Location 1.0 0.1 0.0 0.0 0 Time 0.0 0.0 1.0 0.0 0 The best model of the family 2 confirmed a relationship between the host transcriptome and the location-related variables, while the microbiome was associated with the demographic and location-related variables (see figure 4.9 and S2 data of [199]). Overall, we see that the relationships in the model affected the distribution of samples on the components of both the host transcriptome and the microbiome. Table 4.16: The models on the HSCT and their AVE values. For each model the inner AVE and the outer AVE is presented. Model inner AVE outer AVE 0.0 0.3999234 0.1001689 1.0 0.6230190 0.0842333 1.1 0.5678189 0.0848714 1.2 0.7043881 0.0775766 2.0 0.2517363 0.0982050 2.1 0.6940253 0.0940266 2.2 0.8187640 0.0941628 2.3 0.7761846 0.0943938 The different models selected different variables, some of which are shared between models. The most similar models are those that have split the metadata into 3 blocks, followed by those that have the metadata in a single block. In order to analyze the accuracy of the models, one thousand bootstraps were used to integrate the data from the HSCT CD dataset. Each bootstrap had its own dispersion on the variables according to the samples selected, the distribution of the bootstraps used are represented here: Figure 4.10: Dispersion of the bootstraped samples on age and percentage of colon and controls samples. Evaluating the same model on each bootstrap lead to a dispersion on the inner AVE of the model. The lower the dispersion, the more robust the model was to different conditions than in the initial testing. Figure 4.11: Bootstrap of the models 0, 1.2 and 2.2. The point with the black circle is the AVE of the original data. The dispersion is shown by the ellipses. Model 0 and 1.2 have lower inner and outer AVE score, model 2.2 has lower outer score but higher inner value than the bootstrapped. With the bootstrapped models we used BaseSet to estimate the probability that each variable to be relevant for the association with a disease. However, due to big amount of small probabilities when using the BaseSet package to calculate which variables are more relevant it could not provide a good estimation on time. MCIA was applied as a baseline of the integration, the first two components were represented similarly to those of the blocks when using RGCCA 4.12. Figure 4.12: MCIA first two synthetic variables on the IBD related cohorts. In red circles the colon and in blue triangles the ileum. The AUC of classifying the transcriptome in colon or ileum segments was compared between the models (see 4.13 and with MCIA. Figure 4.13: ROC curve of the different datasets with the models from RGCCA and the result with MCIA. In gray the MCIA method and with discontinued lines the models of RGCCA. These models have the following AUC to classify the location of the sample according to the first component of the transcriptome block. Table 4.17: AUC values of RGCCA models of the HSCT dataset classyfing the location of the sample according to the first component of the transcriptome block. From model 0 to model 2.3, the best classification is achieved with model 2.1. Note that this is removing two samples for which the location is unkown. Model AUC 0 0.4537037 1 0.4309414 1.1 0.4639275 1.2 0.5958719 2 0.9450231 2.1 0.9988426 2.2 0.9980710 2.3 0.9969136 On MCIA the AUC for the classification of ileum or colon samples is of 0.9851 once those two samples with unknown location are excluded. This is on par with the models of family 2 as can be seen on the table 4.17. The different models selected different variables as can be seen below: Figure 4.14: Upset plot of the variables selected on each model from 0 to 2.3 showing the intersection between them regarding genes and OTUs. Differences and similarities between the selected features of each model can be observed on Figure 4.14. Genes are very similar between model 0 to 1.2 and between 2 to 2.3, while OTUs are very unique on model 0 and others shared between most models. 4.2.3 Hslers dataset In this dataset, the parameter tau behaved slightly differently than with the previous dataset but the value from the Schfers method for tau was close to the best value. Figure 4.15: Changes on tau on the centroid scheme on the Hsler dataset affect the inner AVE score on the model 1. The panel A shows on the ordinate the RNAseq tau value, the panel B on the right, shows the tau of the microorganism; both of them show the ys tau on the abscissa. Models for Hsler dataset are: Table 4.18: Model 0 of Hsler dataset Model 0 RNAseq micro RNAseq 0 1 micro 1 0 The first model for family 1 is on table 4.19: Table 4.19: Model 1.1 of Hsler dataset Model 1.1 RNAseq micro meta RNAseq 0 0 1 micro 0 0 1 meta 1 1 0 The after optimization of the model of family 1, the best one is on table 4.20: Table 4.20: Model 1.2 of Hsler dataset Model 1.2 RNAseq micro meta RNAseq 0 0.0 1.0 micro 0 0.0 0.1 meta 1 0.1 0.0 The first model for family 2 is on table 4.21: Table 4.21: Model 2.1 of Hsler dataset Model 2.1 RNAseq micro Location Demographic Time RNAseq 0 0.0 1.0 0 0 micro 0 0.0 0.5 1 0 Location 1 0.5 0.0 0 0 Demographic 0 1.0 0.0 0 1 Time 0 0.0 0.0 1 0 After optimization of models of family 1 the best model according to the inner AVE score is on table 4.22: Table 4.22: Model 2.2 of Hsler dataset Model 2.2 RNAseq micro Location Demographic Time RNAseq 0.0 0.1 1 0.0 0.0 micro 0.1 0.0 0 0.1 1.0 Location 1.0 0.0 0 0.0 0.0 Demographic 0.0 0.1 0 0.0 0.1 Time 0.0 1.0 0 0.1 0.0 On table 4.23, we can see here the AVE scores of each of the previous models: Table 4.23: AVE values of RGCCA models on Hslers dataset. The inner and the outer AVE scores of multiple models tested on the Hsler dataset are shown. The model with the hightes inner AVE is model 1.2. Model inner AVE outer AVE 0 0.8217371 0.0961236 1.1 0.7461423 0.1024148 1.2 0.8349410 0.1025486 2.1 0.4980681 0.1008395 2.2 0.7513065 0.1009915 In contrast to the HSCTs dataset (table 4.16), the model with the highest inner AVE was model 1.2 but model 2.2 was close to it (see table 4.23). Model 2.2 has a relationship of 0.1 between microbiome and the host transcriptome and of 1 between the location and the host transcriptome. The microbiome block is also related by a factor of 0.1 with the demographic block and of 1 with the time block. Lastly, the time and the demographic block are related by a factor of a 0.1. In either case the family 1 and family 2 models can correctly separate by sample location (colon or ileum) but not by disease type or inflammation status as can be seen on figure 4.16. Figure 4.16: Models on the Hslers dataset. Model 0 with just the transcriptome and microbiome data. Models 1.1 to 1.2 with transcriptome, microbiome and sample data in a single block. Models 2.1 and 2.2 with transcriptome, microbiome and sample data in multiple blocks. On the A panel colored by disease on the B panel colored by location of the sample. There is no observable cluster of IBD samples and the other samples, showing that on this dataset the differences of the microbiome between the different type of samples are less stark. The classification of samples was very accurate in all the models, specially on model 2.2, see figure 4.17: Figure 4.17: AUC for models with RGCCA on Hslers dataset. The AUC was calculated with the first dimension of the gene expression block ability to predict location of the sample. This accuracy resulted on high AUC values for all the models, as can be seen on table 4.24: Table 4.24: AUC for models with RGCCA on Hslers dataset. The AUC was calculated with the first dimension of the gene expression block ability to predict location of the sample. Model AUC 0 0.8011494 1.1 0.9781609 1.2 0.9977011 2.1 1.0000000 2.2 1.0000000 MCIA was applied as a baseline of the integration and compared to the different models to know which one separates best colon and ileum samples. The result on the first two dimension is shown on figure 4.18: Figure 4.18: MCIA result on Hslers dataset. MCIA first two dimensions of the dataset colored by state, the shape is according to the location of the samples. Shows two vertical groups on the first syntethic dimension according to the location of the samples. MCIAs AUC results was as high as the model 2.2 to classify samples according to their location. It was even better to classify the samples according to the type of sample they are: 0.6248 vs 1 the best AUC from RGCCA that corresponds to model 1.2. 4.2.4 Morgans dataset We tested if results of inteRmodel were consistent on this dataset with the other datasets. The first model we tried is model 0 as in table 4.25: Table 4.25: Relationships between the different blocks on the Morgans dataset for model 0. 0 indicates no relationship and 1 indicates a strong relationship. Model 0 Transcriptome Microbiome Transcriptome 0 1 Microbiome 1 0 We then added the data about the samples as provided 3.1.4, on a simple model as in table 4.26: Table 4.26: Relationships between the different blocks on the Morgans dataset for model 1. 0 indicates no relationship and 1 indicates a strong relationship. Model 1 Transcriptome Microbiome metadata Transcriptome 0 0 1 Microbiome 0 0 1 metadata 1 1 0 When looking for the model that adjust better following this structure we arrived to model 1.2, described below: Table 4.27: Relationships between the different blocks on the Morgans dataset for model 1.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 1.2 Transcriptome Microbiome metadata Transcriptome 0.0 0.1 0 Microbiome 0.1 0.0 1 metadata 0.0 1.0 0 On model two we split the invariable variables from those related to the location (see 4.28): Table 4.28: Relationships between the different blocks on the Morgans dataset for model 2. 0 indicates no relationship and 1 indicates a strong relationship. Model 2 Transcriptome Microbiome Demographic Location Transcriptome 0 1 1 1 Microbiome 1 0 1 1 Demographic 1 1 0 0 Location 1 1 0 0 The model that has higher inner AVE for these blocks is the following: Table 4.29: Relationships between the different blocks on the Morgans dataset for model 2.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 2.2 Transcriptome Microbiome Demographic Location Transcriptome 0.0 0.1 1 0.1 Microbiome 0.1 0.0 0 0.0 Demographic 1.0 0.0 0 0.0 Location 0.1 0.0 0 0.0 Each model is different from previous models. After model 2.2 we looked on the model similar to model 2.3 on HSCT dataset showed but it is the same as in model 2.2. However, it is kept on the further analysis. The different models were not able to separate the samples neither by location or sex. Figure 4.19: First component of the transcriptome and microbiome of models on the Morgans dataset. Model 0 without sample data. Model 1 to 1.2 with all the sample data in a single block and models 2.1 to 2.3 with sample data in several blocks. Panel A shows samples colored by sex and panel B by segment of the sample. There is no clear classification neither by location nor sex on any of the models. Nevertheless, we compared the classification with the MCIA algorithm and still resulted that model 2.2 provide a better classification than MCIA. Table 4.30: The models on Morgans dataset and their AVE values. For each model the inner and outer AVE value is presented. Model inner AVE outer AVE 0.0 0.4735601 0.1098639 1.0 0.6333592 0.1152280 1.1 0.2448234 0.1104746 1.2 0.7868443 0.0422660 2.0 0.4404123 0.1088730 2.1 0.6052598 0.1074900 2.2 0.6895661 0.1081315 2.3 0.6895661 0.1081315 When exploring the bootstraps of the data we found that model 1.2 is highly variable: Figure 4.20: Inner and outer AVE scores of the bootstrapped models 0 1.2 and 2.2. Model 0 does not have sample data. Model 1.2 has microbiome, transcriptonme and sample data in a single block and model 2.2 has microbiome, transcriptome and the sample data split in several blocks. In addition the model 2.2 usually has a lower inner AVE compared to model 1.2. Figure 4.21: AUC for Morgans dataset classificating the localization of the sample according to the first component of the gene expression of the models generated with RGCCA. The area under the curve for these models is: Table 4.31: AUC for Morgans dataset classificating the localization of the sample according to the first component of the gene expression of the models generated with RGCCA. Model AUC 0 0.4969734 1 0.7934971 1.1 0.8161536 1.2 0.5606192 2 0.8546351 2.1 0.8473712 2.2 0.8352646 2.3 0.8352646 With MCIA: Figure 4.22: MCIA firsts dimensions on Morgans dataset. Color and shape is according to the location of the sample. There is some separation by location. If we quantify this separation by the first dimension of MCIA, the AUC is 0.818, which is slightly worst than the models of family 2. 4.2.5 Howells dataset This dataset was processed to confirm the results on the previous datasets. As always first we started with model 0, connecting both the RNAseq and the 16S blocks: Later we look for the best model of family 1 (without looking at any previous model of family 1). This resulted on the following model 4.32: Table 4.32: Relationships between the different blocks on the Howells dataset for model 1.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 1.2 RNAseq 16S metadata RNAseq 0.0 0.1 1 16S 0.1 0.0 0 metadata 1.0 0.0 0 Model 1.2 4.32 was the best according to the AVE score but perform worse when attempting to recreate known biological differences via classifying samples as we can see below 4.23: Figure 4.23: The three main models, model 0, 1.2 and 2.2 on the Howells dataset colored by section colon, ileum and shape according to the disease: square, ulcerative colitis; triangle, normal; circle, Crohns disease. Model 0 has just trancriptomic and microbiome data, model 1.2 has transcriptomic, microbiome and sample data and model 2.2 has transcriptomic, microbiome and sample data split in different blocks. Model 2.2 was selected for further analysis as it describes more accurately the biology of the dataset it. Model 2.2 can be seen on 4.33. Table 4.33: Relationships between the different blocks on the Howells dataset for model 2.2. 0 indicates no relationship and 1 indicates a strong relationship. Model 2.2 RNAseq 16S demographics location RNAseq 0 0 0.0 1.0 16S 0 0 1.0 0.0 demographics 0 1 0.0 0.1 location 1 0 0.1 0.0 Model 1.2 has a 0.1 relationship between the ASV and the transcriptome and 1 between transcriptome and metadata. While model 2.2 has a relationship of 1 between location and transcriptome and demographics and ASV but only of 0.1 between demographics and location. Table 4.34: Relationships between the different blocks on the Howells dataset for model 2.2. 0 indicates no relationship and 1 indicates a strong relationship. Model inner.AVE outer.AVE 0.0 0.7180980 0.1112390 1.2 0.8972258 0.1660267 2.2 0.8433274 0.1659844 Figure 4.24: Bootstrap of the different models on the inner and outer AVE: Model 0 has just trancriptomic and microbiome data, model 1.2 has transcriptomic, microbiome and sample data and model 2.2 has transcriptomic, microbiome and sample data split in different blocks. The bigger points are the models on the original dataset. The bootstrapping showed that model 1.2 has indeed higher inner AVE values than model 2.2 and is more stable than model 1.2. While model 0 shows a high variation according to which samples are selected. Figure 4.25: MCIA method on Howells dataset. The first dimensions separates by location. If we look at the classification of the models on figure 4.26, we can see that models 1.2, 2 and 2.2 classify perfectly the samples by the transcriptome into the location of the sample. Figure 4.26: AUC of the RGCCA models on Howells dataset. The classification of the location of the sample according to the first componenent of the models shown. The AUC of each model can be seen on 4.35: Table 4.35: AUC of the RGCCA models on Howells dataset. The classification of the location of the sample according to the first componenent of the models shown. Model 0, 2 and 2.2 have a perfect classification of the samples to their respective location. model AUC 0 0.6255259 1 0.5974755 1.2 1.0000000 2 1.0000000 2.2 1.0000000 On this dataset we also focused on the most important ASV according to the model 2.2 that were present in more than 2 samples that in total were present in the whole dataset. These ASV were summarized to a single value and then used to calculate the AUC, which was 0.85. The dot product of the ASV and genes were also calculated and used to find out which ASV are related to which genes. 4.2.6 Between datasets The HSCT genes were compared to the Howells genes from model 2.2. There are 3580 selected on model 2.2 on HSCT dataset and 2189 genes on Howells dataset. From them the 1228 genes in common were analyzed for which GO terms and pathways they are enriched. The results is represented on 4.27. Figure 4.27: Significance of pathways on common genes on HSCT and Howells dataset ordered by p-value, the size is according to the number of genes on the pathway found on the dataset. References "],["discussion.html", "Chapter 5 Discussion 5.1 Preliminary steps 5.2 Designing models 5.3 Evaluating models 5.4 Implications", " Chapter 5 Discussion In this chapter we will summarize the main findings in relation to the broad research community and other work as well as the impact of the results to improve further research in the future or be used on clinical environments. 5.1 Preliminary steps Quality of the initial data for integration is important for a valid integration analysis. On RNAseq and 16S analysis the sequencing data quality is important. On sequencing data it is very important to avoid contamination and have enough amount of data for the analysis. To avoid contamination it is very important the protocol how are processed the samples before the sequencing. In addition control samples can be added, a blank to control for extraneous material and a sample with known content to confirm that the sequencing worked correctly. On bulk RNAseq the extraction procedure and sequencing protocol (On Methods 3.2) has been setup many years ago and works very well. Rarely there are problems with bulk RNAseq sequencing and its quality. On the contrary DNA extraction and 16S sequencing was a new procedure. Initially it was done with the guidelines and support of collaborators. Later, we did not include control samples on the first 16S sequencing attempts but after some problems we started adding at least blank samples to identify those bacteria that might come with the solvent or be from a contamination on the sequencing facility. This allowed us to discard contamination sources on later sequencing shipments. Once the 16S data was obtained we initially processed it to obtain OTUs. OTUs were the standard some years back. However, OTUs are not comparable between studies even those that use the same primers. For this reason, and after a suggestion from a reviewer, we moved to process later 16S sequencing datasets to obtain ASV. ASV allow to compare the taxonomic imputation between studies using the same primers. However, comparing 16S taxa ASV or OTUs is hard and selecting the right tool to compare them is important [200]. It is also worth to keep in mind that several species have variable number of 16S rRNA genes. Higher sequences counts of certain 16S might not mean higher abundance of that species compared to another with lower 16S counts. However, the exact copy number status may change even within the same bacterial species, making the correction difficult [201]. It is possible to accurately correct for copy number variation on mock populations where the species and the genomes are known (or at least the 16S rRNA), but harder on samples which content is unknown. Furthermore, the precision of 16S sequencing for classification of taxa is not enough to understand its role on the gut. Adherent invasive cells, are cells with same genetic content that show different behavior [202]. These microbiome is known to be present on patients with IBD. Thus even with the complete sequence play a different role on the health of the patients. Most of these steps were out of my control as they were performed by a lab teammate or by collaborators. Other concerns are currently unavoidable or could not have been found beforehand. 5.1.1 The datasets There is not a methodology to calculate the size of cohorts for interaction or multiomic studies. There are no clear rules about which alpha power or which kind of relations are tested. This might be due to a lack of mathematical background and modelization of the relations on the biological field. Further research on this area might help finding which kind of relationships can be found given certain dataset size. In addition to the highly local and not generalizable interactions that happen on biological samples. Usually size of the datasets is determined from practical reasons: either costs or patient recruitment on protocols. There are some efforts to simulate multi-omic datasets [203]. They usually focus on RNA-seq, ATAC-seq (DNase-seq), ChIP-seq, small RNA-seq, Methyl-seq or proteomics but not 16S or microbiome data. To my knowledge there is no accurate generator of 16S datasets or other microbiome datasets. This made impossible to simulate and compare different tools on a similar synthetic dataset with known relationships. There is not a golden dataset for integration. There are many possible relationships between variables and different ways these relationships might be: regulated DNA, protein recruitment, transcription factors, siRNA regulation. Not a single one can be measured on a sufficiently detailed timescale that it can matter and as a result we are left with trying to guess what are the the relationships between variables. In addition, this is compound by the lack of explanations about how this relationships might come, despite other indications. Comparing different datasets is complicated because each is collected with different goals and processed differently. In addition, there is not a resource were datasets of publications are collected or the relationships between different datasets is not shared. Authors upload their data to different, not centralized, data repositories, such as, gene expression omnibus (GEO), European Genome-Phenome Archive (EGA), European Nucleotide Archive (ENA), among others. Authors might provide the processed data as supplementary material on their articles. Some projects whose primary purpose is providing data for the community establishing their own dedicated sites to store the data [4]. Finding different datasets for very similarly purposes is very difficult. And once it is found there might be batch effects. There are tools to overcome this, but currently only work if they share some features in common [204]. This usually means having undergone similar RNA sequencing procedures. All this made finding annotated, appropriate and similar datasets difficult. We looked up for the most similar datasets in order to be able to confirm our results outside our own cohorts. On this thesis there were two datasets collected and sequenced: the HSCT and the BARCELONA dataset. As explained on methods, the HSCT dataset is a very special cohort of patients undergoing hematopoietic stem cell transplant. This treatment is reserved to patients whose all the other treatment failed and this is the only way to reach remission. This is common on patients with long time CD. These patients are highly characterized and there is follow up data for several years. However, it does not include UC patients and they are not representative of the IBD, or even CD, patients. On the HSCT dataset microbiome features were OTUs. OTUs can not be imputed afterwards to ASV. As such, the annotation of the 16S data is not the same as in the others datasets. The BARCELONA dataset includes samples from CD and UC of an ongoing study. Similar to the HSCT dataset, patients on this study were followed up for up to a year. The patients were younger and had less years with the disease, which could provide with more insights on the initial relationships between the microbiome and the mucosa of the IBD. However, the data quality of BARCELONA was not good enough to make reliable analysis and confidently extract hypothesis or relationships. It is not clear what happened and as a consequence it is not clear how to avoid such problems on the future. It could be a problem of the original DNA extraction and/or of its sequencing process which did not include both positive and negative controls. What happened with the BARCELONA dataset -and the organoids cohort not described here- highlights the importance of data quality checks. On the BARCELONA cohort despite no batch effect or other quality issues with the data as checked with experDesign, the diversity indices made impossible using it. The organoids cohort is a dataset that was sequenced following the experDesign recommendations for distribution of samples on different batches. However, a batch effect was found. The batch effect was created on the lab process before the samples sequencing. Caution and a good relationship with the data sample and data generator is important to discover these kind of problems and ensure the quality of the data. Other datasets used from published sourced were assumed they had already passed enough controls and had a good quality. Nevertheless, they were screened to avoid quality issues. The Pugets dataset dataset provided a good benchmark to test the methods and understand how RGCCA works. It is a completely different type of data (except for the microarray that is similar to the RNAseq). But we were not as much interested on the biology as on learning about the methods and possibilities of integration. Hsler data were obtained using the same sequencing techniques from endoscopic biopsies as our dataset HSCT and Barcelona and it is of the same disease/field. The 16S data was very similar to the The confirmation that the inteRmodel approach works on it it helped to continue forward. The taxonomy of the different datasets was done differently. After the first two datasets it was used the SILVA database to annotate the ASV. Morgans dataset is related to IBD but it is from a cohort of patients that underwent colectomy and samples are specific of the pouch or pre-pouch ileum, there are no healthy samples and there is no follow up. It made very unlikely that a classification of the location could be achieved and the classification of the samples according to the microbiota could not be based on the disease (as all of them had undergone the same procedure with the same disease). There seems to be gradation on the models on 4.19 that could partially be explained by different degrees of inflammation. Howells dataset is very similar to our BARCELONA dataset but on pediatric cases. It includes both CD and UC (and controls). The time with the disease of these patients is much lower, and they have might followed less treatments and probably have not had any surgery The Cristians dataset was also imputed with SILVA but using a different database version. 5.1.2 The methods As seen on the introduction there are many methods are available. There are frequent releases of new tools and methods. The most up to date list of tools might be added to a collaborative list. Between them methods are very different in how they can be used and how they can be classified. The quality of the software and methods also differs. Some of them were tested several datasets but their most important validation process undergone is the mathematical validation they have. With the increase of tools there have been increasingly important to compare different tools. There are reviews of different tools on the same datasets [205], some are more theoretical [206], others are focused on a different field [99]. Of all these methods very few have been applied on IBD datasets. Recently there has been an article focused on integration on IBD [149]. On this publication the authors suggest that one must be mindful of the gap between the experimental conditions and the real world. It also encourages to collect more data about the exposome (the stimuli patients have). It ends up advising to set up guidelines for multi-omic studies tailored to the field, coordinate in a global framework to prevent redundant studies and ensure efficient funding and resources and disseminate training and education on computational approaches to analyze multi-omic datasets. Current methodological approaches focus on comparing tools on (several) previously published datasets [207], an approach that we have taken too. This approach was not used to compare different tools, that we also do it, but to validate findings of one dataset in other datasets. This is specially important due to the lack of golden datasets or a way to reliable simulate datasets as discussed on the previous section 5.1.1. On IBD, many studies focus on finding some genes or bacteria to answer a narrow question they have in mind, like which bacteria is related to inflammation [80]. On this thesis, the focus was on finding a good representation of the relationships that allowed to identify genes and bacteria are relevant on the disease. We made the assumptions that the microbiome presence and the transcriptome are related. This assumption is backed up by several other previous studies supporting this relationship or others [40, 41, 208210]. Tools that relate the variance of a block with other variables, both numeric and categoric, are needed to search which variables are important. PERMANOVA and globaltest worked well to identify which variables about the samples were important. But they do not give any insight into which specific microbial species are driving the association between the microbiome and the variables [211]. In addition, we could miss some other important variables. It is known the role of other factors outside the omics data collected, mainly from environment factors, genetic susceptibility and immune response [212]. Sudhakar et. al. [149] recommend being conscious of the gap between the data available and the biological process. However, even if we missed them, we probably have not collected it or it is not available, there is nothing at this point that can be done. This also enters in conflict with a principle of data protection: collect the minimum sufficient information needed. We might not identify a relevant variable but to identify it we would need to collect too much information from the patients and for little or no clear benefits. One variable we did not keep track was the microbiome load which is linked to the guts microbiome community variation [213]. We did not measure it because at the time of extraction we didnt quantify the cells present and after the DNA extraction it was too late. We tried to find which genes and bacteria are correlated between them using WGCNA. It is a tool designed to find common co-occurring patterns based on correlations. It requires homogeneous samples, with a minimum of 12 samples for condition. When applied to the whole dataset there might bee too much variance in order of WGCNA to find the proper signal. This might be the reason why it failed to achieve a good fit on the scale free topology with around 100 of mean connectivity. In addition, having microbiome and RNASeq on the same matrix, would probably be hard for the process to find good relationships if we applied the same normalization process to both of them without escalation. We could have tried to make smaller groups and then compare the modules between them but it groups would be too small because samples were from multiple segments and conditions. We briefly considered using STATegRa but it is not possible to select which interactions between the blocks exists and by that time we already knew it was important to consider several of the variables collected from the patients. The method implemented on STATegRA might be useful for cases where there is a great agreement between blocks or were the variables of the samples are not so important as with our data. To identify related variables other methods use correlations between the variables [214]. On our dataset we explored the correlations for all the datasets, but the significant correlations were usually driven by an outlier, or was not a good fit of the data due to missing microbiome presence (data not shown). We tried filtering correlations from those identified by the models, being less restrictive on the significance of the correlation, removing those samples that did not have microbiome presence on at least 5 samples, and remove those samples without that microbiome presence. All of these variations didnt provide a clear insight over which genes were correlated to which microbiomes, this differs to other publications that relied on them to draw their conclusions [215]. This might be due to them relying purely on the Spearman \\(\\rho\\) metric instead of visualizing the correlation it measures. BaseSet did not work for its intended usage on this thesis. It failed because it is computationally expensive to calculate the likelihood of 1500 variables; there are too many combinations. In addition, the numeric precision of said calculations suffers from the floating point problem and must be considered carefully [216]. To support multiplying more than 1000 float numbers a different strategy such as using log values might be better. We could not come up with a better strategy to find all the combinations needed, perhaps a better method exists that could be used to find which are the terms more influential to the end result. During the peer-review process of the package for its acceptance on rOpenSci, some concerns were raised about conflating probabilities with fuzzy-sets. For all these reasons this approach was no longer pursued. However, the package was mentioned as top 40 packages added on CRAN that month and it might be useful on other circumstances or when less variables are needed. The development of the experDesign package helped avoid batch effects on the sequencing step. However, as seen on the previous section, this does not prevent all problems with batch effects and still batch effects resulting on a less useful experiment might get through. As discussed on the related publication, there are several tools already focused on this problem: OSAT ([217]), anticlust ([218]) and Omixer ([219]). But these tools have some shortcomings, that are covered by experDesign: OSAT cannot handle missing values and does not work well for arbitrary batches, anticlust only accepts numerical variables but it is based on a powerful mathematical theorem and Omixer has bugs that prevent comparing with the other tools with no possible workaround. In addition, experDesign received requests to have a new feature for expanding experiments. This might help improve the quality of bigger datasets to ensure they can be extended in several sequencing runs. This feature would be useful for multi-omics datasets or in big cohorts to minimize batch effects usually associated with long running collection of samples. We selected MCIA as a baseline to compare our method because it works well, has a good documentation as well and it is fairly widespread used. The method was developed after RGCCA and recently there have been publications that show that it outperforms other methods on its versatility on different contexts [207]. On the dataset analyzed we found it was very competitive with inteRmodel but this will be discussed on section 5.3. 5.2 Designing models Previous publications using RGCCA on IBD focused on validating inflammation markers genes DUOX2 and APOA1 as inflammation predictors ([80]) from previously published articles [220]. Some publication tried to summarize the existing relationships on IBD [79], but none were focused on finding the existing relationships on IBD using RGCCA as we did. Knowing that there are many variables outside transcriptomics and microbiome that are relevant to the disease (and healthy) homeostasis these variables should be included on the models. In addition, the relationships between the blocks are unknown on both the strength and interaction. To identify them the connections between blocks had to be modified on RGCCA to model the relationships. Last, what belongs to a block should be carefully considered as the assumption is that the whole block is correlated with the other blocks they are connected. If one has preexisting theories about the data, a specific model can be used stating these known or hypothetical relationships. However, if new relationships are being explored or no prior beliefs on the data are held the models should be created with random links between blocks, and evaluate which model is better. The connections tested required that all models should be connected and no isolated blocks were left, all blocks should be indirectly connected to other blocks. This avoids optimizing two different networks of blocks that are not connected between them. Thus, forcing the model to represent all the information. Typically blocks are defined by each omic data origin and no other information is included. However, we knew that the transcriptome is mainly related to the location of the samples and we expected that the microbiome would be more related to the variables related to the patients demographic characteristics and dietary and personal habits. This is specially important because in our datasets we have samples from the same patient and time from multiple locations. On other studies there are less samples per individual and timepoint (if there are several timepoints) [79, 151, 152]. Variables of samples were separated according to what they are about on models of family 2. If it is about the individual it would be on the Demographic block, if it is about time on a Time block, and if about the location of the sample on a Location block (c.f. with tables on design of Pugets 4.4, 4.3; HSCT 4.13 4.14, 4.15; Hsler: 4.21, 4.22, Morgans 4.28, 4.29, Howells 4.33 models). 5.3 Evaluating models To evaluate a model RGCCA provides the average variance explained (AVE), inner and outer (see section 3.4.2). As we are more interested on the model of the relationships the inner AVE makes more sense to evaluate it. Furthermore, to evaluate a design bootstraping can be used to know how well the design does apply to a variety of data. Although on inteRmodel there is the option to use a leave-one-out procedure, we didnt use it to evaluate the fitness of the models. Using an external cohort to validate the same model, or using a different method to see if it finds the same relationships or explains the data as accurately is also a common approach to evaluate and validate models. Using the same approach on different data helps to ensure the replicability of the results [221]. Figure 5.1: Reproducibility matrix indicating the terminology used between using the same method and the same data. Figure from The Turing Way: A Handbook for Reproducible Data Science (Version v1.0.1). We used the same approach on 4 different cohorts, from different origins and with different types of samples, but all are related to IBD, have 16S data and intestinal transcriptome. Some of them have multiple samples from the same individuals while others do not [79]. We also compared our method with a different one to see how generalisable are the results. Of the multiple methods available we used MCIA [222]. We compared it with inteRmodel by looking at the area under the curve for classifying the samples with the canonical component of the transcriptome data according to their location. The procedure of separating independent variables in their own block of data and later search the best model that fits the data provides a good strategy that should be consider for integration efforts. The procedural method of searching a model and testing them is implemented on inteRmodel. But the most important thing is to consider which variables are independent of which and if they can be separated into a block for later usage on the modeling (See [223]). This can lead to undesired conclusions like the effect known as Simpsons paradox [224]. Splitting the variables of each sample into several blocks forces RGCCA to adjust for a new canonical dimension. The omics block such as gene expression and 16S data could be split, as the expression of some genes influence other genes, such as transcriptor factors (First mentioned on [225]) , miRNA [226], siRNA [227] All these interactions and regulations could distord the canonical correlations. However, the nature of this relationships is not linear and the interaction between them is multiple and very complex. Its complexity has prevented to accurately account for all subcellular reactions at speeds that could be useful (it has only been recently accomplished for a prokaryote cell [228]). In addition, these interactions are time dependent, not linear and are highly interconected between a large number of variables. For these reasons gene expression and 16S block were not split into several blocks. Besides comparing the results of different methods, these models need to be evaluated by the insights they provide on the biological system they are being applied to, in our case IBD. So far the models were only discussed on their technical merits. It is known that mucosal transciptome is related to localization of the samples [229]. The difference is so great that many times the colon and ileum samples are analyzed separately. As such, it was a reasonable assumption to expect models to reflect this differences on the gene expression canonical component. The variability of microbiome challenges finding clear patterns. On healthy humans, it has been suggested that there is a common microbial composition on all humans [230]. Such theory suggests three groups of similar microbiota composition, named enterotypes. However, the enterotypes classification is not unanimously accepted [231, 232]. There are many factors that influence the gut microbiome [233]. But the role of microbiome on IBD has recently gained some interest [16, 234, 235]. It has been associated with predictors of CD treatment response and suggested that they could be useful for building an improved classifier for CD and treatment response based on sufferers microbiome in the future [236]. Thee is for instance, an association between IBD and butyrate producers (See [237]) which has shown to influence permeability of the epithelial barrier [24]. In addition, It is known that microbiome of patients in remission is different from non-IBD patients [238]. So even if two individual seem healthy at this moment their microbiome might not be similar. The relapse nature of IBD, suggests that at different time points the gene expression or the microbiome might be different. For this reason, time is an important variable when modelling the disease. If multiple timepoints are taken they should be taken into account to identify the state of the disease for each patient. Even on healthy non-IBD patients, multiple samples with microbiota from the same patients could provide a common microbiome background, which can help to identify core individual microbiome and altered microbiome states later on. On the cohorts analyzed on this thesis, only the HSCT and the Hsler dataset had some time related variables available. However, Hsler dataset only had age at date of sampling, and there were not several samples for the same patients at different timepoints. This leaves the HSCT dataset with the only one with multiple timepoints from the same patients. Having that many samples from the same patient might explain why the classification of the disease on this cohort works so well, with relatively a very complicated cohort with several background treatments, refractory to all previous treatments , some of them having undergone surgery too and with wide systemic changes going on. Despite all of this there was a huge agreement in genes identified as relevant between datasets and the pathways identified are relevant to the intestine (See 4.27). Comparing microbiome between datasets is less straight forward. If the same primers are used the ASV could be directly compared. However, the ASV length might be different. In addition, on this thesis we used dataset with different primers and also OTUs. For these reasons a direct comparison is not possible. A comparison of the taxonomy of the annotated microbiome on the datasets is the next possiblity. However, some datasets used did not provide the annotation (Hslers and Morgans dataset). So we are are left with comparing OTUs with ASVs. Despite the errors on ASV annotation ( [239]), OTUs from the HSCT dataset can be compared to ASV from Howells dataset. There were very few common taxonomic levels selected on both datasets (data not shown). This was not a surprise, as there are many factors that influence ones microbiome profile and the diversity indexes already showed high differences between samples of the same dataset. The microbiome of each dataset seem capable to classify the samples according to the disease (data not shown). However, on further evaluation via bootstrapping this resulted not significant, as any relatively big amount of microbiome variables might classify the samples due to their specificity. 5.4 Implications On this thesis several methods has been developed for helping research. experDesign is on the primary steps from moving from the bench to the in-silico experiments. BaseSets and inteRmodel are more relevant for computational analysis. BaseSets might help beyond integration analysis such as single cell annotation7. Some studies using host transcriptomics of faecal wash infer inflammation without colonoscopy [240]. This would help patients to avoid an unpleasant experience, and reduce the usage of clinical facilities. It could be possible that just sequencing the intestinal microbiome could be enough to identify the patients disease. However, this requires further validation to ensure that the diagnosis is accurate enough on a diverse and big population. There are already studies on this direction, not only for IBD or intestine but for several different human regions [4]. With these studies we hoped that they would provided which bacteria played a role on the disease, or which genes and bacteria are related on IBD. We obtained a list of putative genes and bacteria but not a clear pairing of which genes interact with which bacteria. This could mean that the microbiome community is related to all the genes. It could also mean that the methods are not powerful enough to find more tailored relationships as normalizations and generalizations present on RGCCA do not allow to subgroup or classify the variables already detected. Maybe a different method that would not depend on the same principles might be able to detect finer relationships on the variables selected. There is a disconnect between the computational side and the experimental side, driven by the difficulty to think an experiment to test the new information that multi-omic experiments provide. This is referred as the gap in other publications [149]. This gap between the computational methods and the data origin and practical usages should close. Potentially this would require more closer collaboration between clinicians, statisticians, bioinformaticians and research software engineers; in addition to creative ideas accounting for the standard procedure on hospitals and points of care. However, it is also possible that further developments or creatively applying statistical methods might help closing the gap. For instance, there are many combinations of interaction possible. These interactions between microbiome and genes are currently hard to explore statistically. Further research on how to reduce the space of possible combinations of bacteria or evaluate which combinations are more important might be useful on the future. This might involve using more network integration methods. Krassowskis et. al. ([94]) advice and recommendations on software engineering and reproducibility practices to share a comprehensive awareness with new researchers in multi-omics for end-to-end workflow. In addition the recent recommendation about integration on IBD ([149]) suggests that there is still much to be done. To increase the results by these projects and the effectivity. There seems to be a tendency of multi-omics project to focus on metagenomics, metatranscriptomics and metaproteomics and abandon plain 16S sequencing [241]. This could be explained by the numerous problems that 16S sequencing have. Some of these are avoided or solved by using these other omics techniques. For instance the metatranscriptomics and metaproteomics could help detect what is actively producing the microbiota and the metagenomics, what is actually there at a more detailed level than 16S. But these methods also have their shortcomings. Metagenomics still does not detect adherent invasive cells and metatranscriptomics and metaproteomics do not provide who is producing what (even if paired with metagenomics or 16S data). Perhaps as suggested in other publications network methods might be able to provide more detailed information about the relationships [242]. But, it is unclear how complete and valid are these networks. There is some evidence that currently literature focus on already known and studied genes instead of more novel and with higher relevance genes [243]. This could explain why there are many genes selected by the models with few pathways or gene sets. To study complex systems, new technologies like organ-on-chip or more specifically gut-on-chip are being developed [244]. These systems expand on the already useful technique of using organoids to better mimic the epithelial cross-talk on the laboratory. Latest developments include the addition of separate environments and distinct flow on these environments. By introducing gut microbiota, these systems will help studying the interaction of the gut microbiota and the intestinal epithelium. However, it is not clear how accurately accounts for other factors such as the accumulation of inflammation. Several factors affect the disease and the relationship between guts microbiome and guts mucosa. Analysis or comparisons without taking into account them might provide misleading or false results. To our knowledge this is the first study using these variables as part of the integration study with canonical correlations. We hope this provides a first approximation to accurately understand the relationships between genes and bacteria on IBD. References "],["conclusions.html", "Chapter 6 Conclusions", " Chapter 6 Conclusions Microbiome under some dataset can classify the patients according to their disease. Doing so reliable might take some time. It is hard to make it generalizable. Overoptimizing for a dataset might be possible but so far we have not seen this, on the contrary on different dataset the models of connections seem consistently even if the particular taxa do not show a great overlap. If we had to summarize all the work in one sentence we would say that "],["acknowledgments.html", "Chapter 7 Acknowledgments", " Chapter 7 Acknowledgments Azu Juanjo Pau + grup Nuria, Elena, Helena, Aida. Metges: Juli, Helena Ana, Alba, Isa, Marisol, Maica Familia Twitter #rStats and other people on Bioconductor (support.bioconductor.org), Biostars, Bioinformatics SE Dr.Hsler for kindly providing metadata from their cohort and Dr.Cristian Hernndez for their collaboration and selfless sharing the samples of their cohort. "],["references.html", "References", " References "],["online-resources.html", "A Online resources", " A Online resources Some links that I found useful on the thesis and could be useful if you are interested on the multi-omics field. Awesome multi-omics https://github.com/mikelove/awesome-multi-omics : An online repository of references to multi-omics methods. Reproduced here with their references 8: Table A.1: Integration methods available and their references. Method Publication SCCA [245] PCCA [246] PMA [247] sPLS [248] gesca [249] Regularized dual CCA [250] RGCCA [114] SNMNMF [251] scca [252] STATIS [253] joint NMF [254] sMBPLS [255] Bayesian group factor analysis [133] RIMBANET [127] FactoMineR [256] JIVE [135] pandaR [257] omicade4 [163] STATegRa [164] Joint factor model [258] GFAsparse [259] Sparse CCA [260] CCAGFA [261] CMF [262] MOGSA [263] iNMF [264] BASS [265] imputeMFA [266] PLSCA [267] mixOmics [268] mixedCCA [269] SLIDE [270] fCCAC [271] TSKCCA [272] SMSMA [273] AJIVE [274] MOFA [275] PCA+CCA [276] JACA [277] iPCA [278] pCIA [279] sSCCA [280] SWCCA [281] OmicsPLS [282] SCCA-BC [283] WON-PARAFAC [284] BIDIFAC [285] maui [286] SmCCNet [287] msPLS [288] MOTA [289] D-CCA [290] COMBI [291] DPCCA [292] MEFISTO [293] MultiPower [107] Bookdown: The book about how to write this type of books. Bioconductor: The project about bioinformatics on R mostly related to sequencing technologies. CRAN: The main archive of R extensions/packages for R. GitHub: Company which allows to freely host remote git repositories of many projects, including some used or developed on this thesis. References "],["software.html", "B Software B.1 STAR B.2 RSEM B.3 Listed B.4 By project/publication", " B Software Along the years of this thesis several pieces of software have been generated as well as packages. Here they are listed for easier retrieval. They are listed on two ways, one with a brief explanation and another one ordered by what software piece is used on each analysis. B.1 STAR The parameters and options used with STAR are: STAR \\ --outSAMtype BAM SortedByCoordinate \\ --outFilterIntronMotifs RemoveNoncanonical \\ --outSAMattributes All \\ --outReadsUnmapped Fastx \\ --outSAMstrandField intronMotif \\ --outFilterScoreMinOverLread 0.5 \\ --outFilterMatchNminOverLread 0.5 \\ --outFilterType BySJout \\ --alignSJoverhangMin 8 \\ --alignSJDBoverhangMin 1 \\ --outFilterMismatchNmax 999 \\ --outFilterMismatchNoverLmax 0.04 \\ --genomeDir &quot;$genome/STAR&quot; \\ --limitBAMsortRAM 10000000000 \\ --runMode alignReads \\ --genomeLoad NoSharedMemory \\ --quantMode TranscriptomeSAM \\ --outFileNamePrefix $output \\ --runThreadN &quot;$threads&quot; \\ --readFilesCommand zcat \\ --readFilesIn &quot;$file1&quot; &quot;$file2&quot; B.2 RSEM Code used for RSEM rsem-calculate-expression \\ --quiet \\ --paired-end \\ -p &quot;$threads&quot; \\ --estimate-rspd \\ --append-names \\ --no-bam-output \\ --bam &quot;$rseminp&quot; &quot;$genome/RSEM/RSEM&quot; &quot;$rsem&quot; B.3 Listed An improved/tested version of RGCCA, some modifications on the internal functions to ease the maintenance as well as adding tests and sometimes improving the documentation. Also modified so that it is possible to provide a vector of models so that the model of the first dimension is not the same as the model on the second dimension (not sure if mathematically speaking makes sense but from a biological one I think it might be interesting to have it). Designed to be used with RGCCA I wrote the package inteRmodel to ease the bootstrapping and model selection. A package to design batches to avoid batch effect experDesign and its website on GitHub. Explore the effects of the hyperparameters on RGCCA on the provided dataset of gliomaData (Originally provided here) there is this repository sgcca_hyperparameters. We used a pouchitis cohort published in this article[151] that was used to compare how performs our method in others dataset. The code used can be found in this repository. Some functions used to explore the TRIM dataset ended up in the integration package.This include functions for correlation, network analysis, enrichment, normalization of metadata I developed a package to analyze sets and fuzzy sets BaseSet (based on what I learned from a previous iteration of the package). This package was meant to be used with the probabilities that arise from bootstrapping the models. However, due to the long times of calculation that it would require it was not used. To analyze the BARCELONA cohort (also named antiTNF) a different repository was created to analyze the data using the previously developed packages. B.4 By project/publication All code of the analysis of the publications is available (in his messed state and complicated history) and a brief description as to why they were used: Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis: TRIM: Mangle with the sample, dataset, explore several methods Pugets: Explore the effects of the hyperparameters on RGCCA on the provided dataset. inteRmodel: Package for easy repeating the methodology developed on TRIM. Morgans: Work with the pouchitis cohort used in this article. Hslers: Work with the UC/CD dataset used in this article. integration: Package with functions that I wrote or used on different parts of exploring the TRIM dataset ended up here. BaseSet: BaseSet: Fuzzy logic implementation, available on rOpenSci too and its documentation website . experDesign: experDesign: Help design experiments in batches. It has a documentation website too. BARCELONA: BARCELONA: Code to analyze the BARCELONAs dataset Validation: Howells: Work with Howells 2018 dataset. Cristians: Work with Cristians 2020 dataset. References "],["articles.html", "C Articles C.1 Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis C.2 experDesign: stratifying samples into batches with minimal bias", " C Articles Articles related to this project published during the thesis. C.1 Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis Article peer-reviewed published on 2021, freely available online. C.2 experDesign: stratifying samples into batches with minimal bias Article peer-reviewed published on 2021, freely available online "],["other-datasets.html", "D Other datasets D.1 BARCELONA dataset D.2 Hernndez dataset", " D Other datasets D.1 BARCELONA dataset All patients with an established diagnosis of IBD, including Crohns disease, ulcerative colitis, unclassified IBD, indeterminate colitis, or pouchitis, starting treatment with a biologic agent were monitored following the schedule of clinical visits, laboratory tests, imaging procedures and biologic sampling at the beginning of their treatment with anti-TNF therapy and after 14 weeks and 46 weeks a biopsy from an ileocolonoscopy. The protocol was approved by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7845 and HCB/2012/7956). Patients that were referred to the Hospital Clnic de Barcelona IBD unit, who had already started treatment with a biologic agent in another center, were also included adapting to the corresponding time-schedule of their treatment. In all patients, starting anti-TNF treatment will be decided before the protocol entry decision according to medical clinical practice. Anonymized identification of the patients, disease, sex age at diagnostic, age at the moment of the sample taking, time since the start of the treatment and sample segment was collected. Table D.1: Samples included from the BARCELONA dataset characteristics. Characteristic BARCELONA Individuals 62 Status (non-IBD/CD/UC) 8/33/21 Sex (female/male) 29/33 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 2/44/8 Years of disease: mean (min-max) 7.6 (0-32) Age: mean (min-max) 41 (18-68) Time (0/14/46 weeks) 41/40/32 Sample segment (ileum/colon) 39/87 The process of DNA extraction and sequencing was different for this dataset. We used different 16S-V3V4 primers pair 341f/806r on a MiSeq Nano sequencing by the RTSF Genomics Core at Michigan State University, United State of America. The sequence of the primers used was: 341f: 5-CCTACGGGAGGCAGCAG-3 806r: 5-GGACTACHVHHHTWTCTAAT-3 The result of MiSeq Nano were processed with bcl2fastq (v1.8.4). This dataset was processed as usual but as part of the quality controls of the dataset the diversity measures of the samples was analyzed on D.1: Figure D.1: Diversity indices of Barcelona according to the location and disease status. There is a lot of diversity between different groups but importantly the control samples overlap with the patients with inflammatory bowel disease. Control samples diversity should be lower and not on the same range as with samples of patients with IBD. The datasets 16S was sequenced several times by different platforms. Despite the pilots and the negative controls on the sequencing process, each time there were different problems: contamination, low quality and then this suspicious diversity. It does not seem to be a problem of the sequencing facility, so this data was abandoned as unreliable. D.2 Hernndez dataset This dataset was obtained from collaborators at Mount Sinai, Toronto, Canada [81]. Patients with UC, CD were recruited when attending regularly scheduled visits or surveillance. In addition, asymptomatic healthy controls (HC) were recruited during routine, age-related colorectal cancer screening by colonoscopy. 290 samples were collected together with information about the disease, age at diagnosis, age at the moment of the sampling, sex, sample location and smoking status. Table D.2: Characteristics of samples included from Hernndezs dataset. Characteristic Hernndez Disease (non-IBD/CD/UC) 46/54/66 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 29/73/18 Age: mean (min-max) 40 (17-71) Sex (female/male) 81/85 Smoking (never/ex/current) 115/34/16 Sample Location (ileum/colon) 97/193 D.2.1 Results This dataset was processed to confirm the results on the previous datasets. At the time of writing the process is not complete References "],["models-output.html", "E Models output E.1 HSCT E.2 Hsler E.3 Morgan E.4 Howell", " E Models output E.1 HSCT E.1.1 Genes E.1.2 Microbiome E.2 Hsler E.2.1 Genes E.2.2 Microbiome E.3 Morgan E.3.1 Genes E.3.2 Microbiome E.4 Howell E.4.1 Genes E.4.2 Microbiome "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
