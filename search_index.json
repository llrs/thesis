[["index.html", "Data integration on inflammatory bowel disease Preface", " Data integration on inflammatory bowel disease Lluís Revilla Sancho 2022-02-25 Preface The main topic of the thesis is data integration applied in the inflammatory bowel disease (IBD) research. This disease is complex, for instance it is not know if the cause behind Crohn’s disease and ulcerative colitis is the same. There are hypothesis pointing that the microbiome is a major factor in the disease, which together with an aberrant immune response is the dominant theory. In order to find robust relationships between the mcirobiome and the immune system it is important to consider all the relevant variables that influence a disease. On this thesis I seek these relationships using data from different sequencing technologies and the observed or reported phenotype of the patients. The thesis was carried out on the Institut d’Investigacions Biomèdiques August Pi i Sunyer (IDIBAPS) research institute and funded by Centro de Investigación Biomédica en Red (CIBER). The thesis was done on the IBD unit which is a translational team of biologist, microbiologists, veterinaries, bioinformaticians, doctors and nurses (at Hospital Clínic) in a multidisciplinary team. The leading doctor of the unit was Julian Panés whose interest on the disease lead to receiving a grant that made possible this thesis. The thesis is under the doctoral programme in biomedicine of the University of Barcelona (UB). My thesis directors’ were Juanjo Lozano and Azucena Salas, my boss, who helped as bioinformatician and disease expert respectively. They provided advice and guidance on how to analyse the data and where to focus on the different experiments/analysis. This thesis is licensed under the Creative Commons Attribution 4.0 International License (CC-BY). "],["abstract.html", "Abstract", " Abstract Introduction: Inflammatory bowel disease is a complex disease with several factors that affect each other. The origin and process that maintains the disease is not fully understood, and current treatments focus on early detection of the disease and prevent the accumulation of damage on the intestine. Finding those relationships will help to advance treatments, diagnosis or prevent it. Methods: To identify these relationships we based our analysis on intestinal bacterial 16S data and human transcriptome from biopsies. We used canonical correlation analysis to find models that are coherent with previous knowledge on the biologiy of the disease and provide insights on the relationships between bacteria and genes. Results: We compared our own methods with previously published methods, mainly MCIA, to assess the advantages and disadvantages of the different proposed relationships. Furthermore, the classification of samples was used to find which methods provide are more accurate. Discussion: Biological relevance, difficult. Conclusion: Chips or further tests. "],["introduction.html", "Chapter 1 Introduction 1.1 Inflammatory bowel disease 1.2 Integration studies on IBD 1.3 Integration", " Chapter 1 Introduction Inflammatory bowel disease (IBD) involves Crohn’s disease (CD) and ulcerative colitis (UC). It generally affects the terminal ileum and the colon but it can involve any segment of the gastrointestinal tract. UC is a recurrent, chronic and continuous inflammation of the colon and rectum while the CD is not a continuous inflammation and affects the whole gastrointestinal tract causing transmural inflammation. IBD etiology is unknown. However, once it has initiated the most prevalent hypothesis of its chronicity suggests an aberrant immunological response to antigens of the commensal microbiome. To diagnose IBD doctors use endoscopy and/or magnetic resonance imaging and histologies. Treatments provided for IBD include, noninflammatory drugs, suppressors and biologics, i.e, anti TNF- anti-IL2, 23, anti-integrin \\(\\alpha4\\beta7\\). The therapeutic options can induce remission in some patients, but they often need continuous treatment to avoid recurrence. Nevertheless, many patients are refractory or intolerant to those therapies and need to undergo surgery or other strategies like dietetic and psychological support [1] . 1.1 Inflammatory bowel disease IBD includes the CD and UC which are characterized by alternating periods of remission and clinical relapse that mainly affect the gastrointestinal tract. CD is a progressive relapsing disease that can affect all the gastrointestinal tract but shows mostly on both terminal ileum and colon with a discontinuous inflammation. UC is a colonic relapsing disease characterized by continuous inflammation of the colon. Both of them have different risk factors, clinical, endoscopic an histological characteristics (see sections 1.1.2 and 1.1.3 ). Around 3.5 million individuals have IBD in Europe and North America combined [2]. IBD is more commonly found in industrialized and developed regions, suggesting that environmental factors might greatly influence IBD occurrence. In addition, the incidence of IBD is increasing in areas, such as Asia or Eastern Europe, where the number of cases was relatively low hitherto [3]. The dysregulation of the inflammatory response observed in IBD requires interplay between host genetic factors and the intestinal microbiome. Several studies support the concept that IBD arise from an exacerbate immune response against commensal gut microorganisms. Nonetheless, the disease could result from an imbalanced microbial composition leading to generalized or localized dysbiosis1. The role of the gut microbiome in IBD is an active ongoing field of research. Several authors are currently studying the alterations reported in IBD of the intestinal microbiome. However, it is still unclear the cause-effect relation between dysbiosis and IBD. Partly due to the multiple variables already identified that have been linked to IBD; for instance, age, diet, usage of antibiotic, tobacco, and socioeconomic status [4]. The relationship between host and microbiome has been proposed to play a fundamental role in maintaining disease. For instance, some Proteobacteria species which have adherent and invasive properties might exploit host defenses and promote a proinflammatory environment, altering the intestinal microbiota in favor of dysbiosis [5]. The epithelium is often damaged and might present ulcers or other inflammation symptoms. A segment of the gastrointestinal tract might recover if the patient receives treatment or due the natural cycles of the disease. But once a segment is affected by the disease it can be considered as involved, as some damage remains even if the tissues is no longer inflammed. 1.1.1 Etiology and pathogenesis Several mechanisms have been proposed to drive IBD pathogenesis [6, 7]. Some of them are based on a relationship between the immune system and the microbiome [8, 9]. It is also unclear if CD and the UC share the same origin considering their different symptoms. There is also evidence of some genetic component on the onset of the disease, specially if the disease appears very early (less than 2 years old patients) [10, 11]. Disease can be classified based on age at onset as very early, early or adult on-set disease [11]. Genome-wide association studies (GWAS) have linked IBD to over 100 genetic loci, including a NOD2 gene, but so far there is not any known mechanism how polymorfism on this genes are driving the disease [12]. On early pedriatic and adult disease the genetic component is lower than on very early on set and it is thought that the environmental factors are the main cause of the disease at those ages. On the following sections I will explore the role of several of the possible factors involved on the pahtogenesis, starting with the genetics. 1.1.1.1 Genetics IBD is not an heritable disease, except for very early onset IBD, but it has some genetic influence that predisposes people to have it. This has lead to look for genetic factors on IBD both on general population and on the early cases. Genome wide association studies (GWAS) are one of the most common genetic studies performed, together with methylation studies. To discover through linkage desequilibrium genetic variations linked to phenotypes and regulatory transcription changes, respectively. With GWAS several allels on protein coding loci have been found, rising to around 300 genetic variants [13]. Particularly, the NOD2 gene is highly relevant for the disease on European patients, as it is a risk alleles for CD loci but show significant protective effects in UC [14, 15]. The mechanism of how this gene protects from UC has not been confirmed yet [12]. Many of the relevant genetic loci related to IBD are not on protein coding fragments of the genome. Recently expression quantitative trait loci (eQTL) particularly showed [10] that locis are on enhancers or promoters like e.g. H3K27Ac or promoter e.g. H3K4me1 marks as found by chromatin immunoprecipitation sequencing (ChIP-Seq). 1.1.1.2 Microbiome The human intestine is a large reservoir of co-existing microorganisms (bacteria, fungi, viruses, and unicellular eukaryotes) . This microbiome community exerts different functions in the human body influencing nutrients’ metabolism, the maturation of the immune system while suppressing the growth of harmful microorganisms’ [16]. The role of the gut microbiota has been proposed to play a role in IBD pathogenesis. IBD has been characterized by a breakdown in the balance between beneficial and harmful bacteria that are present in the human gut compared to healthy individuals [17, 18]. Indeed, many studies show that patients with IBD have less biodiversity. Biodiversity is measured on \\(\\alpha\\) (alfa) and \\(\\beta\\) (beta) diversity. \\(\\alpha\\)-diversity is a measure of the species present on a single sample and its abundance while \\(\\beta\\)-diversity compares the diversity between samples. There are some reports of taxonomic changes and increase on Enterobacteriaceae sp, Escherichia coli (specially the invasive strain) at the mucosal layer of IBD patients [19]. At the same time there is often a reduction of protective species like Bifidobacterium, Lactobacillus and Faecalibacterium which might be able to protect individuals from mucosal inflammation via several mechanism such as a downregulation of proinflammatory cytokines or the stimulation of IL-10 and antiinflammatory cytokines [19]. Specially Faecalibacterium prausnitzii is one microorganism of interest [20, 21]. In fact, it has been recently proposed that several unique microbial species can distinguish healthy controls from UC and CD patients [22, 23]. Figure 1.1: The microbial composition in a healthy and IBD gut. One of the proposed mechanism of crosstalk between bacteria and host is through bacterial metabolites. They interact with the cells and modulate the state of the intestine. One example of such metabolite is butyrate which has been linked to microorganisms presents on healthy intestines and shown to interact with intestine cells and help regulate some genes [24]. As previously mentioned, adherent invasive Escherichia coli, a proteobacteria specie, has been associated with IBD [25]. Adherent invasive strains are mainly found in ileal and colonic samples of CD patients and their presence in UC is less clear. These adherent invasive cells enter through the epithelium of the more permeable cells and live on their cytosol. The metabolic cocktail composed of soluble factors secreted by life probiotic bacteria, living microorganisms which, when administered in adequate amounts, confer health benefits on the host [26–30] or any bacterial-released molecule capable of providing health benefits through a direct or indirect mechanism, has been collectively known as postbiotics since 2012 [26]. 1.1.1.3 Immune response As explained previously the immune system plays a role in IBD pathogenesis and pathophysiology. Loss of tolerance to commensal bacteria has been suggested as the underlying mechanism triggering the inflammation on the intestine. The immune response involves many different cells lines and regions, which are important to know how they organize for a better understanding of the disease. Figure 1.2: The intestinal epithelial barrier. From the luminal side of the intestine, the first layer is the mucosa (See sections 1.2 or 1.1). In the colon the mucus is organized in two layers: the inner layer, a firm mucus layer; and the outer, loose mucus layer [31]. The intestinal epithelium is a single layer of cells organized into crypts and villi (and circular folds on the large intestine) that carries out a diverse array of functions besides digestion performed by specialized cell lineages. Immune response in the intestinal mucous is mainly excreted by the gut associated lymphoid tissue [32]. Genetically predisposed patients when exposed to certain environmental factors activate immune responses against microbials or self-antigens which in turn, may impair the mucosal barrier of the intestinal mucosa, the first physical barrier on the mucosal surface. Both the adaptive and the innate immune cells are present in the intestine, right below the epithelium. On IBD due to antigen translocation into the lamina propria, the immune response leads the adaptive cells to generate immune response to harmless components of the intestinal microbiota. This initial response induces a local increase in the production of pro-inflammatory cytokines and mediators which damages the mucosa. Therefore, the loss of integrity on this barrier enables the intestinal luminal bacteria to access the intestinal epithelium and to interact with the immune system underneath it more directly [33]. The intestinal epithelium is another line of defense against bacterial invasion. Intestinal epithelial cells play a key role in controlling the integrity of the physical barrier to the intestinal microorganisms [33] not only physically but also secreting antimicrobial peptides and defensins, both of which are altered in IBD patients [34]. The intestinal epithelium also plays a key role on the intake and diffusion of metabolites from the intestinal lumen to the lamina propia. Damaging or increasing the permeability of the intestinal epithelium results on a response from the immune system. Detecting signals of any foreign particle can also trigger the immune system. On the intestine this starts with the identification of these signals by intestinal epithelial cells have pattern recognition receptors. There are two main pattern recognition receptors: toll-like receptors (TLR), which are present on the surface, and nucleotide-binding oligomerization domain-like receptors (NOD-like receptors), present on the cytoplasm of the cells. These receptors upon recognition of pathogen associated molecular patterns (PAMPs) start an amplifying signaling producing chemokines and cytokines which activates the transcription and translation of pro-inflammatory mediators to ensure an effective immune response. Initially the innate response is triggered but the cells also increase the antigen presentation to T cells and thus activate the immune adaptive response [35]. Other cell types, such as monocytes, macrophages and dendritic cells also present the pattern recognition receptors. From those, macrophages and dendritic cells are antigen presenting cells too and secrete several cytokines to activate other immune cells. Usually CD patients express higher amounts of TLR than healthy individuals, which might trigger a stronger immune response. This response is driven by CD4+ T cells proliferation in secondary lymphoid tissues to T helpers in the presence of the antigens and cytokines nearby. T helpers (Th) differentiate depending of the cytokines at which they are exposed. Th type 1 are driven by exposure to IL-12 secreted primarily by dentritic cells. Th2 are driven by cytokines secreted by macrophages. The imbalance between Th1 and Th2-promoting cytokines determines the intensity and duration of the inflammatory response in experimental colitis [36]. Th17-promoting cytokines are less well characterized in human. Treg cells differentiate after exposure to cytokines IL-10, IFN-\\(\\gamma\\) and TGF-\\(\\beta\\). Overall the presence of certain cytokines and the response to self-antigens are factors leads to an inflammation and damage that is related to the onset and establishment of IBD On these kind of diseases autologous hematopoietic stem cell transplantation has shown some benefits on IBD [37]. The benefit of hematopoietic stem cell transplant (HSCT) in autoimmunity is thought to originate from the depletion of auto-reactive cells regardless of their specificity. However, due to its associated risk this therapy is only given when patients are refractory to all available therapeutic options. 1.1.1.4 Environmental Chronic inflammatory disorders and neoplasms have become the main cause of morbidity and mortality in the countries with high standards of personal cleanliness. A decrease in human exposure to microbes or hygene which might affect the proper maturation of the immune system so that it provides less immune response or exacerbated response towards “friendly” microbes [38, 39]. From other environmental factors related to IBD such as tobacco, diet, certain drugs and stress; tobacco is the most influential environmental factor. Surprisingly, it has an opposite effect on UC and CD: in CD tobacco is a risk factor that increases the risk of relapse and/or surgical intervention. In UC, it has been observed that smoking cessation worsens the disease [40]. Pharmacological treatments such as oral contraceptives, non steroidal anti-inflammatory drugs are also related to develop or relapse the disease [41, 42]. The psychological welfare of people also plays an important role in the disease progression, stress, anxiety and depression might be important in relapse and deterioration of the disease [43]. Other environmental factors have been linked to IBD but without enough evidence to support a causative effect in the development of the disease. 1.1.2 CD physiology As previously introduced, CD is a chronic inflammatory disorder characterized by a discontinuous inflammation of the gastrointestinal tract. Inflammation on the gastrointestinal tract is transmural and can affect from the mouth to the anus, but mainly it manifests on the ileum and colon [9]. It is frequently associated with extraintestinal manifestation and/or concomitant immune-mediated diseases. The disease itself manifest an heterogeneous symptoms that can involve, diarrhea, weight loss, abdominal pain, fever, anorexia and malaise. Other less frequent co-occurring manifestations are arthritis, primary sclerosing cholangitis, skin disorders venous or arterial thromboembolism and/or pulmonary involvement [44] . These symptoms make it hard to correctly diagnose the disease by non-specialists, in addition there is not a non invasive easy procedure to diagnose it. All these can lead to delays on correct diagnosis of the disease. The detection of parasites or bacteria, such as Clostridium difficile, have been associated with CD [45]. The detection of fecal calprotectin, is generally a good marker of endoscopic activity with sensitivity above 70% and specificity above 80% [46, 47]. Usually the best diagnosis method is to perform a colonoscopy, whether there is inflammation on the gastrointestinal tract on discontinuous regions then is CD. This inflammation could also present ulceration with rectal sparing and histological lesions which also help to diagnose the patients [48]. Usually on CD a granulome, that is a region with big multinucleous cells, can appear on any intestinal layer. In addition to the inflamed location(s), mosaic zones (patches of inflamed and non-inflamed areas) are more characteristic of CD [49]. The Montreal classification aims to classify patients according to their age of disease onset, standardized anatomical disease location an disease behavior. This classification assumes that the location of CD remains stable over time after diagnosis but behavioral phenotypes change. Other scores consider area affected: Montreal classification allows for early onset of disease to be categorized separately those with age of diagnosis at 16 years or younger, diagnosis at 17–40 years and &gt;40 years, respectively [11]. SES-CD: simple endoscopic score for CD [50]. Score based on size of ulcers, ulcerate surface percentage, affected surface and presence of narrowing on the bowel. CDAI: CD Activity Index takes into account weight, ideal body weight, sex, and events on the last week such as liquid stools, abdominal pain, general well-being and if anti-diarrhea drug usage, as well as knowing if there are fistulas, fever, and other complications [51] To some extent, there is a disassociation between clinical symptoms and the endoscopy finding. Often patients report feeling better despite lack of muscular healing [52]. To overcome this disassociation and be able to compare the well-being of patients several scores and thresholds are used on research and by physicians that will be described later. In the early stages of the disease the relapsing and remitting course is more frequent. Often relapses are accompanied by clinical symptoms, and few have prolonged clinical remission (without treatment) [53]. When there is clinical remission, there can still remain some other lesions and often subclinical inflammation persists. Frequently the damage caused by the disease evolves to fibrostenotic stricture or penetrating lesions (fistula and abscess). Damage of the disease might not be apparent to patients and might be only seen several years later than the first detection [52]. Mucosal healing is a first step towards the healing of deeper layers of the inflamed bowel wall on the CD. Patients might progress from an inflammatory phenotype to a stricturing or penetrating one [11]. Stricturing is a narrowing of a part of the intestine often because of scar tissue and fibrosis in its wall. Penetrating is when the epithelium has some holes or tubes. If these tubes result in an abnormal connection between two body parts it is a fistula, it might also result in an abcess, a collections of pus, often developed in the abdomen, pelvis, or around the anal area. 1.1.3 UC physiology As previously introduced, UC is a chronic inflammatory disorder characterized by a continuous inflammation of the colon. Depending on the inflamed segments of the intestine it is classified in several phenotypes. Around a third of the patients with UC suffer proctitis, the inflammation of the rectum. If the segments from rectum to the sigmoid colon are affected is a distal colitis, if it affects the left colon then it becomes a left colitis. If the inflammation continues to the descending colon it is then an extensive colitis until it affects the whole colon when it becomes a pancolitis. The extension of UC is inversely related to the frequency. However, the extension and severity of the disease correlates: the prognosis is worse the more extended it is [54]. In addition, the damage usually consists in many neutrophil in the lumen crypt [49]. The goal of the clinical care is to recover. As a first step, the symptoms of IBD have to lessen to the point that they are mostly absent, gone, or barely noticeable, this is known as clinical remission. However, this is not enough as the mucosa might be still inflamed and thus the reconstitution of the structure and function of the intestinal mucosa is not complete. Other lesions, might aid to the progression to other phenotypes such as fibrostenotic stricture or penetrating lesions or primary sclerosing cholangitis [55]. To prevent and avoid further damage several procedures are followed: When there is dysplasia, an abnormal development of cells within tissues or organs (which is considered a precedence before colorectal cancer growth [56]), or the damage on the colon has been too big a surgical procedure to remove part or all of the colon must be done. Patients that undergo a colectomy need to have their bowel reconnected with a procedure called ileoanal anastomosis (also know as J-pouch by the shape it takes) surgery. Often the lining of the pouch created during surgery becomes inflamed on what is known as pouchitis [57]. Many scores have been proposed for several purposes, from quality of life to disease severity or patient status. Among the scores most used are the following: Mayo: A score designed to be simple to calculate based on stool frequency, bleeding, mucosal appearance at endoscopy and physicians assessment [58]. IBDQ: A 32 questionnaire used to assess the quality of life grouped into four categories: bowel, systemic, social and emotional [59]. UCEDIS: An endoscopic score based on vascular pattern, internal bleeding and erosion and ulcers [60]. Other measured parameters include, weight, effective weight, fecal calprotectin, C reactive protein and hemoglobine. 1.1.4 Treatment Current treatment attempt to induce and keep the remission of patients and reduce secondary effects of the disease instead of revert the pathogenic mechanisms. As standard of care corticosteroids, aminosalicylates and immunosuppressor and some other drugs like antibiotics or metronidazol are util in some cases. Acid 5-aminosalicylic (5-ASA or mesalazina, pentasa) can be given in a topic way (either liquid, enemas, or suppository) or in oral form (pills or dilutions). In CU it helps in the clinic remission but it does not always mean that there is remission (is twice much likely than placebo to reach remission) [61]. On CD the effects are not so stark and generally it does not produce changes on the disease [62]. Antibiotics, such as metronidazol and ciprofloxacina, are effective to deal with secondary effects of IBD such as abscess and bacterian overgrowth in CD [63, 64], but they do not seem effective on UC [64]. Corticosteroids can be taken orally, such as prednisolona, prednisona and Budesonide; intravenous, hidrocortisona, metilprednisolona; or via enemas and suppositories. Budesonide is not absorved well and has a limited biodistribution but it has good therapeutic benefits with a reduced systemic toxicity in IBD [53, 65]. These drugs work very well as antiinflammatory for mild or severe IBD but do not work well as maintenance drug [66, 67]. Thiopurines (Azathioprine, mercaptopurina) are immunosuppressants drugs that deactivate key process of limphocytes T that might trigger the inflammation. As a side effect they are toxic due to their interaction with nucleic acids [68]. on CD they are useful to induce and keep remission [69], while on UC they are used to keep the remission [70]. In the last two decades IBD treatment has moved from aminosalicylates, corticosteroids and immunomodulators to anti-TNF\\(\\alpha\\). Anti-TNF\\(\\alpha\\) drugs has changed IBD treatment as it reduced the hospitalization associated with previous treatments, reducing medical costs and risk of surgery as well as induce a better mucosal healing and quality of life for patients [71]. However, 20-30% of patients have no response to this treatments and another 30-40% lose response in a year [72]. Recently a new wave of drugs has been developed targeting different molecules such as vedolizumab, targeting anti-integrin\\(\\alpha 4 \\beta 7\\), ustekinumab, targeting both IL-12 and IL-23, risankizumab an anti IL-23, tofacitinib an inhibitor of JAKs, infliximab an anti-TNF\\(\\alpha\\). Patients might become refractory to drug. Thus, drugs do not have the same effect as previously and the dose might need to be increased with the risk of more secondary effects [1]. Surgery resection might be needed on these patients. Close to 35% of patients with UC will need to have a surgery resection, either due to complications or because the inflammation can not be controlled. Surgery usually removes the inflamed segment of the colon. The most common procedure used is a colectomy (whole colon removal), with ileostomy [73]. CD patients usually require surgery associated to complications like stenosis, abscess, and fistulas ) between 70% and 90% at some point of their lives [74]. Usually the surgery is limited to removing the inflamed segment but occasionally an ileostomy is required [75]. If the drugs fail to contain the inflammation and heal the mucosa doctors might recommend a different procedure. In some cases HSCT is recommended which have shown to improve the life of the patients [76]. This is a new procedure given only to the most extreme cases to reset the immunological state of the patient. To reset or hugely modify the microbiota fecal microbiota transplantation between different people is currently being explored [77]. 1.1.5 Summary IBD is a complex disease that impacts the health of many people for long time and with lasting impact on their quality of life. Current clinical care in some cases is enough to have a sustained clinical and endoscopic remission but most often is not enough and relapse is expected. Several factors, such as becoming refractory to drugs, intermittent course of the disease and lack of validated predictors of disease course or response to therapy make the treatment complex. Lack of knowledge of what are the factors cause of the disease make those treatments and drugs to be addressed to block further inflammation and damage, but cannot prevent it and often they do not stop it completely. 1.2 Integration studies on IBD Many studies have looked up to the origin of the disease. As seen, one of the hypothesis behind the maintenance of the inflammation involves the microbiome and the host epithelium. This has been studied using several data sources, mostly from sequencing data. The technical methods used to obtain the data of the inflamed tissue differ between extracted from biopsied samples at colonoscopy or from surgical samples. Those samples are usually used later on to diagnose or for research purpose. To obtain research-quality data it usually imply using techniques such as immunohistochemistry, histopathology, immunohistochemistry, fluorescence in situ hybridization and polymerase chain reaction. These techniques allow to measure or visualize where are the cells expressing certain proteins or genes, thus helping with the analysis validation. Furthermore several studies have been carried out to discover links between microbiome and the inflammation, followed by those looking for some relationship between genetics and the disease and more recently the metabolome. These studies, known as integration, multi-omic or interaction studies, usually use multiple sequencing assays as the bases of the analysis [78]. However, confirming causal interactions of the variables of each essay is difficult. To find relationships some articles use correlation [79], there are others that use a combination of methods from correlations, partial correlations to integrative methods [80–82] and network integrations [81]. Very rarely there is an experimental confirmation of the relationships between variables of the different assays because it is complicated to test an interaction and to set up the right conditions for the many variables that are accounted for on the integrations. One of the few methods published that shows an interaction between genes and microorganisms on IBD is to expose the ex-vivo sample or cell lines with microbiomes or supernatant of at their culture [83]. 1.2.1 Type of data used for integration analysis According to the data used, we can classify the studies: 1.2.1.1 Transcriptome Most of the integrations refer some other source of data to the transcriptomics of the patient. The transcriptome of patients derived samples has been extensively studied since the existence of microarrays. There are known marker gens of inflammation and many research focus on identifying prognosis predictors and treatment response prediction based on gene expression [84, 85]. Recently single-cell RNA-seq technology has enabled to estimate cell populations of the samples with better degree of success than bulk RNA-sequencing. Single cell technologies are starting to be used for integration. 1.2.1.2 Microbiome Many of the integration analysis on IBD are done between host transcriptome and the microbiome. These studies use datasets from IBD patients usually stratified by disease activity or severity of inflammation or location of the disease. Most of them are based on correlation analysis between the microbiome and RNA-seq [79]. Conclusions of these integrations range from finding differences on the correlation depending on the type of disease ([79]) to finding relationships with inflammatory genes [80]. 1.2.1.3 Genetics Genetics is the next most common data source used to integrate data on IBD. Most studies on genetics and IBD are genome-wide association studies. The genetic component is specially important on IBD that starts on children [13, 82, 86]. When using genetic data to integrate it with transcriptomics it is usually to understand how a genetic variant is affecting a gene expression. This has lead to the identification of expression quantitative trait loci (eQTL) [87–90]. 1.2.1.4 Metabolome More recently there have been an increased interest on the study of the metabolic stat of IBD patients, given that microorganisms interact with the host also via their products and metabolites. There is evidence some of these metabolomic products come from the microbiome [24, 83]. Some studies have integrated the metabolome with the RNAseq and state of the epithelium [91, 92]. 1.3 Integration The term “data integration” is widely used with varying meanings. According to the dictionary integration is defined as: “the process of combining two or more things into one” — Cambridge Dictionary Other words used are integration, and if specific to data from sequencing technologies, multi-omics or pluri-omics. Here integration will be used as it is the more general one and not restricted to omics or sequencing technologies. Since the beginning of the integration methods there have been many methods proposed [93]. Some of the early methods were initially used for surveying the agreement of different evaluating systems [94], others were developed for agricultural sciences [95] or food industry [96]. Some of these methods are specific for one application or data type while others are more generally applicable. Lately, the access to bigger datasets with more variables and often from the same samples has increased the focus of the research community on the methodologies available on several disciplines but mainly on biological sciences. The explosion of data on biological sciences has been driven by the new sequencing technologies that allow to measure thousand of variables of many samples at the same time. If done with multiple sequencing technologies it is usually referred as multi-omics methods, which usually only uses omic data. It is crucial to classify, review and compare tools available, as well as, to benchmark these tools against the same dataset as a way to provide clear recommendations to anyone wishing to use them [97]. Part of these efforts use the methods’ strategies to classify them [98, 99]. Following this view I will review the available integration methods according to several axis: type of data used, aim of the method, relationships between variables, relationships between samples, relationship between variables and samples, input data, mathematical framework and results of the method. 1.3.1 Classification of integration method Integration methods have very different properties that allow them to be classified and compared. Here I classify those meant to be used with omics datasets, with references to concrete methodology and in occasionaly to articles using them. 1.3.1.1 Data type: numeric or categorical The most important distinction in integration methods is what kind of data are combined. In general data can be divided between categorical and numeric variables, which are usually found in several fields. Sometimes clinicians want to understand the relationship between a phenotype they observe and the underlying mechanism. Usually this involves looking how metabolites, gene expression, methylation, number of variants a gene has, and other numeric variables are related to the observed phenotype (i.e. pain). Depending on the method’s aim it handles numeric and categorical data types or just one. Often they are used differently. The most common way to handle different types of data is converting the categorical values to a mock or dummy variable. For each categorical factor there is a new variable whose value is 1 if that sample had this factor and 0 otherwise. For instance, if the categorical variable has three values (A, B, C) it would be converted to A (1, 0, 0) B (0, 1, 0) and C (0, 0, 1). Often the number of variables created is one less than the number of factors that existed, on this example only A and B would be kept. This transformation allows to use categorical values as numeric variables. If the method only accepts categorical data but you want to provide numeric values usually those values are categorized. For example if a variable is (0.123, 0.25, 0.56, 0.78) one could make to categorical values like (“&lt;0.5”, “&lt;0.5”, “&gt;0.5”, “&gt;0.5”). The number of categories to use and how the numeric value splits depends on each case. Very rarely methods allow to use both data types as they are. If they allow so, it is usually for classification purposes only. 1.3.1.2 Objective The objective of any method is one of its most important defining properties. Data integration can be classified according to the (biological) question they try to answer. In general all of them aim to provide a better understanding of the relationships between the ’omics types. Often a single method is not enough and several methods are used on the same dataset. This is specially relevant when a potentially relationship between omics is discovered. For instance, checking that in a particular case or condition a given relationship is present, might require experimental confirmation. Most of the times one (or several) of the following results are expected from integration methods: Classification of the patients or samples One of the purposes of integration might be to use multiple sources of data to accurately describe how do samples or patients fit on a predefined possibles states. The objective here might be to accurately describe whether a patient has one or other related disease of a possible subset of diseases or phenotype [100]. An overview of the role of each individual omic in a biological system Sometimes the question is which omic method is the best describing a disease. This knowledge could prevent performing expensive tests and replace them by more affordable or easier technique that has enough predictive power or is sensitive enough and specific for the task. An example of this is the search of markers on blood to identify links between different cell populations [101, 102]. Finding a molecular signature. A signature is usually a group of features that describe/are representative of a cell type, a process or a stage. Identifying a subset of variables from the omics that are related is often a desired goal because it reduces the amount of variables allowing to perform experiments on the bench on just those that might be important. In other fields, such as machine learning, selecting the important variables is known as feature selection. There are several methods that are used to do this [103]. An example of this is when performing eQTL analysis, where a locus is related to a change in gene expression [87–90]. A predictive model Predictive models usually require a very good understanding of the current and/or past relationships, as well as, a good feature selection procedure. If a good model on previous data exists it might be used to predict future events. Sometimes, models are only built to predict events without being able to accurately understand the underlying mechanism. This kind of methods are used to improve treatment selection, diagnosis and prediction of prognosis [104]. Impute values Some methods aim to accurately guess the values of missing data given some other information. Missing values can happen for a variety of reasons from practical ones, like a sample not being available, to technical ones, such as laboratory mistake [105]. However, this is often a intermediate step to other goals. To complete these goals it is important to have enough statistical power to determine the significance of tests performed (if any) and to understand how complete are the data sources used on the integration [106]. Having more statistical power helps identifying the relationships one seeks when using this methods. 1.3.1.3 Relationship between variables and samples Depending on the amount of variables and samples used in studies can be classified in two types. Traditionally for each sample few variables are measured, for instance on a biopsy with RT-PCR only a few genes are measured, however with the new omics techniques (transcriptomics, metabolomics, methylomics, genomics), thousands of variables are measured for the same sample. This has lead to the following situation: More variables than samples For a single sample of RNA around 50k genome identifiers (genes, long non coding RNAs, iRNA, pseudogenes,…) can be measured. Which leads to the case where there are many more variables than samples. Thus high-throughput data analysis typically falls into the category of \\(p \\gg n\\) problems ( big p, little n), where the number of genes or proteins, \\(p\\), is considerably larger than the number of samples, \\(n\\). With such high number of variables the identification of the relevant variables is hard because variables will co-variate. When many variables are tightly correlated, discovering which one is important using just numerical methods is challenging. This is even more difficult when looking for causal relationships. More samples than variables This was the usual case when for instance, from a cohort of patients the temperature is measured along the stage of a disease: two variables, time and temperature for each sample. If there are more than 2 patients, then the number of samples is greater than the number of variables studied. This is described in the literature as \\(n \\gg p\\) (or big n, little p). Nowadays this is less frequent on the bioscience world, and does not causes trouble analyzing it because the high number of samples allows to accurately estimate the dispersion of variables. There are several methods available to estimate the number of samples required [107]. Having just the enough samples for the desired statistical power, however, might not be enough in case some samples are not correctly processed. In addition, variables might be separated on different blocks of variables. These blocks might be just of the same source or from multiple methods. Depending on the method this blocks might have special meaning: when all the data is joined in a single block it is known as superblock. 1.3.1.4 Relationship between samples Depending on the relationship between the samples, the questions answerable and the methods that work on them differ. A sample can have multiple or one data source, for instance we could have RNA-seq and 16S data from the same sample. In a study if all the samples have all the data from multiple data sources it is a complete case. If some samples have data from some data sources but not from others the study is not a with a complete case. Sometimes because the sample is not enough, or there are some technical or organizational problems a source of data for a sample (which is known as an incomplete case) might be lost. This results in a new source of variation that has to be dealt with, which complicates the conclusion one can draw from the studies of these kind of data. Even when all the cases of a patient are complete the samples can come from several sites of the same individual or with different combinations of variables, which makes it relevant to understand the relationships between the different samples. There is no easy classification of this as each experiment might be designed differently. In general, experiments are designed to be as consistent as possible but in face of adverse events that become a variation of the design the analysis complicates. Either some data is imputed or some samples are omitted for the analysis. This can happen with samples taken at different timepoints as patients for instance if they miss a follow up visit. Time As mentioned above, time is one of the factors that sometimes cannot be controlled, despite having programmed visits every two weeks, for instance, some patients might come early or later due to multiple reasons. Sometimes, the objective of the study is precisely to analyze the relationships at different time, or to asses how the relationships change with time. To discover causality between two variables the cause must precede the consequence, which highlights the importance of time. Being aware of the time differences and time scales is crucial in most cases. During in vitro experiments, conditions can be reproduced even if they are at different time. However, when using patients material replicates can not usually be performed like in vitro experiments. This makes it harder to study time-related change on patients. Lastly, time between the collection of a fresh samples and its processing also influences the readings of the samples of the omics technology, specially RNA-seq [108, 109]. Some genes are more influenced by time than others but as they are measured at the same time in all samples this might distort the data. Keeping track of the time that it takes to process samples is also hard to due and requires a highly coordinated effort [110]. 1.3.1.5 Relationship between variables Once data is collected, the next step is to understand the relationship between the variables present. As mentioned earlier, some variables influence others which can affect the outcome in complex ways. With many variables present in a dataset it is important to be aware of known relationships between variables. Even in a simple dataset, like an RNA-seq dataset, it is important to be aware of the relationships between variables. Since the discovery of the lactose operon it is known how some genes regulate each other [111]. However, it is not know how other variables are related between them. For instance, how does the increase in expression of a gene affects the growth of a microorganism? Usually the relationships between variables are mediated by many factors or interactions. One of the best examples of such interactions is when some variables correlate. Their correlation can be used to reduce the number of variables being analyzed by ignoring the relationships between them and using the most representative variable (less widely correlated and with more variation). This step is usually done by dimension reduction methods. However, sometimes this is not desirable or feasible as the correlation does not explain the direction of the causality of the interaction between the variables (if there is any). Network approaches relate the variables to each other [112]. These approaches are fairly new and growing in popularity partly because they can address the direction of the interaction. In partial correlations the effect of other variables on the two being under study are taken into account [113]. They assumes a linear relationship between the co-occurring variables and those of interest. However, it is computationally expensive when there are thousands of variables. 1.3.1.6 Input data We have classified the studies according to the data they use (as seen in 1.2.1 ). But, some methods to account for relationships of variables only work when a dataset is complete while other do not: Data from the same samples: These methods do not handle well or at all missing data. They need complete cases/data of the samples in order to be able to integrate the results. These methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [114, 115], Multiple co-inertia analysis (MCIA) [116], Multi-Study Factor Analysis (MSFA) [117], Multi-Omics Factor Analysis (MOFA) [118] and STATegRa [119]. Data from different samples: These methods do not need data from the same sample. They draw their conclusions generalizing from the data available. Some of them handle missing data, while others do use the data at face value. These method includes MetaPhlAn2 [120], HUMAnN,[121] and LEfSe [122]. Furthermore, some methods are designed to integrate specific types of datasets, (usually because they make some assumptions that are only met on that kind of data). For instance, HCG, 16S rRNA-seq, RNA-seq and metabolomics do not share the same data distribution, and are different between them. Also even with the same data depending on the processing of the data they can have very different properties: OTUs (operational taxonomic unit) properties are not the same as ASV (amplicon sequence variants) when analyzing 16S rRNAseq data. 1.3.1.7 Mathematical framework Methods use different mathematical frameworks to process the data. Here I briefly describe some common mathematical frameworks, some of which have previously appeared: Networks Networks methods were mentioned because they use and find information about the interaction of variables. Multilayer networks, including the multiplex, Molti-C-DREAM [123], Random Walk with Restart on Multiplex and Heterogeneous Biological Networks RWR-MH, Random walk with Restart on Multiplex RWR-M [124]. Network embedding MultiVERSE are some of the methods using networks [125]. Bayesian approaches are also quite frequent, these methods use the Bayes’ theorem to see the relationships between variables. The Bayes’ theorem explains that the conditional probability of a variable is related to the prior knowledge of conditions that might be related to the event [126]. Some methods that use these approaches are Reconstructing Integrative Molecular Bayesian Network (RIMBANET) [127] and Bayesian Consensus Clustering (BCC) [128]. Dimensional Reduction These methods focus on finding just a few variables and summarizing them using a function that has some desired property such as the correlation between thre transformed variables is maximal while the components are orthogonal.s The selection of variables is usually done with L1 or Lasso Regression regularization technique or L2 also known as Ridge Regression. L1-regularization adds a penalty equal to the absolute value of the magnitude of coefficients which might leads to some coefficients becoming zero and the variable eliminated from the model. On the other hand, L2-regularization does not result in elimination of coefficients or sparse models and can only be used when there is multicollinearity as it works well to avoid over-fitting. Several tools use this approach, Momix [129], regularized canonical correlation analysis (RGCCA) [130], mixOmics [131] and STATegRa [132]. Other methods use bayesian approches like the Bayesian Group Factor Analysis [133]. Active module identification Multiomic objective genetic algorithm (scores based in two metrics; node score and density of interactions score). An example of a method using this approach is Multi-Objective Genetic Algorithm to Find Active Modules in Multiplex Biological Networks (MOGAMUN) [134] Usually depending on the mathematical framework used, these methods return similar outputs. 1.3.1.8 Output results According to the output the integration methods can be classified in several groups: For the network methods the following output is usually returned: Connections between the variables/nodes, a measure of how strong is the connection (or simply if there is a connection or not). For dimensional reduction methods there are three possible outputs: Shared factor across the data, specific factors for each data or mixed factors. Shared factors: Integration results in a vector of the samples in a lower dimensional space that is shared by all the data set used to integrate. Such methods include iCluster, Multi-Omics Factor Analysis (MOFA) [118]. Specific factors: Integration results in several vectors of the samples in a lower dimensional space of each data set used to integrate. Such methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [114, 115], Multiple co-inertia analysis (MCIA) [116] and Multi-Study Factor Analysis (MSFA) [117]. Mixed factors: Integration results in both shared and specific factors, to each dataset and common to all them. Such methods include Joint and Individual Variation Explained (JIVE) [135] and integrative Non-negative Matrix Factorization (iNMF) [136]. 1.3.2 Interpretation How to interpret the results of applying the different methods is highly linked to understanding the method and its output. On a correlation between two variables, the interpretation of the analysis is clear, if one variable increase, the other one too. The implications of this correlation can be far reaching but the principles to understand them are simple. However, on more complex methods the interpretation becomes less clear. The interpretation of a canonical correlation analysis is much harder [137]. Also on more complex methods the number of parameters required increases so the time and intellectual effort to understand the relationships between the parameters is also higher. The interpretation also helps to discuss the results and relate it to other previously know information. Individually: Here we study how each variable relates to another. In the correlation analysis, the relationship between two variables under study. Or if looking by patient: how do interpret that in these patient variable A and B is X and Y? Globally: In a principal component analysis for instance how do we interpret that some variables have the same loading? What happens in a more complex method like canonical correlation analysis? There are some articles about how to interpret those methods on real datasets [138]. Others, to benchmark and to learn how to interpret propose analyzing a simulated dataset [139, 140]. Which is used to compare the results of the integration with the dataset of interest and to compare different tools. These datasets are created with some relationships that the tools are expected to find. There exists several methods to create synthetic datasets like MOSim [141], metaSPARSim [142], CAMISIM [143], ballgown [144], polyester [145] and even edgeR [146]. These methods are useful to compare different setup but they can miss some subtle not previously reported relations on real data. 1.3.3 Reviews The comparison and review of methods independently from original authors have become a crucial step for selecting the right tool to apply a given dataset and research question [129]. Some of these reviews focus on a specific type of data integration: metabolomics [103], genomics [10], microbiomics… Others focus on the disease and the challenges of each omics and the need of an integrative approach to provide better therapies [106, 147, 148]. On this regard there are several efforts to integrate data on IBD but no comprehensive review to date is known to the author. The most comprehensive article to date is a very recent review identifying problems and providing recommendations for future work [149]. 1.3.4 Summary The field of integration is large and complex, with increasing interest over the last few years, specially in the psychology and omics fields As a methodology they are quite complex and diverse but there is a growing interest on them to help answer complex questions without using other complex tools like deep neuronal networks or other machine learning approches (despite not being incompatible). Methods to integrate have many characteristics, depending on the objectives and data that available. Regardless of the method used, interpretation and reporting is usually the main challenge. References "],["hypothesis-and-objectives.html", "Chapter 2 Hypothesis and objectives 2.1 Hypothesis 2.2 Objectives", " Chapter 2 Hypothesis and objectives 2.1 Hypothesis I hypothesize that the presence of certain microorganisms are related to the patient well being. Furthermore, the expression state of the epithelium and the amount of bacterial presence might identify whether patients are suffering an IBD. Finally, I believe that environmental factors interact with the IBD and are responsible of the big variability on the IBD discourse. 2.2 Objectives Identify microorganisms and genes related to the IBD. Genes and microorganisms that are related to healthy patients Genes and microorganisms that are related to IBD patients while controlling for environmental factors. Only related to Crohn’s disease Only related to ulcerative colitis Related to both Effect of the genes and microorganisms related to the IBD. "],["materials-and-methods.html", "Chapter 3 Materials and methods 3.1 Datasets 3.2 Processing samples 3.3 Integration methods 3.4 Regularized generalized canonical correlation analysis 3.5 Statistics 3.6 Functional enrichment methods 3.7 Variance and diversity methods 3.8 Other methods", " Chapter 3 Materials and methods This chapter contains a brief description of the main characteristics of the different datasets used on this thesis . The complete processing protocol before obtaining the data of those dataset that not generated at Hosptial Clínic can be found on their respective reference. Samples of the different cohorts collected on the Hospital Clínic were collected similarly and described only once. Differences between protocols are noted on the respective dataset’s section. It also describes all the methods used to analyze data from the multiple cohorts used on this thesis. The actual code used can be found on the links provided on the appendix. 3.1 Datasets 3.1.1 Puget’s dataset The glioma dataset is the data provided as an example of biological data by the authors of RGCCA from a previous publication [150]. The data came from diffuse intrinsic pontine glioma patients whose transcriptome was analyzed with Agilent 44K Whole Human Genome Array G4410B and G4112F. The copy number variation of the patients was processed with the ADM-2 algorithm, and data from comparative genomic hybridization (CGH) analyzed using Mutation Surveyor software. In addition, this dataset contained information on age, localization of the tumor, sex and a numerical grading of the severity of the tumor [150]. Table 3.1: Table about Puget’s dataset characteristics. Characteristic Puget’s Samples 53 Sex (female/male) 28/25 Location (cort/dipg/midl) 20/22/11 3.1.2 HSCT dataset Samples from the HSCT dataset used in this thesis were from a cohort of patients with severe refractory CD undergoing hematopoietic stem cell transplant. Patients were treated in the Department of Gastroenterology (Hospital Clínic de Barcelona –Spain–). The protocol was approved by the Catalan Transplantation Organization and by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7244). All patients provided written consent following extensive counselling about being included on the study and using their data on publications. Colonic and ileal biopsies were obtained at several time points during ileo-colonoscopy, at inclusion and every six or twelve months after HSCT until 4 years after the start of the treatment. Samples were obtained when possible from both uninvolved and involved areas. In addition, biopsies were taken from the ileum and colon regions of 19 non-IBD controls consisting of individuals with no history of IBD and who presented no significant pathological findings following endoscopic examination for colon cancer surveillance (Hospital Univesitari Mútua de Terrassa –Spain–). The protocol was approved by the Institutional Ethics Committee of the Hospital Univesitari Mútua de Terrassa (Study Number NA1651). At least one biopsy was collected and fresh-frozen at -80°C for microbial DNA extraction. The remaining biopsies were placed in RNAlater RNA Stabilization Reagent (Qiagen, Hilde, Germany) and stored at -80°C until total RNA extraction. In total 158 samples with both RNA and DNA extraction of the same segment and time were available: Table 3.2: Table of HSCT dataset characteristics. Characteristic HSCT Sex (female/male) 22/15 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 7/11/0 Years of disease: mean (min-max) 14 (8-28) Age: mean (min-max) 44 (23-70) Samples (non-disease/CD) 51/107 Location (ileum/colon/unknown) 48/108/2 SES-CD local: mean (min-max) 2.15 (0-12) CDAI: mean (min-max) 120 (0-450 3.1.3 Häsler’s dataset An IBD-related dataset was obtained by Prof. Dr. Rosentiel and Prof. Dr. Robert Häsler [151]. Biopsies were obtained endoscopically during routine diagnosis RNA was extracted. DNA from the 16S rRNA gene was amplified with primers 319F and 806R. Both RNA and DNA was then sequenced on HiSeq 2000 and MiSeq respectively. These biopsies included samples from the terminal ileum and sigma from CD, UC, infectious disease-controls and healthy controls. The dataset included information about location, gender, location, age, and the status (inflamed or non-inflamed) of the region from which the biopsy was taken. Table 3.3: Table of Häsler’s dataset characteristics. Characteristic Häsler’s Samples (non-disease/diseased) 33/26 Sex (female/male) 42/17 Location (ileum/colon) 30/29 3.1.4 Morgan’s dataset A previously published dataset from a pouchitis study was analyzed [152]. On this study patients having undergone proctocolectomy with ileal pouch-anal anastomosis for treatment of UC or familial adenomatous polyposis at least 1 year prior to enrollment were recruited at Mount Sinai Hospital (Toronto, Canada) excluding individuals with a diagnosis of CD. The dataset has a total of 255 samples from 203 patients, containing data for both host transcriptome and intestinal microbiome. On some cases several biopsyes were collected from the same patients. This dataset included anonymous identifiers for patients, whether the sample was from the pre-pouch ileum (PPI) or from the pouch, the sex, the outcome of the procedure and an inflammatory severity score ISCORE. The pouch ileum might be inflamed or not. Table 3.4: Table of Morgan’s dataset characteristics. Characteristic Morgan’s Samples 255 Sex individuals (female/male) 101/102 Location (Pouch/PPI) 59/196 3.1.5 BARCELONA dataset All patients with an established diagnosis of IBD, including Crohn’s disease, ulcerative colitis, IBD unclassified, indeterminate colitis, or pouchitis, starting treatment with a biologic agent were monitored following the schedule of clinical visits, laboratory tests, imaging procedures and biologic sampling at the beginning of their treatment with anti-TNF therapy and after 14 weeks and 46 weeks a biopsy from an ileocolonoscopy. The protocol was approved by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7845 and HCB/2012/7956). Patients that were referred to the Hospital Clínic de Barcelona IBD unit, who had already started treatment with a biologic agent in another center, were also included adapting to the corresponding time-schedule of their treatment. In all patients, starting anti-TNF treatment will be decided before the protocol entry decision according to medical clinical practice. Anonymized identification of the patients, disease, sex age at diagnostic, age at the moment of the sample taking, time since the start of the treatment and sample segment was collected. Table 3.5: Table of samples included from the BARCELONA dataset characteristics. Characteristic BARCELONA Individuals 62 Status (CD/UC/Controls) 33/21/8 Sex (female/male) 29/33 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 2/44/8 Years of disease: mean (min-max) 7.6 (0-32) Age: mean (min-max) 41 (18-68) Time (0/14/46 weeks) 41/40/32 Sample segment (ileum/colon) 39/87 3.1.6 Howell’s dataset This dataset was downloaded after the publication of an article “DNA Methylation and Transcription Patterns in Intestinal Epithelial Cells From Pediatric Patients With IBDs Differentiate Disease Subtypes and Associate With Outcome” [153]. A cohort of 66 treatment-naïve children at diagnosis of their IBD, along with 30 age- and sex-matched non-inflammatory control children, were recruited by the Paediatric Gastroenterology team at Addenbrooke’s Hospital (England). From their dataset data from 77 samples that had both RNAseq and 16S data was used. There are 10 non-IBD samples, 11 with Crohn’s disease and 11 with ulcerative colitis. Data has the following characteristics: disease, age at diagnostic, age at the moment of the study, sex, segment, and clinical history which have the following characteristics: Table 3.6: Table of samples included from Howell’s dataset characteristics. Characteristic Howell’s Disease (CD/UC/controls) 10/11/11 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 32/0/0 Age: mean (min-max) 12 (6-15) Sex (female/male) 10/22 Segment (ileum/colon) 31/46 Clinical history (inflammation/no inflammation) 24/53 3.1.7 Hernández’ dataset This dataset was obtained after a collaboration “Integrative analysis of colonic biopsies from IBD patients identifies an interaction between microbial bile-acid inducible gene abundance and human Angiopoietin-like 4 gene expression” [81]. Patients with UC, CD were recruited when attending regularly scheduled visits or surveillance. In addition, asymptomatic healthy controls (HC) were recruited during routine, age-related colorectal cancer screening by colonoscopy. 290 samples were collected together with information about the disease, age at diagnosis, age at the moment of the sampling, sex, segment and smoking status. Table 3.7: Table of samples included from Hernández’s dataset characteristics. Characteristic Hernández’ Disease (CD/UC/controls) 54/66/46 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 29/73/18 Age: mean (min-max) 40 (17-71) Sex (female/male) 81/85 Smoking (never/ex/current) 115/34/16 Segment (ileum/colon) 97/193 3.2 Processing samples 3.2.1 Transcriptome sequencing Total RNA from mucosal samples (HSCT cohort) was isolated using the RNAeasy kit (Qiagen, Hilde, Germany). RNA sequencing libraries were prepared for paired-end sequencing using HighSeq-4000 platform. Later, those samples with good enough quality as recommended by FastQC afterwards cutadapt (version 1.7.1) [154] was used for quality filtering and the libraries were mapped against the human reference genome using the STAR aligner (2.5.2a) with Ensembl annotation (release 26 of GENCODE, GRCh38.p10 or superior) [155]. Code used with STAR STAR \\ --outSAMtype BAM SortedByCoordinate \\ --outFilterIntronMotifs RemoveNoncanonical \\ --outSAMattributes All \\ --outReadsUnmapped Fastx \\ --outSAMstrandField intronMotif \\ --outFilterScoreMinOverLread 0.5 \\ --outFilterMatchNminOverLread 0.5 \\ --outFilterType BySJout \\ --alignSJoverhangMin 8 \\ --alignSJDBoverhangMin 1 \\ --outFilterMismatchNmax 999 \\ --outFilterMismatchNoverLmax 0.04 \\ --genomeDir &quot;$genome/STAR&quot; \\ --limitBAMsortRAM 10000000000 \\ --runMode alignReads \\ --genomeLoad NoSharedMemory \\ --quantMode TranscriptomeSAM \\ --outFileNamePrefix $output \\ --runThreadN &quot;$threads&quot; \\ --readFilesCommand zcat \\ --readFilesIn &quot;$file1&quot; &quot;$file2&quot; Read counts per gene were obtained with RSEM (version 1.2.31) [156] as previously described [157]. Code used with RSEM rsem-calculate-expression \\ --quiet \\ --paired-end \\ -p &quot;$threads&quot; \\ --estimate-rspd \\ --append-names \\ --no-bam-output \\ --bam &quot;$rseminp&quot; &quot;$genome/RSEM/RSEM&quot; &quot;$rsem&quot; 3.2.2 Microbial DNA sequencing Biopsies from the HSCT CD cohort were resuspended in 180 \\(\\mu\\)l TET (TrisHCl 0.02M, EDTA 0.002M, Triton 1X) buffer and 20mg/ml lysozyme (Carl Roth, Quimivita, S.A.). Samples were incubated for 1h at 37°C and vortexed with 25 \\(\\mu\\)l Proteinase K before incubating at 56°C for 3h. Buffer B3 (NucleoSpin Tissue Kit–Macherey-Nagel) was added followed by a heat treatment for 10 min at 70°C. After adding 100% ethanol, samples were centrifuged at 11000 x g for 1 min. Two washing steps were performed before eluting DNA. Concentrations and purity were checked using NanoDrop One (Thermo Fisher Scientific). Samples were immediately used or placed at -20°C for long-term storage until DNA sequencing. 3.2.2.1 DNA sequencing Library preparation and sequencing of the HSCT dataset were performed at the Technische Universität München. Briefly, volumes of 600\\(\\mu\\)L DNA stabilization solution (STRATEC biomedical) and 400\\(\\mu\\)L Phenol:choloform:isoamyl alcohol (25:24:1, Sigma-Aldrich) were added to the aliquots. Microbial cells were disrupted by mechanical lysis using FastPrep-24. Heat treatment and centrifugation were conducted after adding a cooling adaptor. Supernatants were treated with RNase to eliminate RNA. Total DNA was purified using gDNA columns as described in detail previously [158]. Briefly, the V3-V4 regions of 16S rRNA gene were amplified (15x15 cycles) following a previously described two-step protocol [159] using forward and reverse primers 341F-ovh/785R-ovh [160]. Purification of amplicons was performed by using the AMPure XP system (Beckmann). Next, sequencing was performed with pooled samples in paired-end modus (PE275) using an MiSeq system (Illumina, Inc.) according to the manufacturer’s instructions and 25% (v/v) PhiX standard library. On the BARCELONA dataset the DNA was processed equally in house. The only difference was using different 16S-V3V4 primers pair 341f/806r on a MiSeq Nano sequencing by the RTSF Genomics Core at Michigan State University. The sequence of the primers used was: 341f: 5’-CCTACGGGAGGCAGCAG-3’ 806r: 5’-GGACTACHVHHHTWTCTAAT-3’ The result of MiSeq Nano were processed with bcl2fastq (v1.8.4). 3.2.2.2 Microbial profiling For the HSCT dataset the processing of raw-reads was performed by using the IMNGS (version 1.0 Build 2007) [161] pipeline based on the UPARSE approach [160]. Sequences were demultiplexed, trimmed to the first base with a quality score &lt;3 and then paired. Sequences with less than 300 and more than 600 nucleotides and paired reads with an expected error &gt;3 were excluded from the analysis. The 5 nucleotides from each end of the remaining reads were trimmed to avoid GC bias and non-random base composition. Operational taxonomic units (OTUs) were clustered at 97% sequence similarity. Taxonomy assignment was performed at 80% confidence level using the RDP classifier and the SILVA ribosomal RNA gene database project. Later the data was normalized using the same method as for RNA-seq described above. The microbiome was visually inspected for batch effects in PCA; none were found. The resulting OTUs table was normalized using edgeR (Version 3.28 or later) [146]. For all the other datasets dada2 [162] (Version 1.14 or later) was used to analyze microbiome data. It creates amplicon sequencing variants from the 16S sequencing data, without merging similar sequences at any threshold. It is an alternative to OTUs which allows to be able to compare results between studies because it does not summarize together any sequence and provides more resolution to differences on the fragment amplified. 3.3 Integration methods 3.3.1 MCIA Multiple co-inertia analysis, also known as MCIA, is a method to examine covariant gene expression patterns between two blocks [163]. It is implemented on the package omicade4. On its core MCIA maximizes the following formula: \\[ \\sum_{k=1}^K w_k {cov}^2 (X_k Q_k u_k, v) \\] where \\(K\\) is the total number of matrices, \\(X_k\\) the transformed matrices and \\(Q_k\\) is a square matrix with \\(r_{ij}\\) in diagonal elements indicating the hyperspace of features metrics, \\(u_k\\) are auxiliary axes, \\(v\\) the reference structure and w the weights of the matrices. This can be used to obtain a dimension of \\(P_k^d=u_k^d(u_k^d Q_k {u_k^d}^T)^{-1} u_k^d Q_k\\) given that for each dimension the residuals are obtained following \\(X_k^{d-1} = X_1^d- X_1^d P_1^{d-1}\\) where \\(d\\) are the dimensions needed. It was used as a baseline method to compare the RGCCA integration. 3.3.2 STATegRa To explore how much do different blocks of a dataset have in common STATegRa was used [164]. It is a framework for integrating datasets with two data types using parametric and non-parametric methods. The methods used are omics component analysis based on singular value decomposition (SVD) of the data matrix. There are three different methods provided to this end: DISCO-SCA, JIVE and O2PLS. On DISCO-SCA it uses: \\[ X_k = TP_k ^ T + E_k \\] with \\(T\\) the \\(I \\times R\\) matrix of components scores that is shared between all blocks and \\(P_k\\) the \\(J_k \\times R\\) matrix of components loadings for block \\(k\\). Let \\(X_1,X_2, \\dots X_i\\) be blocks of data and \\(X=[X_1,X_2, \\dots X_i]\\) represent the joint data, then the JIVE decomposition is defined as: \\[X_i=J_i+A_i+\\epsilon_i \\text{, }i = 1,2, \\dots\\] where \\(J=[J_1,J_2, \\dots J_i]\\) is the \\(p \\times n\\) matrix of rank \\(r&lt;rank(X)\\) representing the joint structure, \\(A_i\\) is the \\(p_i\\times n\\) matrix of rank \\(r_i &lt; rank(X_i)\\) representing the individual structure of \\(X_i\\) and \\(\\epsilon_i\\) are \\(p_i \\times n\\) error matrices of independent entries. Finally, the O2PLS approach uses multiple linear regression to estimate the pure constituent profiles and divides the systematic part into two, one common to both blocks and one not. The O2PLS model can be written as a factor analysis where some factors are common between both blocks. \\[ \\text{X model: } X = TW ^ T + T_{\\text{Y-ortho}} P ^ T_{\\text{Y-ortho}} + E \\] \\[ \\text{Y model: } Y = UC ^ T + U_{\\text{X-ortho}} P ^ T_{\\text{X-ortho}} + F \\] \\[ \\text{Inner relation: } U = T + H \\] Were each model is build similarly by adding the subtraction of the projected values of the other component keeping the relationship between them as stated on the third line. 3.3.3 RGCCA The main method used on this thesis has been regularized generalized canonical correlation analysis (RGCCA) a method derived from the canonical correlation. Canonical correlation is a method that uses data about the same unit but from different origins to find how much the different sources agree. Regularized generalized canonical correlation analysis is implemented on the homonymous package RGCCA [130] which was used on this thesis. The method and implementation will be explained in detail on the next section. Figure 3.1: Workflow of the analysis process 3.4 Regularized generalized canonical correlation analysis To understand the regularized canonical correlation analysis first a brief description of principal component analysis is provided. A brief description of the canonical correlation method will be used to explain the general case. Finally the regularized generalized canonical correlation analysis will be introduced. Principal component analysis defines a new orthogonal coordinate system that optimally describes variance in a single dataset. It does so by decomposing the numerical matrix into the eigenvalues and eigenvectors with decreasing variance. Its results include new variables (the eigenvectors) and a vector of loadings or weight indicating the importance of the original variables for these new variables. Typically, data is normalized and standardized to avoid that a variable with higher variance and different scale dominate the results. The canonical correlation analysis [165, 166] is a method to find agreement between two, or more, scorers (as it was first introduced on the literature) or sources. It extends the PCA to a two sources of data. It provides a similar output, new variables and weights of the existing variables indicating their importance. The function it maximizes is: \\[ \\rho = \\underset{a, b}{\\mathrm{argmax}} (\\mathrm{cor}(a^T X, b^T Y)) \\] Being \\(a\\) and \\(b\\) random variables that given \\(X\\) and \\(Y\\), the two data sources, maximize the correlation between \\(a^T X\\) and \\(b^T Y\\), the first canonical variables, the new variables. \\(a\\) and \\(b\\) are the weights. Over several years of progress on the field of canonical correlations [114, 115, 167–171] regularized generalized canonical correlation analysis (RGCCA) emerged with a generalization from canonical correlations extending the procedure to more than two sources of data [R-RGCCA?] and being made more flexible generalizing from other proposed methods. 3.4.1 Description RGCCA works with numeric matrices, as big as you want, as it is designed for datasets with more variables than samples ( \\(p \\gg n\\)). However, for each sample it needs to have a complete case with no missing values (at the time of writing this thesis there is a new development version, not released on CRAN2 yet, that replaces any missing value by a 0) and time is not considered as a special variable. It uses a dimensional reduction approach to relate the different blocks of data between them and produce specific factors for each dataset. The objective function is: \\[ \\underset{a_1,a_2, \\dots,a_J}{\\text{maximize}} \\sum_{j, k = 1}^J c_{jk} g( \\mathrm{cov}(X_j a_j, X_k a_k)) \\mathrm{~~s.t.~~} (1-\\tau_j)var(X_j a_j)+\\tau_j \\Vert a_j \\Vert^2 = 1, j=1,\\ldots,J \\] Being \\(X_j\\) the values from sample \\(j\\), the weights of the variables of said sample are represented by \\(a\\). While \\(g\\) is a function that can take the form of \\(x\\), also known as Horst method, \\(|x|\\) known as centroid method, \\(x^2\\) known as factorial method, or any user-supplied function. \\(C\\) is a symmetric matrix describing the network between blocks. The shrinkage parameter is defined as: \\[ \\widehat{\\lambda}^{\\star} = \\dfrac{\\sum_{i\\neq j}\\widehat{Var}(r_{ij})}{\\sum_{i \\neq j}r_{ij}^2} \\] Where the \\(r_{ij}\\) are the correlation coefficients of the matrix between variables \\(i\\) and \\(j\\) . Where the variance is defined as: \\[ \\widehat{Var}(S_{ij}) = \\dfrac{n^2}{{(n-1)}²} \\widehat{Var}({w}_{ij}) = \\dfrac{n}{{(n-1)}^3} \\sum_{k=1}^n ( w_{kij} - \\overline{w_{ij}})^2 \\] And its components are: \\(w_{kij}=(x_{ki}-\\overline{x}_i)(x_{kj}-\\overline{x}_j)\\) and \\(\\overline{w}_{ij}=\\frac{1}{n}\\sum_{k=1}^nw_{kij}\\) representing \\(x_{ij}\\) the values of a sample \\(j\\) on a variable \\(i\\). The authors realized that there is a special problem due to sparsity on biological data which could be handled using first another normalization to improve the stability and success of the canonical correlation methodology. The method to perform the dimensional reduction using the sparse method consists on maximizing the same equation but with a different constraint: Specifically, RGCCA with all \\(\\tau_j = 1\\), combined with an L1-penalty that gives rise to SGCCA: \\[ \\underset{a_1,a_2, \\dots,a_J}{\\text{maximize}} \\sum_{j, k = 1}^J c_{jk}g( \\mathrm{cov}(X_j a_j, X_k a_k)) \\mathrm{~~s.t.~~} \\Vert a_j \\Vert_2 = 1 \\text{ and } \\Vert a_j \\Vert_1 \\le s_j, j=1,\\ldots,J \\] The \\(s_j\\) controls the sparsity estimated of the data, the smaller it is, the higher sparsity of \\(X_j\\) is. As \\(s_j\\) is closer to 0, more features are selected as it looks to optimize covariance; while if it is closer to 1, less features are selected and it resembles the correlation. The values of \\(s_j\\) were estimated using Schäfer method [172] when the block was from 16S data or RNAseq, otherwise 1 was used. As previously mentioned, there are different \\(g\\) functions that could be used; but the centroid method was chosen to detect both positive and negative relationships. Categorical data was encoded as binary (dummy) variables for each factor except one to keep degrees of freedom, where 0 indicates not present and 1 indicates present. One level was omitted to avoid overfitting the data. Each block, regardless if it had continuous numeric variables or dummy variables was standardized to zero mean and unit variance. Later, it was divided by the square root of the number of variables of the block for an unbiased estimation. 3.4.2 Output RGCCA as other dimensional reduction techniques provides specific weights for each variable on each dimension and a sample score on each dimension, together with quality scores. To measure the quality of the model, the implementation provides indicators based on the Average Variance Explained (AVE). RGCCA returns an AVE score for each block, which measures how the variables of the block correlate with the dimension component of the block. It also provides two AVE scores of the whole model: the inner, which measures how each dimension accounts for the variance, and the outer, which measures how variables correlates with the dimensions component. The closer the inner AVE is to 1, the better the model adjusts to the data. However, that mathematically works better does not mean that it have more biological meaning or that it provides more insights on the biology. It can also generate results as other related methods based on the maximization of a function of correlations: SUMCOR (sum of correlations method) [173], SSQCOR (sum of squared correlations method) [174], SABSCOR (sum of absolute values of the correlations method) [175]. Others are based on the maximization of a function of covariances: SUMCOV (sum of covariances method), SSQCOV (sum of squared covariances method), SABSCOV (sum of absolute values of the covariances method). The following table summarized the equivalent parameters needed on RGCCA to work like these methods: Table 3.8: Equivalences of RGCCA for multi-block data analysis to other methods Method Scheme Normalization Shrinkage SUMCOR Horst \\(Var(X_j a_j) = 1\\) 0 SSQCOR Factorial \\(Var(X_j a_j) = 1\\) 0 SABSCOR Centroid \\(Var(X_j a_j) = 1\\) 0 SUMCOV Horst \\(\\Vert a_j \\Vert = 1\\) 1 SSQCOV Factorial \\(\\Vert a_j \\Vert = 1\\) 1 SABSCOV Centroid \\(\\Vert a_j \\Vert = 1\\) 1 There are other methods that can be done with RGCCA. Canonical correlation analysis would be equivalent to data variance equal to 1 and shrinkage of 0. Partial least squares (PLS) regression, which maximizes covariance, would be equal to data variance of 1 and shrinkage of 1. Redundancy analysis could be performed with one block weights’ normalized value is 1 and the variance of the other equal to 1. 3.4.3 Models There is no formal definition of what constitutes a block of data on multi-omics tools. Most multi-omics and integration tools assume one block for each type of data, such as an essay a survey or an experiment. We decided to split the block with data about the samples to separate independent variables from the same block. The hypothesis we made was that more blocks with highly related variables but independent from the other blocks would fit better the data and thus help to identify causal or dependent variables. To model what might be the relationships within datasets current practices include using a pre-selected model of relations between blocks. However, this model might not be an accurate representation of the relations between blocks and several models might need to be fitted. To help find the fitting model for the data I created an R package, named inteRmodel, which helps finding the right model to the data via a bootstrapping procedure. This method was applied to the previously described datasets to find the relationship between microorganisms and the disease. Following this method; to provide a ground truth, a model with only the relationships between the two experimental obtained data is analyzed, on what it is called the model 0. The next models analyzed consisted on relationships between the two experimental blocks and a block with all the metadata of the samples. These models are denoted by 1.Y, where 1 denotes the family 1 and Y is used to label some of the models of this family. Later instead of a big metadata block, following our theory we split this metadata block on several ones, having a block for time related variables, another one for location and the other about the people on the study. This allows to design a model with an expected relationships between these blocks and makes more interpretable the relationships. These models are from the family 2 denoted by the name 2.Y, where 2 denotes the family of the model and Y change for particular models with different relationships between the blocks change. For each family of models we tested all possible relationships with weights between 0 and 1 by 0.1 intervals to find the best model on each datasets according to the AVE score. The final models were further validated using a bootstrap approach to measure their accuracy and likelihood on the data available. 3.5 Statistics Differential expression analysis was performed with the limma-trend method [176, 177] and edgeR [146] (Bioconductor version 3.10 or superior) packages. Data was normalized using the trimmed mean of M-values and log-2 transformed into counts per millions following the workflow described on [178] using voom. The samples were adjusted for inter-patient differences blocking for patient. To correct for multiple testing, the false discovery rate was estimated using the method of Benjamini and Hochberg [179]. A gene was considered differentially expressed when it was significant at 5% FDR special attention was given to those genes that showed a fold-change higher than |1.5|. 3.6 Functional enrichment methods Functional enrichment methods are those methods that aim to provide with more information about the variables besides their numerical value measured. They can be very different in nature but they all use the numeric values of the variables and other information, being it from the same experiment data collection or from external data sources. Many functional enrichment methods are based on an over representation analysis, where a group of elements is tested whether they are present on other group. This can be done with clusterProfiler which tests genes enrichment for functionality based on information on pathway databases [180] and used on several publications [181]. It checks the enrichment of features of a given group on the (background) list provided. \\[ \\begin{cases} H_0 &amp; : &amp; P_{subset} \\leq P_{overall} \\\\ H_1 &amp; : &amp; P_{subset} &gt; P_{overall} \\end{cases} \\] The test used is usually the fisher test, the hypergeometric test or the proportion test. I describe the hypergeometric test and the proportion test below. 3.6.1 Hypergeometric test The hypergeometric distribution describes the probability of \\(k\\) successes (when the object drawn has a specified feature) in \\(n\\) draws3, from a finite population of size \\(N\\) that contains exactly \\(K\\) objects with that feature, wherein each draw is either a success or a failure. Applied on this context \\(N\\) is the number of genes being used and \\(n\\) the number of genes on a pathway. So it can be used to compare the genes found on it (\\(k\\)) compared to the expected \\(K\\) numbers of the distribution using the following equation: \\[ P_X(k) = P(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}} \\] 3.6.2 Proportion test The overrepresentation of a given group of elements can be also tested with the proportion test, which is sometimes also used on clusterProfiler. The proportion test uses the \\(\\chi^2\\) distribution to test if the observed frequency (\\(O_i\\)) is close to the expected frequency (\\(E_i\\)): \\[ \\chi^2 = \\sum_{i =1}^n \\dfrac{(O_i - E_i)^2}{E_i} \\simeq \\chi_{n-p}^2 \\] As this is usually done on a 2x2 contingency table it is equivalent to the Z-test of proportion. Sometimes, the expected frequency is so low that a correction must be done to the estimation: \\[ \\chi_{\\text{Yates}}^2 = \\sum_{i =1}^n \\dfrac{(|O_i - E_i | - 0.5)^2}{E_i} \\] This increases the \\(p\\)-value as it raises the Chi-square statistic. 3.6.3 Gene Set Enrichment Analysis There are other methods that to test if some variables shows an unexpected importance according to a statistic like fold change or value gene set enrichment analysis was developed [182] . Gene set enrichment analysis (GSEA) is a computational method originally developed to determine whether a priori defined set of genes shows statistically significant and concordant differences between two biological states. This methods check if a group of variables present in an ordered list is present in some skewed distribution and compare it against a random group of similar size. It has been widely used since its original publication, also on IBD [183]. This method consist on first calculating the rank of genes \\(rank(g_j)=r_j\\) where each \\(g\\) is a gene. Then later calculate the following functions: \\[ P_{hit}(S, i)=\\sum_{g_j \\in S, j \\leq i}\\dfrac{|r_j|^p}{N_R} \\text{, where } N_R = \\sum_{g_j \\in S}|r_j|^p \\] \\[ P_{miss}(S, i) = \\sum_{g_j \\not \\in S, j \\leq i}\\dfrac{1}{N - N_H} \\] With these values the enrichment score (ES) defined as: \\(ES=max(|P_{hit}(S, i)-P_{miss}(S, i)\\vert)\\) is calculated from the walk. At least 1000 permutations are usually used but a high number of permutations are required for an accurate score of the enrichment score. However, when more than one pathway (\\(S\\)) is evaluated in order to compare between the enrichment scores between them, they must be normalized by dividing the scores by the mean of all the ES. When power \\(p\\) is 0 it is equivalent to the standard Kolmogorov–Smirnov statistic, though it is usually set to 1. For testing gene set enrichment analysis I used fgsea [184] implementation for its speed and integration with other methods used on this thesis. Pathways of genes from the REACTOME database were tested on the weight of different models or on the comparisons performed [185]. 3.6.4 GSVA To estimate the expression of the pathways and compare their expression levels between conditions gene set variation analysis as implemented on GSVA was used [186]. It is a method that summarize the variables’ numerical value changing the space of \\(\\text{variable x sample}\\) to \\(\\text{group x sample}\\) . This enables other methods to use this new space instead of the original variables, which provides a successful way to look into data [187]. GSVA was used (again from the REACTOME database) to find the relationships between the pathways and the microbiome at different taxonomic levels. This is done via an estimation and a comparison with a discrete Poisson kernel: \\(i\\) indicates the gene till \\(k\\) and samples are indicated by \\(j\\) till \\(n\\). \\[ z_{ij}=\\hat{F_r}(x_{ij}) = \\frac{1}{n}\\sum_{k=1}^n\\sum_{y=0}^{x_{ij}}\\dfrac{e^{-(x_{ik}+r)}(x_{ik} + r)^y }{y!} \\] where \\(r = 0.5\\) is used in order to set the mode of the Poisson kernel at each \\(x_{ik}\\), that is, similar to a gene expression for a given sample. Later this is converted to ranks \\(z_{(i)j}\\) for each sample and normalized: \\(r_{ij}=|\\frac{p}{2}-z_{(i)j}|\\) to make the distribution of ranks symmetric around zero to later compare with a normal distribution using a Kolmogorov-Smirnov-like random walk statistic: \\[ v_{jk}(l)=\\dfrac{\\sum_{i=1}^l|r_{ij}|^{\\tau}I(g_{(i)}\\in \\gamma_k)}{\\sum_{i=1}^p|r_{ij}|^{\\tau}I(g_{(i)} \\in \\gamma_k)} - \\dfrac{\\sum_{i=1}^lI(g_{(i)} \\not \\in\\gamma_k)}{p-|\\gamma_k|} \\] Here \\(\\tau\\) describes the weight of the tail in the random walk (default is set to 1). \\(\\gamma_k\\) is the k-th gene set and \\(I(g_{(i)}\\in \\gamma_k)\\) is the indicator function whether the gene ranked i-th belongs to the gene set \\(\\gamma_k\\) . \\(|\\gamma_k|\\) indicates the ordination of the gene set, the number of genes of the gene set and \\(p\\) the number of genes in the data set. This difference is later converted to enrichment score for each gene set for each sample, similar to GSEA. This score can be calculated as a difference of hits and misses or the maximum deviation from zero of the random walk (which allows to detect gene sets that have genes with different opposing expression patterns). 3.7 Variance and diversity methods 3.7.1 PERMANOVA The PERMANOVA method [188, 189], provided by the vegan package on the adonis function, was used to test if microbiome data variance is due to other variables when using distances metrics. It uses the residual sum of squares such as: \\[ SS_W = \\frac{1}{n} \\sum_{i=1}^{N-1}\\sum_{j=i+1}^N d_{ij}^2\\epsilon{ij} \\] When using euclidian distances (\\(d\\)) it is equivalent to MANOVA. Here \\(\\epsilon_{ij}\\) takes the value of 1 if the observation \\(i\\) and the observation \\(j\\) are in the same group, otherwise it takes the value of zero. This can be later used to test which variance is bigger, inter-groups or intra-groups by using the following formula: \\[ F = \\dfrac{SS_A/(\\alpha -1)}{SS_W/(N-\\alpha)} \\] Where \\(SS_A\\) is the among group sum of squares, representing the intra-group variance. \\(N\\) is the number of samples and \\(\\alpha\\) the number of different groups. This allows to test if the variables are related to the variance of the data as it can be compared with the \\(F\\) statistic after a high number of permutations. 3.7.2 globaltest It is a methods for testing complex hypothesis and help decide if the variables were influencing [190]. I tested which variables, (sex, age, location, time since diagnostic, treatment) are important on the datasets with globaltest (Version 5.40 or later). It provides a general statistic to test hypothesis against a high dimensional dataset. \\[ S = \\sum_{i=1}^p x_i^{&#39;} x_i g(t_i^2) \\] The global test performs a test statistic on the transformed t-test, where if \\(p\\), the number of variables, is large the test is more powerful on average over all possible sparse alternatives of general functions \\(g\\). It was performed with variables individually and also with interactions between the different variables. 3.7.3 Diversity indices Microbiome diversity was measured using vegan and phyloseq methods [191]. \\(\\alpha\\)-diversity is a measure of how much a given microbiome at a taxonomic level is present on a sample. Several measures exists, on the thesis I used the effective Simpson or Shannon diversity index to compare diversity between samples and conditions. \\(\\beta\\)-diversity was calculated using the phyloseq package for exploratory analysis. The effective Simpson (also known as inverse Simpson) is: \\[ D_2 = \\dfrac{1}{\\sum_{1=1}^Sp_i^2} \\] The effective Shannon diversity is: \\[ D = \\dfrac{1}{-\\sum_{i =1}^S p_i \\log_e{p_i}} \\] Where \\(p_i\\) is the proportion of species \\(i\\) and \\(S\\) is the number of species. 3.8 Other methods 3.8.1 WGCNA To look for relationships between the microbiome and the RNAseq we used weighted gene co-expression network analysis. We used weighted gene co-expression network analysis as implemented on WGCNA [192] and also correlations. The Spearman rank correlation coefficient is Being \\((X_1 , Y_1 ),\\dots, (X_n , Y_n)\\), assign a rank where \\((R_1 , S_1 ), \\dots , (R_n , S_n )\\) for n being all the variables which: \\[ R_s(X,Y) = \\dfrac{\\sum_{i=1}^n (R_i - \\bar{R}) (S_i - \\bar{S} )}{\\sqrt{\\sum_{i=1}^n (R_i - \\bar{R})^2}\\sqrt{\\sum_{i=1}^n (S_i - \\bar{S})^2}} \\] where \\(\\bar{R}=\\dfrac{1}{n}\\sum_{i=1}^n R_i\\) conversely to S: \\(\\bar{S}=\\dfrac{1}{n}\\sum_{i=1}^n S_i\\). The distribution of the Spearman correlation coefficient is symmetric around 0 and can be approximated to a normal distribution as \\(\\sqrt{n-1}R_{s(X,Y)} \\sim N(0,1)\\) which can be used to calculate the p-value of a given estimation. 3.8.2 BaseSet BaseSet was developed and used to find which variables are really involved on the interaction and how likely were to be together more robustly. It is a package that uses fuzzy set logic to calculate the probability to belong to a group, in this case, those genes and bacteria selected by the model that interact with the other. Under the standard fuzzy set logic a set \\(S\\) is a group of elements for which each element \\(e\\) has an \\(\\alpha\\) membership to that set [193, 194]. \\(\\alpha\\) is usually bounded between 0 and 1: \\(\\alpha \\in [0, 1]\\) . A given element \\(e\\) can belong to more than one set. Assimilating the membership function to probability we can calculate the probability of a given element \\(e\\) to belong to a set \\(S\\) and not any other set: \\(P(e \\in S|e\\not \\in S^c)\\). Which applied to the data and the case at hand, it is the probability that a given variable is associated with a given outcome and not with any other outcome. The membership function was derived from the bootstraps used for each model on the thousand iterations of the integrative method applied to give an estimation of how probable is a given gene and bacteria to be selected as relevant for the model. With the probabilities of the bootstraps it was used to calculate (via set_size) the genes and bacteria that are specific of the model that allows to separate the transcriptome by its location and the microbiome by the disease status. 3.8.3 experDesign experDesign was developed [195] to prevent and quantify if a given experiment has batch effect due to the batches used to measure the values or other known variable. It might help to detect a bad design of the experiment. On pseudo code the core of the program can be described as: for each index: for each batch pick size(batch) samples if samples are in another batch pick other samples for each batch calculate some summary statistics compare with the summary statistics of all the samples keep the index with less differences between the index and all the samples Summary statistics taken into account are the median, the variance, the range, the number of missing values, and the entropy of the categorical variables. It can take into account spatial distribution and, given the number of samples that fit on a batch, provide which technical replicates4 are best to use. 3.8.4 ROC- AUC To estimate if the selected features (genes or microorganism) by the integration methods have some biological meaningful contribution I measured if they can classify features, such as, which gastrointestinal segment is each sample from, or which type of disease does each patient have. To compare between different models the area under the curve (AUC) of the receiver operating characteristic (ROC) was calculated with the pROC package [196]. It is based on the following formulas, where \\(\\text{FP}\\) is false positive, \\(N\\) is a negative, \\(P\\) is positive, \\(\\text{TP}\\) is true positive and \\(\\text{FN}\\) is false negative: \\[ FPR = \\frac{FP}{N}=\\dfrac{FP}{FP+TN} \\] \\[ TPR = \\frac{TP}{P}=\\dfrac{FP}{FP+FN} \\] The ROC curve is that where the true positive rate (TPR) or sensitivity, recall or hit rate is represented against the false positive rate (FPR) on the x axis. The area under this curve is a measure of how good such classifications performs overall, being 0.5 as good as a random selection. The higher it is to 1 the better as it classifies incorrectly less samples and accurately classify them. References "],["results.html", "Chapter 4 Results 4.1 Packages 4.2 Analysis", " Chapter 4 Results 4.1 Packages 4.1.1 experDesign experDesign package built in R was released for the first time on CRAN on 2020-09-08 after nearly a year after the initial release made on github. It was published on a journal on 2021-11-27[195]. The package uses functional programming to create and modify objects and the features used. The package bases its performance on the large body of work made by the R core team. It adds the information to the introduced data.frame or returns an vector with the appropriate information. experDesign functions are divided into several categories: Helper functions to aid on deciding how many batches are or how many samples per batch. There are some also that report how good a given distribution of the samples felt for a given dataset. Functions generating indexes. Functions distributing the samples on indexes Regarding time related variables experDesign will use them as factors, while issuing a warning to the user. Since its development it has been used on a couple of RNA sequencing experiments that required a batch design, one of organoids bulk RNA-seq and another one of biopsies bulk RNA-seq from the BARCELONA cohort. It was also used to check if there is any observable batch effect on the datasets analyzed. On the designed datasets experDesign worked well and no batch effect was created when sequencing the samples. However, on the organoids dataset, a change on the matrigel used to produce them introduced a batch effect that made it impossible to compare samples before and after that change (there were not any shared sample before and after the change of matrigel). Since its release on CRAN it has had a median of ~400 downloads each month from RStudio repository mirror. Showing the interest the community have on solutions like this. 4.1.2 BaseSet BaseSet package, built in R, was released for the first time on CRAN on 2020-11-11, nearly two years after the initial work started on github. The package uses both functional programming and object oriented program to create and modify the TidySet S4 object defined5. Mixing it with S3 generic functions it provides a powerful interface compatible with the tidyverse principles, a group of packages following the same design. The package provides a new class to handle fuzzy sets and the associate information. BaseSet methods are divided into several categories: General functions to create sets of the TidySet class or convert from it to a list or about the package. Set operations like adjacency cartesian product, cardinality, complement, incidence, independence, intersection, union, subtract, power set or size. Functions to work with TidySets to add relationships, sets, elements or some complimentary data about them. Remove the same or simply move around data or calculate the number of elements, relations and sets. Functions to read files from formats where sets are usually stored in the bioinformatician field: GAF, GMT and OBO formats Last, some utility functions to use set name conventions and some other auxiliary functions The package had a long development process with initial iterations basing on GSEABase package which was later abandoned (GSEAdv) to also include some uncertainty on the relationship of a gene with a given gene set. The package also participated on an exploration on part of the Bioconductor community (project to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assay) for more modern and faster handling of sets. There were three different packages created as part of this process, BaseSet, BiocSet published on Bioconductor and unisets, available on github. The three different approaches were presented at a birds of feather on BioC2019. The package passed the review on the rOpenSci organization (See review) and is now part of the packages hosted there too. Since its release on CRAN it has had a median of ~400 downloads each month from RStudio package manager. 4.1.3 inteRmodels The package was build once the method used to find accurate models of the relationships of the data available of a dataset using RGCCA was established. Using the package on github simplifies the process and makes it easier to redo it . The package has functions that can be grouped in three categories: To look for models and evaluate them. There are functions to search for a model given some rules, that check them using leave-one-out methodology. Reporting: To make better reports by improving handling of names or simplifying the objects or how to calculate scores. Building: To easier build correct models on RGCCA, simplifying the process to create a symmetric matrix. Currently it is only available on github, so the number of downloads and usage is unknown but since its release a user has contacted to keep it up to date with development versions of RGCCA. Currently, it is compatible with the next release of RGCCA being prepared6. 4.2 Analysis On the following sections the main results of analyzing each dataset are presented. 4.2.1 Puget’s dataset On this dataset the different parameters and capabilities of RGCCA were tested. The three different methods, centroid, factorial or horst were tested and compared. The main result of this comparison was that the differences of the selection of the variables mattered more than the number of variables selected with each method. The models were tested with different weights on all three schemes: horst, centroid and factorial. The horst and the centroid scheme were similar while the factorial resulted in the most different AVE values (see S1 Data of [198]). The centroid scheme was selected because it takes into account all the relationship regardless of the canonical correlation sign between the blocks and it is similarity to horst scheme. The effect of the sparsity value was measured by its effect on the inner AVE scores and the combination of the different values for each block. Figure 4.1: Effect of tau on the inner AVE. The suggested tau value is the column between the regular grid. Exploratory analysis with the superblock model was done. The first two components of the superblock did not help to explain the biology or classify the tumors: Figure 4.2: First components of the superblock The same data was used to look for a good model from the data itself including a model with a superblock but looking at the first component of the CGH and transcriptome block. This allowed to visually inspect if each model’s components helped to classify the samples: Figure 4.3: Different models tried with the same data showing the first components of the CGH data and the transcriptome. Showing the components of the CGH and the transcriptomics of the superblock show better classification than that of the superblock. However the other models show a better classification of the samples with much simpler models. To find these models the three blocks with the best tau and the centroid scheme were analyzed by changing the weights between 0 and 1 by 0.1 intervals. According to the inner AVE, the best model was the one in which the weights (1) between the host transcriptome and location, (2) the host transcriptome and the CGH, and (3) the CGH block were linked to variables related to the location with weights of 1, 0.1 and 0.1, respectively. When we added a superblock to the data, there was a slight increase of 0.01 on the inner AVE of the model. The model with the superblock that explained most of the variance was that in which the weights of the interaction within (1) the host transcriptome, (2) between the superblock and the CGH, (3) between the host transcriptome and the localization, and (4) between CGH and the host transcriptome were 1, 1, 1 and 1/3, respectively. To see if the superblock could classify the sample by location, we plotted the first two components of the superblock. We can clearly see that they do not classify the samples according to the location of the tumor, which is known to affect the tumor phenotype [150]. Adding one block containing the age of the patient and the severity of the tumor to the model, decreased the inner AVE. The best model with these blocks, according to the inner AVE, was that in which the interactions (1) within the host transcriptome, (2) between the host transcriptome and the localization, (3) between the host transcriptome and(4) the CGH and between the CGH and the other variables were 1, 1, 1/3 and 1/3, respectively. The first components of each model can be seen in the figure: We can observe on the figure 4.3, the strong dependency between gene expression and location since the first model while the weaker relationship with the CGH assay [150]. On the other hand, the major difference is the dispersion on the CGH component on each model. The effect of the superblock and weights on different models to the inner AVE. There are significant differences between having the superblock and not having it. Figure 4.4: Effect of superblock and weights on the inner AVE 4.2.2 HSCT dataset The permanova analysis was performed on this dataset to estimate which were the variables that are more relevant. From the many variables the location, sex, patient id and others were found to be related to the variability of the microbiome or the transcriptome on this dataset. With the permanova analysis we found that more of the 50% of the variance of normalized RNA-seq data and microbiome data respectively is explained by the variables of location, disease, sex, and the interaction between disease and sex. On the transcriptome the most important factor is location which is more than 15% of the variance, while on the microbiome data the most important factor is the patient id followed by location of the sample. With globaltest the results were similar. The resulting p-value was well below the 0,05 threshold defined for RNA-seq data on the models including the segment of the sample, sex and treatment. On the microbiome data the results were similar but the p-value was considerably higher but still below the threshold. Figure 4.5: Alpha diversity according to Shannon and Simpon effective measures Diversity indices of the samples were explored and compared for several subsets. Splitting by location of the sample and disease provided the highest differences and the diversity index along time did not change much. Weighted gene co-expression network analysis did not provide relevant links between bacteria and transcriptome as it failed to find an acceptable scale free degree. Figure 4.6: Power evaluation of WGCNA of the HSCT dataset. As can be seen on the Figure 4.6, the scale free topology doesn’t reach the recommended threshold of 0.9 and the mean connectivity is also very low even for the first power. STATegRa was used between stool 16S data and intestinal 16S data under the assumption that there is a shared common factor without influence of other categorical variables. However, it didn’t find a good agreement between these two data sources and 16S data source was not longer used on the analysis. In addition, the model is fixed, so it did not allow to find new or other relationships that are not one to one. With RGCCA we could select different models and use all the data available without much assumptions. The models with the highest inner AVE of the family 1 and the family 2 models were similar to those on the Häsler dataset. Figure 4.7: On the x-axis the transcriptome, on the Y axis the Microbiome. Each square represents a different model of the HSCT dataset. On panel A colored by disease status, on panel B colored by sample location. The weights of these models can be observed here: The best model of the family 2 confirmed a relationship between the host transcriptome and the location-related variables, while the microbiome was associated with the demographic and location-related variables (see Figure and S2 data of [198]). Overall, we see that the relationships in the model affected the distribution of samples on the components of both the host transcriptome and the microbiome. The different models selected different variables, some of which are shared between models. The most similar models are those that have split the metadata into 3 blocks, followed by those that have the metadata in a single block. In order to analyze the accuracy of the models, one thousand bootstraps were used to integrate the data from the HSCT CD dataset. Each bootstrap had its own dispersion on the variables according to the samples selected, the distribution of the bootstraps used are represented here: Figure 4.8: Dispersion of the bootstraps on the age and percentage of colon and controls samples. Evaluating the same model on each bootstrap lead to a dispersion on the inner AVE of the model. The lower the dispersion, the more robust the model was to different conditions than in the initial testing. Figure 4.9: The point with the black circle is the AVE of the original data. The dispersion is shown by the ellipses With the bootstrapped models we used BaseSet to estimate the probability that each variable to be relevant for the association with a disease. However, due to big amount of small probabilities when using the BaseSet package to calculate which variables are more relevant it could not provide a good estimation on time. MCIA was applied as a baseline of the integration, the first two components were represented similarly to those of the blocks when using RGCCA. Figure 4.10: MCIA first two synthetic variables on the IBD related cohorts. The AUC of classifying the transcriptome in colon or ileum segments was compared between the two methods. Figure 4.11: ROC curve of the different datasets with the models from RGCCA and the result with MCIA The different models selected different variables as can be seen below: Figure 4.12: Upset plot of the variables selected on each model showing the intersection between the different models Differences and similarities between the selected features of each model can be observed on Figure 4.12. Genes are very similar between model 0 to 1.2 and between 2 to 2.3, while OTUs are very unique on model 0 and others shared between most models. 4.2.3 Häsler’s dataset In this dataset, the parameter tau behaved slightly differently than with the previous dataset but the value from the Schäfer’s method for tau was close to the best value. In contrast to the HSCT’s dataset, the model with the highest inner AVE was model 1.2 (inner AVE value of ) but model 2.2 was close to it (inner AVE of ). Model 2.2 has a relationship of 0.1 between microbiome and the host transcriptome and of 1 between the location and the host transcriptome. The microbiome block is also related by a factor of 0.1 with the demographic block and of 1 with the time block. Lastly, the time and the demographic block are related by a factor of a 0.1. In either case the family 1 and family 2 models can correctly separate by sample location (colon or ileum) but not by disease type or inflammation status as can be seen below. Figure 4.13: Models on the Häsler’s dataset There is no observable cluster of IBD samples and the other samples, showing that on this dataset the differences of the microbiome between the different type of samples are less stark. MCIA was applied as a baseline of the integration and compared to the different models to know which one separates best colon and ileum samples. Figure 4.14: ROC curve of the different datasets with the models from RGCCA and the result with MCIA MCIA results was high but not as high as the model 2.2. 4.2.4 Morgan’s dataset We tested if results of inteRmodel were consistent on this dataset. The different models were not able to separate the samples neither by location or sex. Figure 4.15: First component of the transcriptome and microbiome of the Morgan’s dataset. Nevertheless we compared the classification with the MCIA algorithm and still resulted that model 2.2 provide a better classification than MCIA. When exploring the bootstraps of the data we found that model 1.2 is highly variable: Figure 4.16: Inner and outer AVE scores of the bootstrapped models. In addition the model 2.2 usually has a lower inner AVE compared to model 1.2. 4.2.5 BARCELONA dataset This dataset was processed but the results are not trustable as the microbiome diversity falls off the accepted values. Diversity plots 4.2.6 Howell’s dataset This dataset was processed to confirm the results on the previous datasets. Model 1.2 was the best according to the AVE score but perform worse when attempting to recreate known biological differences via classifying samples. Model 2.2 was selected. Figure 4.17: The three main models on the Howells dataset. Colored by section colon, ileum and shape according to the disease: square, ulcerative colitis; triangle, normal; circle, Crohn’s disease. Model 1.2 has a 0.1 relationship between the ASV and the transcriptome and 1 between transcriptome and metadata. While model 2.2 has a relationship of 1 between location and transcriptome and demographics and ASV but only of 0.1 between demographics and location. Figure 4.18: Bootstrap of the different models. The bootstrapping showed that model 1.2 has indeed higher inner AVE values than model 2.2 and is more stable than model 1.2. While model 0 shows a high variation according to which samples are selected. On this dataset we also focused on the most important ASV according to the model 2.2 that were present in more than 2 samples that in total were present in the whole dataset. These ASV were summarized to a single value and then used to calculate the AUC, which was 0.85. The dot product of the ASV and genes were also calculated and used to find out which ASV are related to which genes. 4.2.7 Hernández’ dataset This dataset was processed to confirm the results on the previous datasets. The models are. Model 1.2 is, model 2.2. References "],["discussion.html", "Chapter 5 Discussion 5.1 Relevance for other research 5.2 Implications of research", " Chapter 5 Discussion Häsler data data were obtained using the same sequencing techniques from endoscopic biopsies as our dataset HSCT and Barcelona and it is of the same disease/field. The confirmation that it works on it it helped to continue forward. Transciptome is related to localization of the samples. Microbiome is more related to the disease and other variables of the patients. Time is an important variable when modelling the disease, if multiple timepoints are taken they should be taken into account to identify the state of the disease for each patient. What happened with the BARCELONA cohort and the organoids cohort (not described here) highlights the importance of the quality check of the data. On the first case it was a clear batch effect that at the time of sequencing it was unavoidable and could not be corrected. On the BARCELONA cohort despite no batch effect or other quality issues with the data, the diversity indices made impossible using it. Comparing different datasets is complicated because each is collected with different goals and processed differently. We looked up for the most similar datasets in order to be able to confirm our results. Although on each section we analyzed multiple datasets. Shared selected variables BaseSet did not work for its intended usage on this thesis. It failed because it is computationally expensive to calculate the likelihood of 1500 variables, there are too many combinations. In addition, the numeric precision of said calculations suffers from the flotating point problems and must be considered carefully. To support multiplying more than 1000 float numbers a different strategy such as using log values might be better. We could not come up with a better strategy to find all the combinations needed, perhaps a better method exists that could be used to find which are the terms more influential to the end result. In this chapter we will summarize the main findings in relation to the broad research community and other work as well as the impact of the results to improve further research in the future or be used on clinical environments. If we had to summarize all the work in one sentence we would say that… Many work focus on finding some genes or bacteria to answer a question they have in mind. On the thesis the focus was more on finding a good representation of the disease that allowed me to identify genes and bacteria that were relevant to our questions. There are many methods for integration developed, each with their shortcomings and strengths. There are frequent new tools and methods released, and the most up to date list of tools might be added to a collaborative list (from which the appendix table of methods has been taken). With the increase of tools there have been more pressure on comparing different tools. There have been already some articles comparing the different tools, which the previous mentioned list also has. There are reviews of different tools on the same datasets [199], some are more theoretical, others are focused on a different field. Recently there has been a recent article focused on integration on IBD [149]. WGCNA did not work well. As it is a tool designed to find common patterns based on correlations it requires homogeneous samples. Having samples from multiple sites and conditions did not help. adonis worked to identify which variables were important. MCIA works well as baseline. STATegRa not customizable the interaction between the datasets. RGCCA some improvements. Multi-omics seems to focus on metagenomics, metatranscriptomics and metaproteomics and abandon plain 16S sequencing. [200] How to move the transition from bench to hospital [201]. New technologies like “organ-on-chip” or more specifically “gut-on-chip” will be of great help in identifying causality by introducing the gut microbiota in this system and then study the interaction of the gut microbiota and the intestinal epithelium. 5.0.0.1 Designing models The model of the generalized canonical correlation is highly dependent of the blocks present. If one has preexisting theories about the data, a specific model can be used stating these known or hypothetical relationships. However, if new relationships are being explored or no prior beliefs on the data are held the models should be created with random links between blocks, and evaluate which model is better. 5.0.0.2 Evaluating models To evaluate a model RGCCA provides the average variance explained (AVE), inner and outer. Inner AVE is for how well do all the canonical dimensions correlate with the design of the experiment, so it a measure of how good the model is. While outer AVE is a measure of how well do the variables of each block correlate with the canonical dimension, so it measures the agreement between the variables and the canonical dimensions. Depending on the goals of the research one or the other should be used. If we are more interested on the model of the relationships the inner AVE makes more sense. Furthermore, to evaluate a design bootstraping can be used to know how well the design does apply to a variety of data. Another option is to use an external cohort to validate the same model, or using a different method to see if it finds the same relationships or explains the data as accurately. Of the multiple methods available we used MCIA [202]. Which was compared by looking at the area under the curve for classifying the samples according to their location. Besides a way to compare methods, these models do need to be evaluated by the insights they provide on the biological system they are being applied to, in our case the Crohn’s disease. In this article we did not look in depth to the biological relevance of the microorganisms an genes found. The procedure of separating independent variables in their own block of data and later search the best model that fits the data provides a good strategy that should be consider for integration efforts. The procedural method of searching a model and testing them is implemented on inteRmodel. But the most important thing is to consider which variables are independent of which and if they can be separated into a block for later usage on the modeling. 5.0.1 Other Quality of RNAseq analysis and 16S sequencing is important. Avoid contamination, same primers and read lengths (if using Illumina machines). Comparing 16S taxa ASV or OTUs is hard and selecting the right tool to compare them is important [203]. Some batch effects are avoidable due to the experDesign package. This might help to improve the quality of the datasets and to expand previously sequenced datasets. [204] A paper about multi-omics and microbiome… [93] advice and recommendations on software engineering and reproducibility practices to share a comprehensive awareness with new researchers in multi-omics for end-to-end workflow. 5.1 Relevance for other research It is hard to provide more information that can be later used by researchers on the wet lab. Paper about using transcriptomics to infer inflammation without colonoscopy [205] There is a disconnect between the computational side and the experimental side, driven by the difficulty to think an experiment to test the new information that multi-omic experiments provide. Many combinations possible that are hard to explore, further research on how to reduce the space of possible combinations of bacteria or evaluate which combinations are more important might be useful on the future. As this might help focus on the most promising bacteria. 5.2 Implications of research Translational research? How to apply this results ? Sequence just the ASV on patients to classify them on disease might be an option that would require further clinical validation. Selecting which genes are related to the microorganisms is will require also further validation (for which I did not have the creativity to think of). The several environmental factors highly affect the disease, so analysis or comparisons without taking into account them might provide misleading or false results. References "],["conclusions.html", "Chapter 6 Conclusions", " Chapter 6 Conclusions Microbiome under some dataset can classify the patients according to their disease. Doing so reliable might take some time. It is hard to make it generalizable. "],["acknowledgments.html", "Chapter 7 Acknowledgments", " Chapter 7 Acknowledgments Juanjo Pau + grup Nuria, Elena, Helena, Aida. Metges: Julià, Helena… Ana, Alba, Isa, Marisol, Maica… Azu Familia Twitter #rStats and other people on Bioconductor (support.bioconductor.org) Biostars, Bioinformatics SE Dr. Häsler for kindly providing metadata from their cohort and Dr. Cristian Hernández for their collaboration and selfless sharing the samples of their cohort. "],["references.html", "References", " References "],["appendix.html", "Appendix 7.1 Online resources 7.2 Software", " Appendix 7.1 Online resources Some links that I found useful on the thesis and could be useful if you are interested on the multi-omics field. Awesome multi-omics: An online repository of references to multi-omics methods. Reproduced here with their references 7: Table 7.1: Integration methods available and their references. Method Publication SCCA [206] PCCA [207] PMA [208] sPLS [209] gesca [210] Regularized dual CCA [211] RGCCA [114] SNMNMF [212] scca [213] STATIS [214] joint NMF [215] sMBPLS [216] Bayesian group factor analysis [133] RIMBANET [127] FactoMineR [217] JIVE [135] pandaR [218] omicade4 [163] STATegRa [164] Joint factor model [219] GFAsparse [220] Sparse CCA [221] CCAGFA [222] CMF [223] MOGSA [224] iNMF [225] BASS [226] imputeMFA [227] PLSCA [228] mixOmics [229] mixedCCA [230] SLIDE [231] fCCAC [232] TSKCCA [233] SMSMA [234] AJIVE [235] MOFA [236] PCA+CCA [237] JACA [238] iPCA [239] pCIA [240] sSCCA [241] SWCCA [242] OmicsPLS [243] SCCA-BC [244] WON-PARAFAC [245] BIDIFAC [246] maui [247] SmCCNet [248] msPLS [249] MOTA [250] D-CCA [251] COMBI [252] DPCCA [253] MEFISTO [254] MultiPower [107] &lt;!--# Check https://github.com/mikelove/awesome-multi-omics --&gt; Bookdown: The book about how to write this type of books. Bioconductor: The project about bioinformatics on R mostly related to sequencing technologies. CRAN: The main archive of R extensions/packages for R. GitHub: Company which allows to freely host remote git repositories of many projects, including some used or developed on this thesis. 7.2 Software Along the years of this thesis several pieces of software have been generated as well as packages. Here they are listed for easier retrieval. They are listed on two ways, one with a brief explanation and another one ordered by what software piece is used on each analysis. 7.2.1 Listed An improved/tested version of RGCCA, some modifications on the internal functions to ease the maintenance as well as adding tests and sometimes improving the documentation. Also modified so that it is possible to provide a vector of models so that the model of the first dimension is not the same as the model on the second dimension (not sure if mathematically speaking makes sense but from a biological one I think it might be interesting to have it). Designed to be used with RGCCA I wrote the package inteRmodel to ease the bootstrapping and model selection. A package to design batches to avoid batch effect experDesign and its website on GitHub. Explore the effects of the hyperparameters on RGCCA on the provided dataset of gliomaData (Originally provided here) there is this repository sgcca_hyperparameters. We used a pouchitis cohort published in this article[152] that was used to compare how performs our method in other’s dataset. The code used can be found in this repository. Some functions used to explore the TRIM dataset ended up in the integration package.This include functions for correlation, network analysis, enrichment, normalization of metadata… I developed a package to analyze sets and fuzzy sets BaseSet (based on what I learned from a previous iteration of the package). This package was meant to be used with the probabilities that arise from bootstrapping the models. However, due to the long times of calculation that it would require it was not used. To analyze the antiTNF cohort (also named BARCELONA) a different repository was created to analyze the data using the previously developed packages. 7.2.2 By project All code of the analysis of the publications is available (in his messed state and complicated history) and a brief description as to why they were used: Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis: TRIM: Mangle with the sample, dataset, explore several methods… sgcca_hyperparameters: Explore the effects of the hyperparameters on RGCCA on the provided dataset. inteRmodel: Package for easy repeating the methodology developed on TRIM. pouchitis: Work with the pouchitis cohort used in this article. uncoupling: Work with the UC/CD dataset used in this article. integration: Package with functions that I wrote or used on different parts of exploring the TRIM dataset ended up here. BaseSet: BaseSet: Fuzzy logic implementation, available on rOpenSci too experDesign: experDesign: Help design experiments in batches, available on CRAN too. References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
