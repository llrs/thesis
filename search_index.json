[["index.html", "Data integration on inflammatory bowel disease Preface", " Data integration on inflammatory bowel disease Llu√≠s Revilla Sancho 2021-02-04 Preface The main topic of the thesis is data integration applied in the inflammatory bowel disease (IBD) research. The data I will be integrating are different omics data and the phenotype of the patients. This disease is complex and there are hypothesis pointing that the microbiome is a major factor in the disease. In the precision medicine framework data integration is important to consider all the relevant variables that influence a disease. The thesis is performed on the IDIBAPS research institute. My colleges are biologist, microbiologists, veterinaries... and we have weekly meetings with the doctors visiting at the nearby hospital. The thesis program allows one to defend the thesis after 3 years, and up to 5 in total. My timeline is to finish in 2022 after 4 years (I'll add it when it is finished to see how it went) with the guide of my thesis directors Juanjo Lozano and Azucena Sala, who help as bioinformatician and disease expert respectively. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The inflammatory bowel disease (IBD) involves Crohn's disease(CD) and ulcerative colitis(UC). It generally affects the terminal ileum and the colon but it can affect any segment of the gastrointestinal tract. UC is a recurrent, chronic and continuous inflammation of the colon and rectum while the CD is not a continuous inflammation and affects the whole gastrointestinal tract. IBD etiology is unknown. However, once it has initiated the most prevalent hypothesis of its chronicity suggests an aberrant immunological response to antigens of the commensal microbiome. Treatments provided for the IBD are palliative. Those treatments include, noninflammatory drugs, suppressors and biologic. The therapeutic options can induce remission in some patients, however they often need continuous treatment to avoid recurrence. Nevertheless, many patients are refractory or intolerant to those therapies and need to undergo surgery. The disease present unique characteristics that require the usage of integration methods in order to find the specific relationship of the microbiome and the intestine on the disease. "],["integration.html", "1.1 Integration", " 1.1 Integration Integration is defined as: &quot;the process of combining two or more things into one&quot; --- Cambridge Dictionary Other words that are used integrati(-on, -ve), multi-omics, pluri-omics. However, here integration will be used as it is the more general one and not restricted to omics. Since the beginning of the integration methods there have been many methods proposed. Some of these methods are specific for one application or data while others are more general. On the recent years with the increase of more datasets with more variables than previously has seen an increase on the number of methodologies available on several disciplines but mainly on the biological science. This in turn has increased the importance of classification, review and comparison of the tools available, as well as, benchmarking these tools against the same dataset [1]. Other times only the strategies used are used to classify the methods [2, 3]. 1.1.1 Classification of integration method's We can characterize and classify integration methods in several ways. Here I outline some classifications in the bioscience field, with relationships (and references) to concrete methodology and usages. 1.1.1.1 Data type: numeric vs categorical The most important distinction in integration methods is what kind of data are combined. In general data can be divided between categorical and numeric variables, which are usually found in several fields. Sometimes doctors want to understand the relationship between a phenotype they observe and the underlying mechanism. Usually this involves looking how the metabolites, the gene expression, the methylation, the number of variants a gene has, and other numeric variables are related to the observed (categorical) phenotype (like pain). Depending on what is a method aimed for it handles both data types or just one, often they are used differently. The most common way to handle different type of data is converting the categorical values to a mock or dummy variable. For each categorical factor there is a new numerical value with 1 if that sample had this value or 0 otherwise. Sometimes Often the number of variables created is one less than the number of factors that existed. For instance, if the categorical variable has three values (A, B, C) it would be converted to A (1, 0, 0) B (0, 1, 0) and C (0, 0, 1) and usually one of these three variables is omitted. This transformation allows to use the categorical values in methods though for numeric variables. 1.1.1.2 Aim The results and methods of data integration depend on the objective of the method and on the (biological) question. Most of the times one (or all) of the following results are expected: An overview of the role of each 'omics in a biological system Sometimes several omics are used and the question is which omic method is the best one to describe the disease. Or several methods are used to understand better how two omics interact. A better understanding of the relationships between the 'omics types When the relationship between omics is known there are methods to find those relationships and improve upon previous knowledge. For instance, checking that in a particular case or condition there is a given relationship, and that this relationships follows our model or not. A molecular signature1 leading to more insight into molecular mechanisms The usage of multifactor analysis might lead to the need to redefine the features that are essential for identifying a phase or a cell line. A predictive model If a good model is known it can be used as a prediction if enough information is gathered, which might improve the treatment. 1.1.1.3 Relationship between variables and samples Depending on the amount of variables and in which samples they have been measures we can classify in two types of integration. Traditionally for each sample few variable has been measured, for instance for a tree only the height and width are measured, however with the new omics techniques (transcriptomics, metabolomics, methylomics, genomics), thousands of variables are measured for the same sample. More variables than samples: For a single sample of RNA around 50k genome identifiers (genes, long non coding RNAs, iRNA, pseudogenes,...) can be measured. High-throughput data analysis typically falls into the category of \\(p \\gg n\\) problems, where the number of genes or proteins, \\(p\\), is considerably larger than the number of samples, \\(n\\). Which leads to the case where there are (many) more variables than samples, generally &quot;old&quot; statistics don't consider this case, as it has its own complications like co-variance between the variables. When two variables are tightly correlated, discerning which is the lead and which is following is near to impossible. More samples than variables: An example would be when from a cohort of patients the temperature is measured along the stage of a disease: two variables for each sample. If in the cohort there are more than 2 patients, then the number of samples is greater than the number of variables studied. This is described in the literature as \\(n \\gg p\\). 1.1.1.4 Relationship between samples Depending on the relationship between the samples, the questions answerable and the methods available differ. If each sample has all the expected data we wanted to measure it is a complete case. Sometimes because the sample is not enough, or there are some technical or organizational problems we might lose a source of data for a sample (which is known as an incomplete case). This results in a new source of variation that has to be dealt with, which complicates the conclusion one can draw from the studies of these kind of data. Even when all the cases of a sample are complete the samples can come from several sites of the same individual or with different combinations of variables, which makes is relevant to understand. Time Time is one of the factors that sometimes cannot be controlled, despite having programmed visits every two weeks some patients might come early or later due to calendar reasons (holidays), family reasons, or disease state. Sometimes is precisely the object of the study, to see the relationships at different time, or see how the relationships change with time. Simultaneously is very important to consider it because two variables can seem correlated if we don't take time into consideration. Also, to discover causality between two variables the cause must be before the consequence, which highlights the importance of time. Being aware of the time differences and time scales is crucial in most cases. 1.1.1.5 Relationship between variables Since the lactose operon we know how some genes regulate each other. For other variables we don't know how they are related. For instance, how does the increase in expression of a gene affects the growth of a microorganism? Usually the relationships between variables are mediated by many factors or interactions. Network approaches relate the variables between them (such as [4]). And are fairly new and despite being used in rare occasions they are growing in popularity. In partial correlations some or all of the other variables and considered on how much do they affect and deduced. 1.1.1.6 Input data Methods can be classified by the kind of input data required. Some of them need data from the same patients on each data set used to integrate while other do not. Data from the same samples: These methods do not handle well or at all missing data. They need complete cases/data of the samples in order to be able to integrate the results. These methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [5, 6], Multiple co-inertia analysis (MCIA) [7], Multi-Study Factor Analysis (MSFA) [8], Multi-Omics Factor Analysis (MOFA) [9], STATegRa [10]. Data from different samples: These methods do not need data from the same sample. They draw their conclusions generalizing from the the data available. Some of them handle missing data, while others do use the data at face value. These method includes MetaPhlAn2, HUMAnN, LEfSe [11‚Äì13]. Data types: HCG, 16S, RNA-seq, metabolomics do not share the same data distribution, and are different between them. Also even with the same data depending on the processing of the data they can have very different properties: OTUs (operational taxonomic unit) do not behave equally as ASV (amplicon sequence variants) 1.1.1.7 Mathematical framework Depending on the input and the objective methods use different mathematical framework to process the data. The most important ones are listed below: Network Multilayer, including the multiplex, Molti-C-DREAM, RWR-MH, RWR-M. Network embedding MultiVERSE Dimensional Reduction Momix, RGCCA, Active module identification Multiomic objective genetic algorithm (scores based in two metrics, node score and density of interactions score). MOGAMUN 1.1.1.8 Output results According to the output the integration methods can be classified in several groups: For those from dimensional reduction methods there are three: Shared factor across the data, specific factors for each data or mixed factors. Shared factors: The integration results in a vector of the samples in a lower dimensional space that is shared by all the data used to integrate. Such methods include iCluster, Multi-Omics Factor Analysis (MOFA) [9]. Specific factors: The integration results in several vectors of the samples in a lower dimensional space of each data used to integrate. Such methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [5, 6], Multiple co-inertia analysis (MCIA) [7], Multi-Study Factor Analysis (MSFA) [8]. Mixed factors: The integration results in both previous factors, specific of each data and common to all the data. Such methods include Joint and Individual Variation Explained (JIVE), integrative Non-negative Matrix Factorization (intNMF). 1.1.1.9 Interpretation Understanding how to interpret the results of the methods is highly tight to understanding the method. If one does a correlation between two variables, the interpretation of the analysis is clear, if one variable increase, the other one too. However as more complicated methodologies are developed the interpretation becomes less clear, for instance how can one interpret the result of a canonical correlation analysis? Individually: How each variable relates to another, like in the correlation analysis, the relationship between two variables under study. Or by patient: how do interpret that in these patient variable A and B is X and Y? Globally: In a PCA for instance how do we interpret that some variables have the same loading? What happens in a more difficult method like canonical correlation analysis? There have been some articles about how to interpret those methods on real datasets [14]. Others, to benchmark and to learn how to interpret propose analyzing a simulated dataset [15, 16]. To help interpretation frequently synthetic datasets are used to compare the results of the integration with the dataset of interest and to compare different tools. This datasets are created with some relationships that the tools are expected to find. There exists several methods to create synthetic datasets like MOSim, metaSPARSim, CAMISIM, ballgown, polyester and even edgeR can be used. 1.1.1.10 Conclusion The field of integration is large and complex, with high interest in the recent days, specially in the psychology and omics field, which lacks of a large study. 1.1.2 Reviews and benchmarking The comparison and review of methods independently from original authors have become a crucial step for selecting the right tool for a research [17]. Some of these reviews are focused on a field: metabolomics (ref), genomics, microbiomics... or on a specific characteristic: ?. 1.1.3 Summary Methods to integrate have many characteristics that allow to classify them which explains the diverse results one can have using one of them. References "],["IBD.html", "1.2 Inflammatory bowel disease", " 1.2 Inflammatory bowel disease Inflammatory bowel disease (IBD) includes the chronic diseases Crohn's disease (CD) and ulcerative colitis (UC). CD is a progressive reincident disease that can affect all the gastrointestinal tract but shows mostly on both terminal ilium and colon. The UC is a colonic reincident disease characterized by a continuous inflammation of the colon. Around 4,2 million individuals suffer from IBD in Europe and North America combined. The dysregulation of the inflammatory response observed in IBD requires interplay between host genetic factors and the intestinal microbiome. Several studies support the concept that IBD arise from an exacerbate immune response against commensal gut microorganisms. Nonetheless, the disease could result from an imbalanced microbial composition leading to generalized or localized dysbiosis2. The role of the gut microbiome in IBD is an ongoing field of research. Several authors are currently studying the alterations reported in IBD of the intestinal microbiome. However, it is still unclear the cause-effect relation between dysbiosis and IBD. Partly due to the multiple variables that might contribute to the disease progression; for instance, age, diet, usage of antibiotic, tobacco, environment, and eventually socioeconomic status. This could be due to both the genetic predisposition and environmental factors; for instance, bacterial or viral infection, diet, usage of antibiotic, and eventually the socioeconomic status. (see [18] ) The relationship between host and microbiome has been proposed to play a fundamental role to maintain the disease. Little is known of the influence of the gastrointestinal microbiome in the expression of the gastrointestinal tract. 1.2.1 Disease etiology As noted on the introduction, the origin of the disease is unknown. There are several articles that point out to a relationship between the immune system and the microbiome. As seen the Crohn's disease and the ulcerative colitis are considered two different disease due to their differences despite their similarities. Although so far we do not know what starts the disease there are differences on the disease when appears at different age. There is a rough classification between very early, early or adult on-set disease. The main differences are [ask Isa?]. 1.2.2 Ulcerative colitis 1.2.3 Crohn's disease 1.2.4 Disease discourse Several factors difficult the study of the disease, the intermittent discourse, the heterogeneity of the symptoms, and that the symptoms are common with other disease which often delays the right diagnosis. There is to some extent a disassociated between patient's (clinic) well-being report and the colonoscopy observation. Sometimes when patients report being better but their gut has bigger ulcers or is more inflamed. Although Crohn's disease and colon disease are similar the drugs and treatments for each is different. The order of the treatment/drugs changes depending on the disease. 1.2.5 Clinical care Mucosal healing Colectomy Dysplasia Fistula Herpes and citomegalovirus virus. Scores: Mayo SESCD Other measured parameters: Weight Calprotectin PCR (Proteinc C reactive) Hemoglobine 1.2.5.1 Drugs The ones given on the hospital more or less on the order of administration: anti-TNF\\(\\alpha\\) Vedolizumab Ustekinumab Risankizumab Tofacitinib surgery TRIM/HSCT 5-asa corticoesteroids Azathiprine/Mercaptopurine 6MCP Methotrexate MTX Tacrolimus FK Cyclosporin A CyA Infliximab Adalimumab Antibiotics All these drugs and procedures are available at the time of writing. Often a reduction in dose results in losing the response to the drug. Thus, after an increase of the dose to the previous levels the patients not recover the initial response they had. Hematopoietic Stem Cell Transplant (HSCT) Fecal microbiota transplantation 1.2.6 Summary The integration of data might help to improve the medicine and reveal links in difficult diseases like IBD. So far it has been applied in IBD with partial success. References "],["integration-on-ibd.html", "1.3 Integration on IBD", " 1.3 Integration on IBD One of the hypothesis behind the maintenance of the inflammation involves the microbiome. Several studies have been carried out to discover links between microbiome and the inflammation. Some of these studies used metabolites, DNA-seq sequencing of the microbiome content, or targeted 16S sequencing. Also the technical method used can differ between extracted from stools or from biopsied samples at colonoscopy or from surgical samples. Some articles use correlation like [19]. There are others that use a combination of methods Very rarely there is an experimental confirmation. This is due to how complicated it is to test an interaction and the simulate the conditions. One of the few methods published were the interaction is measured is to expose the ex-vivo sample or cell line with supernatant of a microbiome culture. Previous methods used include the use of RGCCA, [20] [21] Maaslin2 [22] 1.3.1 Genetics 1.3.2 Microbiome 1.3.3 Multiple data 1.3.4 Summary Multiple methods and multiple studies have been done Few focus on discovering the relationships on the disease [22] References "],["rgcca.html", "Chapter 2 RGCCA", " Chapter 2 RGCCA The canonical correlation is a method that using data from the same sample but from different datasets. Characteristics Input Data type: numeric More variables than samples: It is appropriate when the number of variables is higher than the number of samples. Need to be a complete case. Time is not considered as a specially variable Relationship between variables Output: Specific factors Interpretation: Depending on the model and options used. Over several years of progress [5, 6, 23‚Äì26] on the field of canonical correlations it provides a robust method and there is an implementation [???]. The regularized generalized canonical correlation is a method that combines several datasets, using data from the same sample. It is a good choice when the number of variables is higher than samples. There have been some improvements to generalize the method when \\(p \\gg n\\). Current practices include using a pre selected model of relations between blocks. However this model might not be accurate. To help find the fitting model for the data I created an R package. The package, which is named inteRmodel helps finding the right model and how fit it is for your data. This method applied to an existing dataset of an autologous haematopoietic stem cell transplantation [27]. From this dataset there is data about the human transcriptome and the 16S DNA present at biopsies from colonoscopy. For several patients we have samples at different time point and at different locations. This allows us to see both location and time differences. We looked to several models and searched for the model that fit better with the data. The first model just accounted the transcriptomics and the microbiome data, then in another family of models we added the information we know about those samples. In a further family of models we split the information we know about these samples into three different blocks grouping them according to how are they related between them. We fitted the best model for all the three family of models and we found that the most fitting model was from the family that had the data split on several. Additionally I introduced some changes to RGCCA package. The modified package can be found here. This version return the same results as the version on CRAN but provides also some code optimization for a lower computation time. Some of these changes are to to be able to change the design between different dimensions. The idea behind this change is that if the first dimensions correctly fits the data the second dimension might need a different model of relationships. For instance, the first dimensions are dominated by two blocks of data while the second is dominated by another block of data. References "],["model-of-relationships-.html", "2.1 Model of relationships.", " 2.1 Model of relationships. More blocks more possible models. Limitations: symmetric Y -&gt; X &amp; X -&gt; Y Not possible to find causal relationships. 2.1.1 Designing models What defines a block: Blocks of variables are treated independently but variables within a block are used in a linear combination. Create models with random links between blocks. Create models considering the knowledge of the system or disease. 2.1.2 Evaluating models Average variance explained (AVE), inner and outer Bootstraping/validation on an external cohort. Biological relevance "],["evaluating-the-method.html", "2.2 Evaluating the method", " 2.2 Evaluating the method MCIA was used to compare with the new method [28]. Maaslin2 References "],["conclusions.html", "2.3 Conclusions", " 2.3 Conclusions This has been pre-published and is now published on Plus One. "],["biological-relevance-of-results.html", "Chapter 3 Biological relevance of results", " Chapter 3 Biological relevance of results Provide more information that can be later used by researchers on the wet lab. "],["comparing-different-dataset.html", "3.1 Comparing different dataset", " 3.1 Comparing different dataset "],["shared-selected-variables.html", "3.2 Shared selected variables", " 3.2 Shared selected variables "],["conclusions-1.html", "3.3 Conclusions", " 3.3 Conclusions "],["references.html", "References", " References ::: {#refs} ::: "],["online-resources.html", "A Online resources", " A Online resources Some links that I found useful on the thesis and could be useful if you are interested on the multi-omics field. Awesome multi-omics: An online repository of references to multi-omics methods. Bookdown: The book about how to write this type of books. "],["software.html", "B Software", " B Software Along the years of this thesis several pieces of software have been generated as well as packages. Here they are listed for easier retrieval. They are listed on two ways, one with a brief explanation and another one ordered by what software piece is used on each analysis. "],["listed.html", "B.1 Listed", " B.1 Listed An improved/tested version of RGCCA, some modifications on the internal functions to ease the maintenance as well as adding tests and sometimes improving the documentation. Also modified so that it is possible to provide a vector of models so that the model of the first dimension is not the same as the model on the second dimension (not sure if mathematically speaking makes sense but from a biological one I think it might be interesting to have it). Designed to be used with RGCCA I wrote inteRmodel to ease the bootstrapping and model selection. A package to design batches to avoid batch effect experDesign and its website on GitHub A package to analyze sets and fuzzy sets BaseSet. This package was meant to be used with the probabilities that arise from bootstrapping the models. However due to the long times of calculation that it would require it was not used. (A previous iteration of the package called GSEAdv was developed too.) "],["by-publication.html", "B.2 By publication", " B.2 By publication All code of the analysis of the publications is available (in his messed state and complicated history) and a brief description as to why they were used: Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis: TRIM: Mangle with the sample, dataset, explore several methods... sgcca_hyperparameters: Explore the effects of the hyperparameters on RGCCA on the provided dataset. inteRmodel: Package for easy repeating the methodology developed on TRIM. integration: Package with functions that I wrote several times on the TRIM ended up here. Paper 2: Barcelona: Mangle with the sample, dataset ... inteRmodel: To repeat the same procedure as in the first analysis. integration: To not repeat myself for the same process. "]]
