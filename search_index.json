[["index.html", "Data integration on inflammatory bowel disease Preface", " Data integration on inflammatory bowel disease Lluís Revilla Sancho 2022-02-01 Preface The main topic of the thesis is data integration applied in the inflammatory bowel disease (IBD) research. This disease is complex, for instance it is not know if the cause behind Crohn’s disease and ulcerative colitis is the same. There are hypothesis pointing that the microbiome is a major factor in the disease, which together with an aberrant immune response is the dominant theory. In order to find robust relationships between the mcirobiome and the immune system it is important to consider all the relevant variables that influence a disease. On this thesis I seek these relationships using data from different sequencing technologies and the observed or reported phenotype of the patients. The thesis was carried out on the Institut d’Investigacions Biomèdiques August Pi i Sunyer (IDIBAPS) research institute and funded by Centro de Investigación Biomédica en Red (CIBER). The thesis was done on the IBD unit which is a translational team of biologist, microbiologists, veterinaries, bioinformaticians, doctors and nurses (at Hospital Clínic) in a multidisciplinary team. The leading doctor of the unit was Julian Panés whose interest on the disease lead to receiving a grant that made possible this thesis. The thesis is under the doctoral programme in biomedicine of the University of Barcelona (UB). My thesis directors’ were Juanjo Lozano and Azucena Sala, my boss, who helped as bioinformatician and disease expert respectively. They provided advice and guidance on how to analyse the data and where to focus on the different experiments/analysis. This thesis is licensed under the Creative Commons Attribution 4.0 International License (CC-BY). "],["abstract.html", "Abstract", " Abstract Introduction: Inflammatory bowel disease is a complex disease with several factors that affect each other. The origin and process that maintains the disease is not fully understood, and current treatments focus on early detection of the disease and prevent the accumulation of damage on the intestine. Finding those relationships will help to advance treatments, diagnosis or prevent it. Methods: To identify these relationships we based our analysis on intestinal bacterial 16S data and human transcriptome from biopsies. We used canonical correlation analysis to find models that are coherent with previous knowledge on the biologiy of the disease and provide insights on the relationships between bacteria and genes. Results: We compared our own methods with previously published methods, mainly MCIA, to assess the advantages and disadvantages of the different proposed relationships. Furthermore, the classification of samples was used to find which methods provide are more accurate. Discussion: Biological relevance, difficult. Conclusion: Chips or further tests. "],["introduction.html", "Chapter 1 Introduction 1.1 Inflammatory bowel disease 1.2 Integration studies on IBD 1.3 Integration", " Chapter 1 Introduction Inflammatory bowel disease (IBD) involves Crohn’s disease (CD) and ulcerative colitis (UC). It generally affects the terminal ileum and the colon but it can involve any segment of the gastrointestinal tract. UC is a recurrent, chronic and continuous inflammation of the colon and rectum while the CD is not a continuous inflammation and affects the whole gastrointestinal tract causing transmural inflammation. IBD etiology is unknown. However, once it has initiated the most prevalent hypothesis of its chronicity suggests an aberrant immunological response to antigens of the commensal microbiome. To diagnose the IBD doctors use endoscopy and/or magnetic resonance imaging and histologies. Treatments provided for the IBD include, noninflammatory drugs, suppressors and biologics, i.e, anti TNF- anti-IL2, 23, anti-integrin \\(\\alpha4\\beta7\\). The therapeutic options can induce remission in some patients, but they often need continuous treatment to avoid recurrence. Nevertheless, many patients \\[ref:needed\\] are refractory or intolerant to those therapies and need to undergo surgery. 1.1 Inflammatory bowel disease IBD includes the CD and UC which are characterized by alternating periods of remission and clinical relapse that mainly affect the gastrointestinal tract. CD is a progressive relapsing disease that can affect all the gastrointestinal tract but shows mostly on both terminal ileum and colon with a discontinuous inflammation. UC is a colonic relapsing disease characterized by continuous inflammation of the colon. Both of them have different risk factors, clinical, endoscopic an histological characteristics (see sections 1.1.2 and 1.1.3 ). Around 3.5 million individuals have IBD in Europe and North America combined [1]. IBD is more commonly found in industrialized and developed regions, suggesting that environmental factors might greatly influence IBD occurrence. In addition, the incidence of IBD is increasing in areas, such as Asia or Eastern Europe, where the number of cases was relatively low hitherto [2]. The dysregulation of the inflammatory response observed in IBD requires interplay between host genetic factors and the intestinal microbiome. Several studies support the concept that IBD arise from an exacerbate immune response against commensal gut microorganisms. Nonetheless, the disease could result from an imbalanced microbial composition leading to generalized or localized dysbiosis1. The role of the gut microbiome in IBD is an active ongoing field of research. Several authors are currently studying the alterations reported in IBD of the intestinal microbiome. However, it is still unclear the cause-effect relation between dysbiosis and IBD. Partly due to the multiple variables already identified that have been linked to IBD; for instance, age, diet, usage of antibiotic, tobacco, and socioeconomic status [3]. The relationship between host and microbiome has been proposed to play a fundamental role in maintaining disease. For instance, some Proteobacteria species which have adherent and invasive properties might exploit host defenses and promote a proinflammatory environment, altering the intestinal microbiota in favor of dysbiosis [4]. The epithelium is often damaged and might present ulcers or other inflammation symptoms. A segment of the gastrointestinal tract might recover if the patient receives treatment or due the natural cycles of the disease. But once a segment is affected by the disease it can be considered as involved, as some damage remains even if the tissues is no longer inflammed. 1.1.1 Etiology and pathogenesis Several mechanisms have been proposed to drive IBD pathogenesis \\[@zhang2014; @hugot2004\\]. Some of them are based on a relationship between the immune system and the microbiome \\[@silva2016a; @demattos2015\\]. It is also unclear if CD and the UC share the same origin considering their different symptoms. There is also evidence of some genetic component on the onset of the disease, specially if the disease appears very early (less than 2 years old patients) \\[@mcgovern2015b; @satsangi2006c\\]. Disease can be classified based on age at onset as very early, early or adult on-set disease \\[@satsangi2006c\\]. Genome-wide association studies (GWAS) have linked IBD to over 100 genetic loci, including a NOD2 gene, but so far there is not any known mechanism how polymorfism on this genes are driving the disease [5]. On early pedriatic and adult disease the genetic component is lower than on very early on set and it is thought that the environmental factors are the main cause of the disease at those ages. On the following sections I’ll explore the role of several of the possible factors involved on the pahtogenesis, starting with the genetics. 1.1.1.1 Genetics IBD is not an heritable disease, except for very early onset IBD, but it has some genetic influence that predisposes people to have it. This has lead to look for genetic factors on IBD both on general population and on the early cases. Genome wide association studies (GWAS) are one of the most common genetic studies performed, together with methylation studies. To discover through linkage desequilibrium genetic variations linked to phenotypes and regulatory transcription changes, respectively. With GWAS several allels on protein coding loci have been found, rising to around 300 genetic variants \\[@kumar2019\\]. Particularly, the NOD2 gene is highly relevant for the disease on European patients, as it is a risk alleles for CD loci but show significant protective effects in UC \\[@jostins2012; @momozawa2018\\]. The mechanism of how this gene protects from UC has not been confirmed yet [5]. Many of the relevant genetic loci related to IBD are not on protein coding fragments of the genome. Recently expression quantitative trait loci (eQTL) particularly showed [6] that locis are on enhancers or promoters like e.g. H3K27Ac or promoter e.g. H3K4me1 marks as found by chromatin immunoprecipitation sequencing (ChIP-Seq). 1.1.1.2 Microbiome The human intestine is a large reservoir of co-existing microorganisms (bacteria, fungi, viruses, and unicellular eukaryotes) . This microbiome community exerts different functions in the human body influencing nutrients’ metabolism, the maturation of the immune system while suppressing the growth of harmful microorganisms’ [7]. The role of the gut microbiota has been proposed to play a role in the IBD pathogenesis. IBD has been characterized by a breakdown in the balance between beneficial and harmful bacteria that are present in the human gut compared to healthy individuals [8, 9]. Indeed, many studies show that patients with IBD have less biodiversity. Biodiversity is measured on \\(\\alpha\\) (alfa) and \\(\\beta\\) (beta) diversity. \\(\\alpha\\)-diversity is a measure of the species present on a single sample and its abundance while \\(\\beta\\)-diversity compares the diversity between samples. There are some reports of taxonomic changes and increase on Enterobacteriaceae sp, Escherichia coli (specially the invasive strain) at the mucosal layer of IBD patients \\[@ott2004\\]. At the same time there is often a reduction of protective species like Bifidobacterium, Lactobacillus and Faecalibacterium which might be able to protect individuals from mucosal inflammation via several mechanism such as a downregulation of proinflammatory cytokines or the stimulation of IL-10 and antiinflammatory cytokines \\[@ott2004\\]. Specially Faecalibacterium prausnitzii is one microorganism of interest \\[@kostic2014; @sender2016\\]. In fact, it has been recently proposed that several unique microbial species can distinguish healthy controls from UC and CD patients [10, 11]. Figure 1.1: The microbial composition in a healthy and IBD gut. One of the proposed mechanism of crosstalk between bacteria and host is through bacterial metabolites. They interact with the cells and modulate the state of the intestine. One example of such metabolite is butyrate which has been linked to microorganisms presents on healthy intestines and shown to interact with intestine cells and help regulate some genes [12]. As previously mentioned, adherent invasive Escherichia coli, a proteobacteria specie, has been associated with IBD [13]. Adherent invasive strains are mainly found in ileal and colonic samples of CD patients and their presence in UC is less clear. These adherent invasive cells enter through the epithelium of the more permeable cells and live on their cytosol. The metabolic cocktail composed of soluble factors secreted by life probiotic bacteria, living microorganisms which, when administered in adequate amounts, confer health benefits on the host [14–18] or any bacterial-released molecule capable of providing health benefits through a direct or indirect mechanism, has been collectively known as postbiotics since 2012 [14]. 1.1.1.3 Immune response As explained previously the immune system plays a role in IBD pathogenesis and pathophysiology. Loss of tolerance to commensal bacteria has been suggested as the underlying mechanism triggering the inflammation on the intestine. The immune response involves many different cells lines and regions, which are important to know how they organize for a better understanding of the disease. Figure 1.2: The intestinal epithelial barrier. From the luminal side of the intestine, the first layer is the mucosa (See sections 1.2 or 1.1). In the colon the mucus is organized in two layers: the inner layer, a firm mucus layer; and the outer, loose mucus layer [19]. The intestinal epithelium is a single layer of cells organized into crypts and villi (and circular folds on the large intestine) that carries out a diverse array of functions besides digestion performed by specialized cell lineages. Immune response in the intestinal mucous is mainly excreted by the gut associated lymphoid tissue [20]. Genetically predisposed patients when exposed to certain environmental factors activate immune responses against microbials or self-antigens which in turn, may impair the mucosal barrier of the intestinal mucosa, the first physical barrier on the mucosal surface. Both the adaptive and the innate immune cells are present in the intestine, right below the epithelium. On IBD due to antigen translocation into the lamina propria, the immune response leads the adaptive cells to generate immune response to harmless components of the intestinal microbiota. This initial response induces a local increase in the production of pro-inflammatory cytokines and mediators which damages the mucosa. Therefore, the loss of integrity on this barrier enables the intestinal luminal bacteria to access the intestinal epithelium and to interact with the immune system underneath it more directly [21]. The intestinal epithelium is another line of defense against bacterial invasion. Intestinal epithelial cells play a key role in controlling the integrity of the physical barrier to the intestinal microorganisms [21] not only physically but also secreting antimicrobial peptides and defensins, both of which are altered in IBD patients [22]. The intestinal epithelium also plays a key role on the intake and diffusion of metabolites from the intestinal lumen to the lamina propia. Damaging or increasing the permeability of the intestinal epithelium results on a response from the immune system. Detecting signals of any foreign particle can also trigger the immune system. On the intestine this starts with the identification of these signals by intestinal epithelial cells have pattern recognition receptors. There are two main pattern recognition receptors: toll-like receptors (TLR), which are present on the surface, and nucleotide-binding oligomerization domain-like receptors (NOD-like receptors), present on the cytoplasm of the cells. These receptors upon recognition of pathogen associated molecular patterns (PAMPs) start an amplifying signaling producing chemokines and cytokines which activates the transcription and translation of pro-inflammatory mediators to ensure an effective immune response. Initially the innate response is triggered but the cells also increase the antigen presentation to T cells and thus activate the immune adaptive response [23]. Other cell types, such as monocytes, macrophages and dendritic cells also present the pattern recognition receptors. From those, macrophages and dendritic cells are antigen presenting cells too and secrete several cytokines to activate other immune cells. Usually CD patients express higher amounts of TLR than healthy individuals, which might trigger a stronger immune response. This response is driven by CD4+ T cells proliferation in secondary lymphoid tissues to T helpers in the presence of the antigens and cytokines nearby. T helpers (Th) differentiate depending of the cytokines at which they are exposed. Th type 1 are driven by exposure to IL-12 secreted primarily by dentritic cells. Th2 are driven by cytokines secreted by macrophages. The imbalance between Th1 and Th2-promoting cytokines determines the intensity and duration of the inflammatory response in experimental colitis [24]. Th17-promoting cytokines are less well characterized in human. Treg cells differentiate after exposure to cytokines IL-10, IFN-\\(\\gamma\\) and TGF-\\(\\beta\\). Overall the presence of certain cytokines and the response to self-antigens are factors leads to an inflammation and damage that is related to the onset and establishment of IBD On these kind of diseases autologous hematopoietic stem cell transplantation has shown some benefits on IBD [25]. The benefit of hematopoietic stem cell transplant (HSCT) in autoimmunity is thought to originate from the depletion of auto-reactive cells regardless of their specificity. However, due to its associated risk this therapy is only given when patients are refractory to all available therapeutic options. 1.1.1.4 Environmental Chronic inflammatory disorders and neoplasms have become the main cause of morbidity and mortality in the countries with high standards of personal cleanliness. A decrease in human exposure to microbes or hygene which might affect the proper maturation of the immune system so that it provides less immune response or exacerbated response towards “friendly” microbes [26, 27]. From other environmental factors related to IBD such as tobacco, diet, certain drugs and stress; tobacco is the most influential environmental factor. Surprisingly, it has an opposite effect on UC and CD: in CD tobacco is a risk factor that increases the risk of relapse and/or surgical intervention. In UC, it has been observed that smoking cessation worsens the disease [28]. Pharmacological treatments such as oral contraceptives, non steroidal anti-inflammatory drugs are also related to develop or relapse the disease [29, 30]. The psychological welfare of people also plays an important role in the disease progression, stress, anxiety and depression might be important in relapse and deterioration of the disease [31]. Other environmental factors have been linked to IBD but without enough evidence to support a causative effect in the development of the disease. 1.1.2 CD physiology As previously introduced, CD is a chronic inflammatory disorder characterized by a discontinuous inflammation of the gastrointestinal tract. Inflammation on the gastrointestinal tract is transmural and can affect from the mouth to the anus, but mainly it manifests on the ileum and colon [32]. It is frequently associated with extraintestinal manifestation and/or concomitant immune-mediated diseases. The disease itself manifest an heterogeneous symptoms that can involve, diarrhea, weight loss, abdominal pain, fever, anorexia and malaise. Other less frequent co-occurring manifestations are arthritis, primary sclerosing cholangitis, skin disorders venous or arterial thromboembolism and/or pulmonary involvement [33] . These symptoms make it hard to correctly diagnose the disease by non-specialists, in addition there is not a non invasive easy procedure to diagnose it. All these can lead to delays on correct diagnosis of the disease. The detection of parasites or bacteria, such as Clostridium difficile, have been associated with CD \\[@khanna2017\\]. The detection of fecal calprotectin, is generally a good marker of endoscopic activity with sensitivity above 70% and specificity above 80% \\[@guardiola2018; @sands2015\\]. Usually the best diagnosis method is to perform a colonoscopy, whether there is inflammation on the gastrointestinal tract on discontinuous regions then is CD. This inflammation could also present ulceration with rectal sparing and histological lesions which also help to diagnose the patients [34]. Usually on CD a granulome, that is a region with big multinucleous cells, can appear on any intestinal layer. In addition to the inflamed location(s), mosaic zones (patches of inflamed and non-inflamed areas) are more characteristic of CD [35]. The Montreal classification aims to classify patients according to their age of disease onset, standardized anatomical disease location an disease behavior. This classification assumes that the location of CD remains stable over time after diagnosis but behavioral phenotypes change. Other scores consider area affected: Montreal classification allows for early onset of disease to be categorized separately those with age of diagnosis at 16 years or younger, diagnosis at 17–40 years and &gt;40 years, respectively [36]. SES-CD: simple endoscopic score for CD [37]. Score based on size of ulcers, ulcerate surface percentage, affected surface and presence of narrowing on the bowel. CDAI: CD Activity Index takes into account weight, ideal body weight, sex, and events on the last week such as liquid stools, abdominal pain, general well-being and if anti-diarrhea drug usage, as well as knowing if there are fistulas, fever, and other complications [38] To some extent, there is a disassociation between clinical symptoms and the endoscopy finding. Often patients report feeling better despite lack of muscular healing [39]. To overcome this disassociation and be able to compare the well-being of patients several scores and thresholds are used on research and by physicians that will be described later. In the early stages of the disease the relapsing and remitting course is more frequent. Often relapses are accompanied by clinical symptoms, and few have prolonged clinical remission (without treatment) [40]. When there is clinical remission, there can still remain some other lesions and often subclinical inflammation persists. Frequently the damage caused by the disease evolves to fibrostenotic stricture or penetrating lesions (fistula and abscess). Damage of the disease might not be apparent to patients and might be only seen several years later than the first detection [41]. Mucosal healing is a first step towards the healing of deeper layers of the inflamed bowel wall on the CD. Patients might progress from an inflammatory phenotype to a stricturing or penetrating one [42]. Stricturing is a narrowing of a part of the intestine often because of scar tissue and fibrosis in its wall. Penetrating is when the epithelium has some holes or tubes. If these tubes result in an abnormal connection between two body parts it is a fistula, it might also result in an abcess, a collections of pus, often developed in the abdomen, pelvis, or around the anal area. 1.1.3 UC physiology As previously introduced, UC is a chronic inflammatory disorder characterized by a continuous inflammation of the colon. Depending on the inflamed segments of the intestine it is classified in several phenotypes. Around a third of the patients with UC suffer proctitis, the inflammation of the rectum. If the segments from rectum to the sigmoid colon are affected is a distal colitis, if it affects the left colon then it becomes a left colitis. If the inflammation continues to the descending colon it is then an extensive colitis until it affects the whole colon when it becomes a pancolitis. The extension of UC is inversely related to the frequency. However, the extension and severity of the disease correlates: the prognosis is worse the more extended it is [43]. In addition, the damage usually consists in many neutrophil in the lumen crypt \\[@bassolasmolina2018a\\]. The goal of the clinical care is to recover. As a first step, the symptoms of IBD have to lessen to the point that they are mostly absent, gone, or barely noticeable, this is known as clinical remission. However, this is not enough as the mucosa might be still inflamed and thus the reconstitution of the structure and function of the intestinal mucosa is not complete. Other lesions, might aid to the progression to other phenotypes such as fibrostenotic stricture or penetrating lesions or primary sclerosing cholangitis [44]. To prevent and avoid further damage several procedures are followed: When there is dysplasia, an abnormal development of cells within tissues or organs (which is considered a precedence before colorectal cancer growth [45]), or the damage on the colon has been too big a surgical procedure to remove part or all of the colon must be done. Patients that undergo a colectomy need to have their bowel reconnected with a procedure called ileoanal anastomosis (also know as J-pouch by the shape it takes) surgery. Often the lining of the pouch created during surgery becomes inflamed on what is known as pouchitis [46]. Many scores have been proposed for several purposes, from quality of life to disease severity or patient status. Among the scores most used are the following: Mayo: A score designed to be simple to calculate based on stool frequency, bleeding, mucosal appearance at endoscopy and physicians assessment [47]. IBDQ: A 32 questionnaire used to assess the quality of life grouped into four categories: bowel, systemic, social and emotional [48]. UCEDIS: An endoscopic score based on vascular pattern, internal bleeding and erosion and ulcers [49]. Other measured parameters include, weight, effective weight, fecal calprotectin, C reactive protein and hemoglobine. 1.1.4 Treatment Current treatment attempt to induce and keep the remission of patients and reduce secondary effects of the disease instead of revert the pathogenic mechanisms. As standard of care corticosteroids, aminosalicylates and immunosuppressor and some other drugs like antibiotics or metronidazol are util in some cases. Acid 5-aminosalicylic (5-ASA or mesalazina, pentasa) can be given in a topic way (either liquid, enemas, or suppository) or in oral form (pills or dilutions). In CU it helps in the clinic remission but it does not always mean that there is remission (is twice much likely than placebo to reach remission) [50]. On CD the effects are not so stark and generally it does not produce changes on the disease [51]. Antibiotics, such as metronidazol and ciprofloxacina, are effective to deal with secondary effects of IBD such as abscess and bacterian overgrowth in CD [52, 53], but they do not seem effective on UC [53]. Corticosteroids can be taken orally, such as prednisolona, prednisona and Budesonide; intravenous, hidrocortisona, metilprednisolona; or via enemas and suppositories. Budesonide is not absorved well and has a limited biodistribution but it has good therapeutic benefits with a reduced systemic toxicity in IBD \\[@rezaie2015; @peyrin-biroulet2010\\]. These drugs work very well as antiinflammatory for mild or severe IBD but do not work well as maintenance drug \\[@lichtenstein2009; @ouellette2001\\]. Thiopurines (Azathioprine, mercaptopurina) are immunosuppressants drugs that deactivate key process of limphocytes T that might trigger the inflammation. As a side effect they are toxic due to their interaction with nucleic acids [54]. on CD they are useful to induce and keep remission [55], while on UC they are used to keep the remission [56]. In the last two decades the IBD treatment has moved from aminosalicylates, corticosteroids and immunomodulators to anti-TNF\\(\\alpha\\). Anti-TNF\\(\\alpha\\) drugs has changed the IBD treatment as it reduced the hospitalization associated with previous treatments, reducing medical costs and risk of surgery as well as induce a better mucosal healing and quality of life for patients [57]. However, 20-30% of patients have no response to this treatments and another 30-40% lose response in a year [58]. Recently a new wave of drugs has been developed targeting different molecules such as vedolizumab, targeting anti-integrin\\(\\alpha 4 \\beta 7\\), ustekinumab, targeting both IL-12 and IL-23, risankizumab an anti IL-23, tofacitinib an inhibitor of JAKs, infliximab an anti-TNF\\(\\alpha\\). Patients might become refractory to drug. Thus, drugs do not have the same effect as previously and the dose might need to be increased with the risk of more secondary effects [59]. Surgery resection might be needed on these patients. Close to 35% of patients with UC will need to have a surgery resection, either due to complications or because the inflammation can not be controlled. Surgery usually removes the inflamed segment of the colon. The most common procedure used is a colectomy (whole colon removal), with ileostomy [60]. CD patients usually require surgery associated to complications like stenosis, abscess, and fistulas ) between 70% and 90% at some point of their lives [61]. Usually the surgery is limited to removing the inflamed segment but occasionally an ileostomy is required [62]. If the drugs fail to contain the inflammation and heal the mucosa doctors might recommend a different procedure. In some cases HSCT is recommended which have shown to improve the life of the patients [63]. This is a new procedure given only to the most extreme cases to reset the immunological state of the patient. To reset or hugely modify the microbiota fecal microbiota transplantation between different people is currently being explored \\[@weingarden2017\\]. 1.1.5 Summary IBD is a complex disease that impacts the health of many people for long time and with lasting impact on their quality of life. Current clinical care in some cases is enough to have a sustained clinical and endoscopic remission but most often is not enough and relapse is expected. Several factors, such as becoming refractory to drugs, intermittent course of the disease and lack of validated predictors of disease course or response to therapy make the treatment complex. Lack of knowledge of what are the factors cause of the disease make those treatments and drugs to be addressed to block further inflammation and damage, but cannot prevent it and often they do not stop it completely. 1.2 Integration studies on IBD Many studies have looked up to the origin of the disease. As seen, one of the hypothesis behind the maintenance of the inflammation involves the microbiome and the host epithelium. This has been studied using several data sources, mostly from sequencing data. The technical methods used to obtain the data of the inflamed tissue differ between extracted from biopsied samples at colonoscopy or from surgical samples. Those samples are usually used later on to diagnose or for research purpose. To obtain research-quality data it usually imply using techniques such as immunohistochemistry, histopathology, immunohistochemistry, fluorescence in situ hybridization and polymerase chain reaction. These techniques allow to measure or visualize where are the cells expressing certain proteins or genes, thus helping with the analysis validation. Furthermore several studies have been carried out to discover links between microbiome and the inflammation, followed by those looking for some relationship between genetics and the disease and more recently the metabolome. These studies, known as integration, multi-omic or interaction studies, usually use multiple sequencing assays as the bases of the analysis [64]. However, confirming causal interactions of the variables of each essay is difficult. To find relationships some articles use correlation, like \\[@hasler_uncoupling_2016\\], there are others that use a combination of methods from correlations, partial correlations to integrative methods \\[@tang_integrated_2017; @hernández-rocha2021; @huWholeExomeSequencing2021\\] and network integrations. Very rarely there is an experimental confirmation of the relationships between variables of the different essays because it is complicated to test an interaction and to set up the right conditions for the many variables that are accounted for on the integrations. One of the few methods published that shows an interaction between genes and microorganisms on the IBD is to expose the ex-vivo sample or cell lines with microbiomes or supernatant of at their culture [65]. 1.2.1 Data origin According to the data used, we can classify the studies: 1.2.1.1 Transcriptome Most of the integrations refer some other source of data to the transcriptomics of the patient. The transcriptome of the patients has been extensively studied since the existence of microarrays. There are known marker gens of inflammation and many research focus on identifying prognosis predictors, treatment response prediction based on gene expression [66, 67]. Recently single-cell technology has enabled to estimate cell populations of the samples with better degree of success than bulk RNA-sequencing, which is helpful to understand the immune response and the population change of the cells. Single cell technologies are starting to be used for integration. Notably this is probably the most common data available from IBD patients, probably due to existing commercial solutions to gather this data and the experience with this kind of protocols. 1.2.1.2 Microbiome Many of the integration analysis on inflammatory bowel are done between host transcriptome and the microbiome. These studies use datasets from IBD patients usually stratified by some inflammatory index or location of the disease. Most of them are based on correlation analysis between the microbiome and RNA-seq [68]. Conclusions of these integrations range from finding differences on the correlation depending on the disease ([68]) to finding relationships with inflammatory genes [69]. 1.2.1.3 Genetics Genetics is the next most common data source used to integrate data on the IBD. Most studies on genetics and IBD are genome-wide association studies [70–72]. No clear mechanism has been proved despite clear and significant findings, probably because IBD relapses and remits with time. When using genetic data to integrate it with transcriptomics it is usually to understand how a genetic variant is affecting a gene expression. This has lead to expression quantitative trait loci (eQTL). 1.2.1.4 Metabolome More recently there have been an increased interest on metabolome, given that microorganisms interact with the host also via their products and metabolites. Some studies have integrated the metabolome with the RNAseq and state of the epithelium [73, 74]. There is evidence some of these metabolomic products, such as small chemicals regulate the epithelium cells and their interaction with the microbiome and outer agents [65, 75]. 1.2.2 Summary Multiple methods and multiple studies have been used to learn more about the interaction between different data and understand the IBD. The task is challenging and with many different aspects one could use. There are many studies focused on understanding different aspects of the disease: the relation between location and severity, relapse, remission, treatments, origin. The integration of data might help to improve the medicine and reveal links in difficult diseases like IBD. 1.3 Integration Data integration is informally widely used with varying definitions, according to the dictionary integration is defined as: “the process of combining two or more things into one” — Cambridge Dictionary Other words used are integrati(-on, -ve), and if specific to data from sequencing technologies multi-omics, pluri-omics. Here integration will be used as it is the more general one and not restricted to omics or sequencing technologies. Since the beginning of the integration methods there have been many methods proposed [76]. Some of the early methods were initially used for surveying the agreement of different evaluating systems, others were developed for agricultural sciences [77] or food industry [78]. Some of these methods are specific for one application or data while others are more generally applicable. Lately, the increase of bigger datasets with more variables and often from the same samples has increased the focus of the research community on the methodologies available on several disciplines but mainly on the biological science. The explosion of data on the biological science has been driven by the new sequencing technologies that allow to measure thousand of variables of many samples at the same time, if done with multiple sequencing tecnologies it is usually referred as multi-omics methods, which usually only integrate omic data. It has been observed the importance of classification, review and comparison of the tools available, as well as, benchmarking these tools against the same dataset as a way to provide clear recommendations to anyone wishing to use them \\[@wu_selective_2019\\]. Part of these efforts uses the methods’ strategies to classify them \\[@cavill_transcriptomic_2016; @chongComputationalApproachesIntegrative2017\\]. Following this view I’ll review the integration methods according to several axis: type of data used, aim of the method, relationships between variables, relationships between samples, relationship between variables and samples, input data, mathematical framework and results of the method. 1.3.1 Classification of integration method’s Integration methods have very different properties that allow them to be classified and compared. I classified them in the bioscience field, which means that they are meant to be used with omics datasets, with references to concrete methodology and in occasionaly to articles using them. 1.3.1.1 Data type: numeric or categorical The most important distinction in integration methods is what kind of data are combined. In general data can be divided between categorical and numeric variables, which are usually found in several fields. Sometimes doctors want to understand the relationship between a phenotype they observe and the underlying mechanism. Usually this involves looking how the metabolites, the gene expression, the methylation, the number of variants a gene has, and other numeric variables are related to the observed phenotype (like pain). Depending on what does a method aim for it handles both data types or just one, but often they are used differently. The most common way to handle different type of data is converting the categorical values to a mock or dummy variable. For each categorical factor there is a new variable whose value is 1 if that sample had this factor and 0 otherwise. For instance, if the categorical variable has three values (A, B, C) it would be converted to A (1, 0, 0) B (0, 1, 0) and C (0, 0, 1). Often the number of variables created is one less than the number of factors that existed, on this example only A and B would be kept. This transformation allows to use the categorical values in methods though for numeric variables. If the method only accepts categorical data but you want to provide numeric values usually those values are categorized. For example if a variable is (0.123, 0.25, 0.56, 0.78) one could make to categorical values like (“&lt;0.5”, “&lt;0.5”, “&gt;0.5”, “&gt;0.5”). The number of categories to use and how is the numeric value split depends on case by case. Very rarely methods allow to use both of them as they are. If they allow so, it is usually for classification purposes. 1.3.1.2 Objective The objective of the method is one of its most important defining properties. Data integration can be classified according to their objective and the (biological) question they try to answer. Most of the times one (or several) of the following results are expected from integration methods: Classification of the patients or samples As seen one of the purposes of the integration might be to use multiple sources of data to accurately describe how do samples or patients fit on a predefined possibles states. This might be to accurately describe if a patient has one or other related disease of a possible subset of diseases given their condition. An overview of the role of each individual block in a biological system Sometimes the question is which omic method is the best describing the disease. This could prevent peforming expensive tests that can be estimated by other cheaper or easier technique that has enough predictive power or is enough sensitive and specific for the task. An example of this is the search of markers on blood to identify if there are cancer cells on tissues. A better understanding of the relationships between the ’omics types Several methods are used to understand better how the different data sets interact. When a potentially relationship between omics is disvoered then other focused methods are used to understand the mechanistic relationship between them. For instance, checking that in a particular case or condition is present on a given relationship, and that this relationships follows our model or not. Finding a molecular signature. A signature is usually a group of features that describe/are representative of a cell line, a process or a stage. Identifying a subset of the variables from the omics that are related is often a desired goal because it reduces the amount of variables allowing to perform experiments on the bench on just those that might be important. In other fields, such as machine learning, selecting the important variables is known as feature selection. There are several methods that are used to do this. An example of this is when performing eQTL, where a locus is related to the expression change of genes, so finding the right positions that induce a specific expression level is a very common task. A predictive model Predictive models usually require a very good understanding of the current and/or past relationships, as well as, a good feature selection procedure. If a good model on previous data exists it might be used to predict future events. Sometimes, models are only build to predict events without being able to accurately describe the mechanism, why they work. This kind of methods are used improve treatments, diagnosis and prognosis prediction. Impute values Some methods aim to accurately guess which values have one block given some other information. Missing values can happen for a variety of reasons from practical ones, like a sample not being available, to technical ones, such as laboratory method failing. However, this is often a intermediate step to other goals. To complete these goals it is important to have enough statistical power to determine the significance of tests performed (if any) and to understand how complete are the data sources used on the integration [79]. Having more statistical power helps identifying the relationships one seeks when using this methods. Formally the objective of methods might be defined as a function to be optimized or a procedure to be followed until certain conditions are meet between the variables received as input. 1.3.1.3 Relationship between variables and samples Depending on the amount of variables and in which samples have been measured studies can be classiffied in two types of integration. Traditionally for each sample few variables are measured, for instance on a biopsy with RT-PCR only a few genes are measured, however with the new omics techniques (transcriptomics, metabolomics, methylomics, genomics), thousands of variables are measured for the same sample. This has lead to the following situation: More variables than samples For a single sample of RNA around 50k genome identifiers (genes, long non coding RNAs, iRNA, pseudogenes,…) can be measured. Which leads to the case where there are many more variables than samples. Thus high-throughput data analysis typically falls into the category of \\(p \\gg n\\) problems ( big p, little n), where the number of genes or proteins, \\(p\\), is considerably larger than the number of samples, \\(n\\). With such high number of variables the identification of the relevant variables is hard because variables will co-variate. When many variables are tightly correlated, discovering which one is important using just numerical methods is very difficult. It is even more difficult when looking for causal relationships. More samples than variables This was the usual case when for instance, when from a cohort of patients the temperature is measured along the stage of a disease: two variables for each sample. If there are more than 2 patients, then the number of samples is greater than the number of variables studied. This is described in the literature as \\(n \\gg p\\) (or big n, little p). Nowadays this is less frequent on the bioscience world, and does not causes trouble analyzing it because the high number of samples allow to accurately estimate variables’ dispersion. To estimate the number of samples required there are several methods available [80]. Having just the enough amount of samples for the desired statistical power is not enough if the samples of different blocks do not match between them. 1.3.1.4 Relationship between samples Depending on the relationship between the samples of the different blocks of data, the questions answerable and the methods that work on them differ. A sample can have multiple or one data source. In a study if all the samples have all the data from multiple data sources it is a complete case. If some samples have data from some data sources but not from others the study is not a with a complete case. Sometimes because the sample is not enough, or there are some technical or organizational problems a source of data for a sample (which is known as an incomplete case) might be lost. This results in a new source of variation that has to be dealt with, which complicates the conclusion one can draw from the studies of these kind of data. Even when all the cases of a patient are complete the samples can come from several sites of the same individual or with different combinations of variables, which makes is relevant to understand the relationships between the different samples. There is no easy classification of this as each experiment might be designed differently. In general, experiments are designed to be as consistent as possible but in face of adverse events that become a variation of the design the analysis complicates. Either some data is imputed or some samples are omitted for the analysis, or if the differences are minor these samples are used as being from a different group. This can happen with samples taken at different timepoints as patients might not come to the visit with the doctor on time. Time As mentioned above, time is one of the factors that sometimes cannot be controlled, despite having programmed visits every two weeks some patients might come early or later due to multiple reasons (holidays, other conflicting duties, …). Sometimes, precisely the objective of the study is to analyze the relationships at different time, or see how the relationships change with time. To discover causality between two variables the cause must be before the consequence, which highlights the importance of time. Being aware of the time differences and time scales is crucial in most cases. On cell lines or other lab experiments conditions can be repeated even if they are at different time points. However, with patients there are not biological replicates like with cell-lines. This makes it harder to study time-related change on patients. Lastly, time between the collection of a fresh samples and its processing also influences the readings of the samples of the omics technology, specially RNA-seq [81, 82]. Some features and genes are more affected by time than others but as they are measured at the same time it might distort the data. Processing time is also very hard to keep track on practical settings and requires a highly coordinated effort [83]. 1.3.1.5 Relationship between variables Once the data is collected, the next step is understand the relationship between the variables present. As seen, some variables influence other variables which affects the outcome in a complex ways. With many variables present in a dataset it is important to be aware of known relationships between variables. Even in a simple dataset, like a RNA-seq dataset, it is important to be aware of the relationships between variables. Since the discovery of the lactose operon it is known how some genes regulate each other. It is no know how other variables are related between them. For instance, how does the increase in expression of a gene affects the growth of a microorganism? Usually the relationships between variables are mediated by many factors or interactions. One of the best example of such interactions is when some variables correlate. Their correlation can be used to reduce the number of variables being analyzed by ignoring the relationships between them and using the most representative variable (less correlated and with more variation). This step is usually done by the dimension reduction methods… However, sometimes this is not desired or feasible as the correlation does not explain the direction of the causality of the interaction between the variables (if there is any). Network approaches relate the variables between them (such as [84]). These approaches are fairly new and growing in popularity partly because they can address the direction of the interaction. In partial correlations some or all of the other variables are considered on how much do they affect the others and this effect is reduced. This assumes a linear relationship and is computationally expensive when there are thousands of variables. 1.3.1.6 Input data We have classified the studies according to the data they use (as 1.2.1 ). But, some methods to account for relationships of variables only work when data is from the same patients on each data set used to integrate while other do not: Data from the same samples: These methods do not handle well or at all missing data. They need complete cases/data of the samples in order to be able to integrate the results. These methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [85, 86], Multiple co-inertia analysis (MCIA) [87], Multi-Study Factor Analysis (MSFA) [88], Multi-Omics Factor Analysis (MOFA) [89], STATegRa [90]. Data from different samples: These methods do not need data from the same sample. They draw their conclusions generalizing from the data available. Some of them handle missing data, while others do use the data at face value. These method includes MetaPhlAn2 [91], HUMAnN,[92], LEfSe [93]. Furthermore, some methods are designed to integrate specific types of datasets, (usually because they make some assumptions that are only met on that kind of data). For instance, HCG, 16S, RNA-seq and metabolomics do not share the same data distribution, and are different between them. Also even with the same data depending on the processing of the data they can have very different properties: OTUs (operational taxonomic unit) properties are not the same as ASV (amplicon sequence variants). 1.3.1.7 Mathematical framework Depending on the input and the objective methods use different mathematical framework to process the data. Some of them have previously appeared: Networks Networks methods were mentioned because they use and find information about interaction of variables. Multilayer networks, including the multiplex, Molti-C-DREAM[94], RWR-MH, RWR-M [95]. Network embedding MultiVERSE are some of the methods using networks [96]. Bayesian approaches are also quite frequent, these methods use the Bayes theorem to see the relationships between variables. Some methods that use these approaches are RIMBANET [97], BCC [98] and others like [99]. Dimensional Reduction These methods focus on finding just a few variables and summarizing them using some function that has some properties. The selection of variables is usually done with L1 or Lasso Regression regularization technique or L2 also known as Ridge Regression. L1-regularization adds a penalty equal to the absolute value of the magnitude of coefficients which might leads to some coefficients becoming zero and the variable eliminated from the model. On the other hand, L2-regularization does not result in elimination of coefficients or sparse models and can only be used when there is multicollinearity as it works well to avoid over-fitting. Several methods use this approach: Momix [100], RGCCA [101], mixOmics [102], STATegRa [103], … Active module identification Multiomic objective genetic algorithm (scores based in two metrics; node score and density of interactions score). An example of a method using this approach is MOGAMUN [104] Usually depending on the mathematical framework used they return these methods return similar output. 1.3.1.8 Output results According to the output the integration methods can be classified in several groups: For the network methods the following output is usually returned: Connections between the variables/nodes, a measure of how strong is the connection (or simply if there is a connection or if there is not one). For those from dimensional reduction methods there are three: Shared factor across the data, specific factors for each data or mixed factors. Shared factors: The integration results in a vector of the samples in a lower dimensional space that is shared by all the data used to integrate. Such methods include iCluster, Multi-Omics Factor Analysis (MOFA) [89]. Specific factors: The integration results in several vectors of the samples in a lower dimensional space of each data used to integrate. Such methods include Regularized Generalized Canonical Correlation Analysis (RGCCA) [85, 86], Multiple co-inertia analysis (MCIA) [87], Multi-Study Factor Analysis (MSFA) [88]. Mixed factors: The integration results in both previous factors, specific of each data and common to all the data. Such methods include Joint and Individual Variation Explained (JIVE), integrative Non-negative Matrix Factorization (iNMF) [105]. These methods are used to understand and discuss the results with previous knowledge or independent sources of data. 1.3.2 Interpretation How to interpret the results of the methods is highly linked to understanding the method and its output. On a correlation between two variables, the interpretation of the analysis is clear, if one variable increase, the other one too. The implications of these observation can be far reaching but the principles to understand them are simple. However, on more complex methods the interpretation becomes less clear. The interpretation of a canonical correlation analysis is much harder [106]. Also on more complex methods the number of parameters required increases so the time and intellectual effort to understand the relationships between the parameters is also higher. The interpretation also helps to discuss the results and relate it to other previously know information. Individually: How each variable relates to another, like in the correlation analysis, the relationship between two variables under study. Or by patient: how do interpret that in these patient variable A and B is X and Y? Globally: In a principal component analysis for instance how is interpreted that some variables have the same loading? What happens in a more difficult method like canonical correlation analysis? There have been some articles about how to interpret those methods on real datasets \\[@sherryConductingInterpretingCanonical1981\\]. Others, to benchmark and to learn how to interpret propose analyzing a simulated dataset \\[@chung_multi-omics_2019; @martinez-mira_mosim_2018\\]. Which is used to compare the results of the integration with the dataset of interest and to compare different tools. These datasets are created with some relationships that the tools are expected to find. There exists several methods to create synthetic datasets like MOSim [107], metaSPARSim [108], CAMISIM [109], ballgown [110], polyester [111] and even edgeR [112] can be used. These methods are useful to compare different setup and methods but they can miss some subtle not previously reported relations on real data. 1.3.3 Reviews The comparison and review of methods independently from original authors have become a crucial step for selecting the right tool for a research [100]. Some of these reviews are focused on a specific type of data integration: metabolomics \\[@cavill2016\\], genomics \\[@mcgovern2015a\\], microbiomics… Others focus on the disease and the challenges of each omics and the need of an integrative approach to provide better therapies \\[@de_souza_ibd_2017; @tarazona2021a; @valles-colomer2016\\]. On this regard there are several efforts to integrate data on IBD but no comprehensive review to date is known to the author. 1.3.4 Summary The field of integration is large and complex, with high interest over the last few years, specially in the psychology and omics field. As a methodology they are quite complex and diverse but there is a growing interest on them to help answer complex questions without using other complex tools like deep neuronal networks or other machine learning approches (despite not being incompatible). Methods to integrate have many characteristics, depending on the objectives and data that available. Regardless of the method used, interpretation and reporting is usually a main problem when using any of these methods as there are less people familiar with it. See table below for a list of existing methods2: Integration methods available and their references. Method Publication SCCA [113] PCCA [114] PMA [115] sPLS [116] gesca [117] Regularized dual CCA [118] RGCCA [85] SNMNMF [119] scca [120] STATIS [121] joint NMF [122] sMBPLS [123] Bayesian group factor analysis [124] RIMBANET [125] FactoMineR [126] JIVE [127] pandaR [128] omicade4 [129] STATegRa [130] Joint factor model [131] GFAsparse [132] Sparse CCA [133] CCAGFA [134] CMF [135] MOGSA [136] iNMF [137] BASS [138] imputeMFA [139] PLSCA [140] mixOmics [141] mixedCCA [142] SLIDE [143] fCCAC [144] TSKCCA [145] SMSMA [146] AJIVE [147] MOFA [148] PCA+CCA [149] JACA [150] iPCA [151] pCIA [152] sSCCA [153] SWCCA [154] OmicsPLS [155] SCCA-BC [156] WON-PARAFAC [157] BIDIFAC [158] maui [159] SmCCNet [160] msPLS [161] MOTA [162] D-CCA [163] COMBI [164] DPCCA [165] MEFISTO [166] MultiPower [80] mixedCCA [167] References "],["hypothesis-and-objectives.html", "Chapter 2 Hypothesis and objectives 2.1 Hypothesis 2.2 Objectives", " Chapter 2 Hypothesis and objectives 2.1 Hypothesis I hypothesize that the presence of certain microorganisms are related to the patient well being. Furthermore, the expression state of the epithelium and the amount of bacterial presence might identify whether patients are suffering an IBD. Finally, I believe that environmental factors interact with the IBD and are responsible of the big variability on the IBD discourse. 2.2 Objectives Identify microorganisms and genes related to the IBD. Genes and microorganisms that are related to healthy patients Genes and microorganisms that are related to IBD patients while controlling for environmental factors. Only related to Crohn’s disease Only related to ulcerative colitis Related to both Effect of the genes and microorganisms related to the IBD. "],["materials-and-methods.html", "Chapter 3 Materials and methods 3.1 Datasets 3.2 Processing samples 3.3 Statistics 3.4 Regularized generalized canonical correlation analysis", " Chapter 3 Materials and methods This chapter explains the methods used to collect and analyze data from the multiple cohorts analyzed. The actual code used can be found on the links of the appendix. Some sections of this chapter has been adapted from “Multi-Omic Modelling of IBD with Regularized Canonical Correlation Analysis” [168] or other publications of the research group. Samples of the different cohorts collected on the Hospital Clínic were collected similarly, differences are described below together with a brief description of the main characteristics of the different datasets. The processing protocol followed for those dataset that were not generated at Hosptial Clínic can be found on their respective reference and briefly summarized here. 3.1 Datasets 3.1.1 Puget’s dataset The glioma dataset is the data provided as an example by the authors of RGCCA from a previous pulication [169]. The data came from diffuse intrinsic pontine glioma patients that included the host transcriptome analyzed with Agilent 44K Whole Human Genome Array G4410B and G4112F, patients copy number variation was processed with the ADM-2 algorithm, and data from comparative genomic hybridization (CGH) analyzed using Mutation Surveyor software. In addition, this dataset contained information on age, localization of the tumor, sex and a numerical grading of the severity of the tumor [169]. Table about Puget’s dataset Characteristic Puget’s Samples 53 Sex (female/male) 28/25 Location (cort/dipg/midl) 20/22/11 3.1.2 HSCT dataset Samples from the HSCT dataset used in this thesis were from a cohort of patients with severe refractory CD undergoing hematopoietic stem cell transplant. Patients were treated in the Department of Gastroenterology (Hospital Clínic de Barcelona –Spain–). The protocol was approved by the Catalan Transplantation Organization and by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7244). All patients provided written consent following extensive counselling about being included on the study and using their data on publications. Colonic and ileal biopsies were obtained at several time points during ileo-colonoscopy, at inclusion and every six or twelve months after HSCT until 4 years after the start of the treatment. Samples were obtained whenever possible from both uninvolved and involved areas. In addition, biopsies were taken from the ileum and colon regions of 19 non-IBD controls consisting of individuals with no history of IBD and who presented no significant pathological findings following endoscopic examination for colon cancer surveillance (Hospital Univesitari Mútua de Terrassa–Spain–). The protocol was approved by the Institutional Ethics Committee of the Hospital Univesitari Mútua de Terrassa (Study Number NA1651). At least one biopsy was collected and fresh-frozen at -80°C for microbial DNA extraction. The remaining biopsies were placed in RNAlater RNA Stabilization Reagent (Qiagen, Hilde, Germany) and stored at -80°C until total RNA extraction. Table of HSCT dataset. Characteristic HSCT Sex (female/male) 22/15 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 7/11/0 Years of disease: mean (min-max) 14 (8-28) Age: mean (min-max) 44 (23-70) Samples (non-disease/CD) 51/107 Location (ileum/colon/unknown) 48/108/2 SES-CD local: mean (min-max) 2.15 (0-12) CDAI: mean (min-max) 120 (0-450 3.1.3 Häsler’s dataset An IBD-related dataset was obtained by Prof. Dr. Rosentiel and Prof. Dr. Robert Häsler. It included samples from the terminal ileum and sigma from CD, UC, infectious disease-controls and healthy controls [170]. The provided data included location, gender, location, age, and the status (inflamed or non-inflamed) of the region from which the biopsy was taken. Table of Häsler’s dataset Characteristic Häsler’s Samples (non-disease/diseased) 33/26 Sex (female/male) 42/17 Location (ileum/colon) 30/29 3.1.4 Morgan’s dataset A previously published dataset from a pouchitis study was analyzed [171]. The dataset has a total of 255 samples from 203 patients, containing data for both host transcriptome and intestinal microbiome. This dataset included anonymous identifiers for the patients, whether the sample was from the pre-pouch ileum (PPI) or from the pouch, the sex, the outcome of the procedure and an inflammatory severity score ISCORE. The pouch ileum might be inflamed or not. Table of Morgan’s dataset Characteristic Morgan’s Samples 255 Sex individuals (female/male) 101/102 Location (Pouch/PPI) 59/196 3.1.5 BARCELONA dataset All patients with an established diagnosis of IBD, including Crohn’s disease, ulcerative colitis, IBD unclassified, indeterminate colitis, or pouchitis, starting treatment with a biologic agent were monitored following the schedule of clinical visits, laboratory tests, imaging procedures and biologic sampling at the beginning of their treatment with anti-TNF therapy and after 14 weeks and 46 weeks a biopsy from an ileocolonoscopy. The protocol was approved by the Institutional Ethics Committee of the Hospital Clinic de Barcelona (Study Number HCB/2012/7845 and HCB/2012/7956). Patients that were referred to the Hospital Clínic de Barcelona IBD unit, who had already started treatment with a biologic agent in another center, were also included adapting to the corresponding time-schedule of their treatment. In all patients, starting anti-TNF treatment will be decided before the protocol entry decision according to medical clinical practice. Anonymized identification of the patients, disease, sex age at diagnostic, age at the moment of the sample taking, time since the start of the treatment and sample segment was collected. Table of samples included from the BARCELONA dataset. Characteristic BARCELONA Individuals 62 Status (CD/UC/Controls) 33/21/8 Sex (female/male) 29/33 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 2/44/8 Years of disease: mean (min-max) 7.6 (0-32) Age: mean (min-max) 41 (18-68) Time (0/14/46 weeks) 41/40/32 Sample segment (ileum/colon) 39/87 3.1.6 Howell’s dataset This dataset was downloaded after the publication of an article “DNA Methylation and Transcription Patterns in Intestinal Epithelial Cells From Pediatric Patients With IBDs Differentiate Disease Subtypes and Associate With Outcome” [172]. From their dataset I used data from 77 samples that had both RNAseq and 16S data collected. There are 10 non-IBD samples, 11 with Crohn’s disease and 11 with ulcerative colitis. Data has the following characteristics: disease, age at diagnostic, age at the moment of the study, sex, segment, and clinical history which have the following characteristics: Table of samples included from Howell’s dataset. Characteristic Howell’s Disease (CD/UC/controls) 10/11/11 Age at diagnostic (&lt;17/&lt;40/&gt;40 years) 32/0/0 Age: mean (min-max) 12 (6-15) Sex (female/male) 10/22 Segment (ileum/colon) 31/46 Clinical history (inflammation/no inflammation) 24/53 3.1.7 Hernández’ dataset This dataset was obtained after a collaboration “Integrative analysis of colonic biopsies from IBD patients identifies an interaction between microbial bile-acid inducible gene abundance and human Angiopoietin-like 4 gene expression” [173]. Table of samples included from Hernández’s dataset. Characteristic Hernández’ Disease (CD/UC/controls) Age at diagnostic (&lt;17/&lt;40/&gt;40 years) Age: mean (min-max) Sex (female/male) Segment (ileum/colon) 3.2 Processing samples 3.2.1 Transcriptome sequencing Total RNA from mucosal samples (HSCT cohort) was isolated using the RNAeasy kit (Qiagen, Hilde, Germany). RNA sequencing libraries were prepared for paired-end sequencing using HighSeq-4000 platform. Later, those samples with good enough quality as recommended by FastQC afterwards cutadapt (version 1.7.1) [174] was used for quality filtering and the libraries were mapped against the human reference genome using the STAR aligner (2.5.2a) with Ensembl annotation (release 26 of GENCODE, GRCh38.p10 or superior) [175]. Code used with STAR STAR \\ --outSAMtype BAM SortedByCoordinate \\ --outFilterIntronMotifs RemoveNoncanonical \\ --outSAMattributes All \\ --outReadsUnmapped Fastx \\ --outSAMstrandField intronMotif \\ --outFilterScoreMinOverLread 0.5 \\ --outFilterMatchNminOverLread 0.5 \\ --outFilterType BySJout \\ --alignSJoverhangMin 8 \\ --alignSJDBoverhangMin 1 \\ --outFilterMismatchNmax 999 \\ --outFilterMismatchNoverLmax 0.04 \\ --genomeDir &quot;$genome/STAR&quot; \\ --limitBAMsortRAM 10000000000 \\ --runMode alignReads \\ --genomeLoad NoSharedMemory \\ --quantMode TranscriptomeSAM \\ --outFileNamePrefix $output \\ --runThreadN &quot;$threads&quot; \\ --readFilesCommand zcat \\ --readFilesIn &quot;$file1&quot; &quot;$file2&quot; Read counts per gene were obtained with RSEM (version 1.2.31) [176] as previously described [177]. Code used with RSEM rsem-calculate-expression \\ --quiet \\ --paired-end \\ -p &quot;$threads&quot; \\ --estimate-rspd \\ --append-names \\ --no-bam-output \\ --bam &quot;$rseminp&quot; &quot;$genome/RSEM/RSEM&quot; &quot;$rsem&quot; 3.2.2 Microbial DNA sequencing Biopsies from the HSCT CD cohort were resuspended in 180 μl TET (TrisHCl 0.02M, EDTA 0.002M, Triton 1X) buffer and 20mg/ml lysozyme (Carl Roth, Quimivita, S.A.). Samples were incubated for 1h at 37°C and vortexed with 25 μl Proteinase K before incubating at 56°C for 3h. Buffer B3 (NucleoSpin Tissue Kit–Macherey-Nagel) was added followed by a heat treatment for 10 min at 70°C. After adding 100% ethanol, samples were centrifuged at 11000 x g for 1 min. Two washing steps were performed before eluting DNA. Concentrations and purity were checked using NanoDrop One (Thermo Fisher Scientific). Samples were immediately used or placed at -20°C for long-term storage. 3.2.2.1 DNA sequencing Library preparation and sequencing were performed at the Technische Universität München. Briefly, volumes of 600μL DNA stabilization solution (STRATEC biomedical) and 400μL Phenol:choloform:isoamyl alcohol (25:24:1, Sigma-Aldrich) were added to the aliquots. Microbial cells were disrupted by mechanical lysis using FastPrep-24. Heat treatment and centrifugation were conducted after adding a cooling adaptor. Supernatants were treated with RNase to eliminate RNA. Total DNA was purified using gDNA columns as described in detail previously [178]. Briefly, the V3-V4 regions of 16S rRNA gene were amplified (15x15 cycles) following a previously described two-step protocol [179] using forward and reverse primers 341F-ovh/785R-ovh [180]. Purification of amplicons was performed by using the AMPure XP system (Beckmann). Next, sequencing was performed with pooled samples in paired-end modus (PE275) using an MiSeq system (Illumina, Inc.) according to the manufacturer’s instructions and 25% (v/v) PhiX standard library. On the BARCELONA dataset the data was processed using 16S-V3V4 primers pair 341f/806r on a MiSeq Nano sequencing bcl2fastq (v1.8.4). The sequence of the primers used was: 341f: 5’-CCTACGGGAGGCAGCAG-3’ 806r: 5’-GGACTACHVHHHTWTCTAAT-3’ 3.2.2.2 Microbial profiling For the HSCT dataset the processing of raw-reads was performed by using the IMNGS (version 1.0 Build 2007) [181] pipeline based on the UPARSE approach [180]. Sequences were demultiplexed, trimmed to the first base with a quality score &lt;3 and then paired. Sequences with less than 300 and more than 600 nucleotides and paired reads with an expected error &gt;3 were excluded from the analysis. The 5 nucleotides from each end of the remaining reads were trimmed to avoid GC bias and non-random base composition. Operational taxonomic units (OTUs) were clustered at 97% sequence similarity. Taxonomy assignment was performed at 80% confidence level using the RDP classifier and the SILVA ribosomal RNA gene database project. Later the data was normalized using the same method as for RNA-seq described above. The microbiome was visually inspected for batch effects in PCA; none were found. The resulting OTUs table was normalized using edgeR (Version 3.28 or later) [112]. For all the other datasets dada2 [182] was used to analyze microbiome data. It creates amplicon sequencing variants from the 16S sequencing data, without merging similar sequences at any threshold. It is an alternative to OTUs which allows to be able to compare results between studies because it does not summarize together any sequence and provides more resolution to differences on the fragment amplified. 3.3 Statistics Differential expression analysis was performed with the following the limma-trend method [183, 184] and edgeR [112] (Bioconductor version 3.10 or superior) packages, normalized using the trimmed mean of M-values and log-2 transformed into counts per millions following the workflow described on [185] using voom which in addition performs a count normalization. The samples were adjusted for inter-patient differences specifying block argument for patient variable. To correct for multiple testing, the false discovery rate was estimated using the method of Benjamini and Hochberg [186]. A gene was considered differentially expressed when it was significant at 5% FDR special attention was given to those genes that showed a fold-change higher than |1.5|. Besides differential expression tests and correction for multiple testing, several different methods were used on this thesis which will be described below. Functional enrichment related tests/methods stand out from the other methods used. 3.3.1 Functional enrichment Functional enrichment methods are those methods that aim to provide with more information about the variables besides their numerical value measured. They can be very different in nature but they all use the numeric values of the variables and other information, being it from the same experiment data collection or from external data sources. Most of them are based on an over representation analysis (ORA), where a group of elements is tested whether they are present on other group. This can be done with clusterProfiler which tests genes enrichment for functionality based on information on pathway databases [187]. It checks the enrichment of features of a given group on the (background) list provided. \\[ \\begin{cases} H_0 &amp; : &amp; P_{subset} \\leq P_{overall} \\\\ H_1 &amp; : &amp; P_{subset} &gt; P_{overall} \\end{cases} \\] The test used is usually the fisher test, the hypergeometric test, the proportion test. I describe the hypergeometric test and the wilcoxon test below. 3.3.1.1 Hypergeometric test The hypergeometric distribution describes the probability of \\(k\\) successes (when the object drawn has a specified feature) in \\(n\\) draws3, from a finite population of size \\(N\\) that contains exactly \\(K\\) objects with that feature, wherein each draw is either a success or a failure. Applied on this context \\(N\\) is the number of genes being used and \\(n\\) the number of genes on a pathway. So it can be used to compare the genes found on it (\\(k\\)) compared to the expected \\(K\\) numbers of the distribution using the following equation: \\[ p_X(k) = \\Pr(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}} \\] 3.3.1.2 Proportion test The overrepresentation of a given group of elements can be also tested with the proportion test, which is sometimes also used on clusterProfiler. The proportion test uses the \\(\\chi^2\\) distribution to test if the observed frequency (\\(O_i\\)) is close to the expected frequency (\\(E_i\\)): \\[ \\chi^2 = \\sum_{i =1}^n \\dfrac{(O_i - E_i)^2}{E_i}\\simeq\\chi_{n-p}^2 \\] As this is usually done on a 2x2 contingency table it is equivalent to the Z-test of proportion. Sometimes, the expected frequency is so low that a correction must be done to the estimation: \\[ \\chi_{Yates}^2=\\sum_{i =1}^n \\dfrac{(|O_i - E_i|-0.5)^2}{E_i} \\] This increases the \\(p\\)-value as it raises the Chi-square statistic. 3.3.1.3 Gene Set Enrichment Analysis There are other methods that to test if some variables shows an unexpected importance according to a statistic like fold change or value gene set enrichment analysis was developed [188] . Gene set enrichment analysis (GSEA) is a computational method originally developed to determine whether a priori defined set of genes shows statistically significant and concordant differences between two biological states. These methods check if a group of variables present in an ordered list is present in some skewed distribution and compare it against a random group of similar size. This method consist on first calculating the rank of genes:\\(rank(g_j)=r_j\\) where each \\(g\\) is a gene. Then later calculate the following functions: \\[ P_{hit}(S, i)=\\sum_{g_j \\in S, j \\leq i}\\dfrac{|r_j|^p}{N_R}\\text{, where } N_R = \\sum_{g_j \\in S}|r_j|^p \\] \\[ P_{miss}(S, i) = \\sum_{g_j \\not \\in S, j \\leq i}\\dfrac{1}{N - N_H} \\] With these values the enrichment score (ES) defined as: \\(ES=max(|P_{hit}(S, i)-P_{miss}(S, i)|)\\) is calculated from the walk. A high number of permutations are required for an accurate score of the enrichment score. However, when more than one pathway (\\(S\\)) is evaluated in order to compare between the enrichment scores, they must be normalized by dividing it by the mean of all the ES. When power \\(p\\) is 0 it is equivalent to the standard Kolmogorov–Smirnov statistic, though it is usually set to 1. For testing gene set enrichment analysis I used fgsea [189] implementation for its speed and integration with other methods used on this thesis. Pathways of genes from the REACTOME database were tested on the weight of different models or on the comparisons performed [190]. 3.3.1.4 GSVA To estimate the expression of the pathways and compare their expression levels between conditions gene set variation analysis as implemented on GSVA was used [191]. It is a method that summarize the variables’ numerical value changing the space of \\(variable\\text{ x }sample\\) to \\(group\\text{ x }sample\\) . This enables other methods to use this new space instead of the original variables. GSVA was used (again from the REACTOME database) and try to find the relationships between the pathways and the microbiome at different taxonomic levels. This is done via an estimation and a comparison with a discrete Poisson kernel: \\(i\\) indicates the gene till \\(k\\) and samples are indicated by \\(j\\) till \\(n\\). \\[ z_{ij}=\\hat{F_r}(x_{ij}) = \\frac{1}{n}\\sum_{k=1}^n\\sum_{y=0}^{x_{ij}}\\dfrac{e^{-(x_{ik}+r)}(x_{ik} + r)^y }{y!} \\] where \\(r = 0.5\\) is used in order to set the mode of the Poisson kernel at each \\(x_{ik}\\), that is, similar to a gene expression for a given sample. Later this is converted to ranks \\(z_{(i)j}\\) for each sample and normalized: \\(r_{ij}=|\\frac{p}{2}-z_{(i)j}|\\) to make the distribution of ranks symmetric around zero to later compare with a normal distribution using a Kolmogorov-Smirnov-like random walk statistic: \\[ v_{jk}(l)=\\dfrac{\\sum_{i=1}^l|r_{ij}|^{\\tau}I(g_{(i)}\\in \\gamma_k)}{\\sum_{i=1}^p|r_{ij}|^{\\tau}I(g_{(i)}\\in \\gamma_k)}-\\dfrac{\\sum_{i=1}^lI(g_{(i)}\\not \\in\\gamma_k)}{p-|\\gamma_k|} \\] Here tau \\(\\tau\\) describes the weight of the tail in the random walk (default is set to 1). \\(\\gamma_k\\) is the k-th gene set and \\(I(g_{(i)}\\in \\gamma_k)\\) is the indicator function whether the gene ranked i-th belongs to the gene set \\(\\gamma_k\\) . \\(|\\gamma_k|\\) indicates the ordination of the gene set, the number of genes of the gene set and \\(p\\) the number of genes in the data set. This difference is later converted to enrichment score for each gene set for each sample similar to GSEA, either a difference of hits and misses or the maximum deviation from zero of the random walk (which is used as it allows to detect gene sets that have genes with different expression patterns). 3.3.2 Other methods used 3.3.2.1 PERMANOVA The PERMANOVA method [192, 193], provided by the vegan package on the adonis function, was used to test if microbiome data variance is due to other variables when using distances metrics. It uses the residual sum of squares such as: \\[ SS_W = \\frac{1}{n} \\sum_{i=1}^{N-1}\\sum_{j=i+1}^N d_{ij}^2\\epsilon{ij} \\] Which when using euclidian distances (\\(d\\)) it is equivalent to MANOVA. Here \\(\\epsilon_{ij}\\) takes the value of 1 if the observation \\(i\\) and the observation \\(j\\) are in the same group, otherwise it takes the value of zero. This can be later used to test which variance is bigger, inter-groups or intra-groups by using the following formula: \\[ F = \\dfrac{SS_A/(\\alpha -1)}{SS_W/(N-\\alpha)} \\] Where \\(SS_A\\) is the among group sum of squares, representing the intra-group variance. \\(N\\) is the number of samples and \\(\\alpha\\) the number of different groups. This allows to test if the variables are related to the variance of the data as it can be compared with the \\(F\\) statistic after a high number of permutations. This is particularly important on the microbiome data, which is highly variable. It was used to detect which variables were important to include in the models of the disease. Variables were tested together in an interaction model and also each one individually alone with the blocks of data. 3.3.2.2 globaltest I tested which variables, (sex, age, location, time since diagnostic, treatment) are important on the datasets with globaltest. It is a package with methods for testing complex hypothesis and help decide if the variables were influencing [194]. It provides a general test statistic to test a hypothesis against a high dimensional dataset. \\[ S = \\sum_{i=1}^p x_i^{&#39;} x_i g(t_i^2) \\] The global test performs a test statistic on the tranformed t-test, where if \\(p\\), the number of variables, is large the test is more powerful on average over all possible sparse alternatives of general functions \\(g\\). It was performed with variables individually and also with interactions between the different variables. 3.3.2.3 Diversity indices Microbiome diversity was measured using vegan and phyloseq methods [195]. \\(\\alpha\\)-diversity is a measure of how much a given microbiome at a taxonomic level is present on a sample. Several measures exists, on the thesis I used the effective Simpson or Shannon diversity index to compare diversity between samples and conditions. \\(\\beta\\)-diversity was calculated using the phyloseq package for exploratory analysis. 3.3.2.4 WGCNA We wanted to see if using some co-expression measure between the microbiome and the RNAseq would help identify relationships. We used weighted gene co-expression network analysis as implemented on WGCNA [196] and also correlations. The spearman rank correlation coefficient is Being \\((X_1 , Y_1 ),\\dots, (X_n , Y_n)\\), assign a rank where \\((R_1 , S_1 ), \\dots , (R_n , S_n )\\) for n being all the variables which: \\[ R_s(X,Y) = \\dfrac{\\sum_{i=1}^n (R_i - \\bar{R}) (S_i - \\bar{S} )}{\\sqrt{\\sum_{i=1}^n (R_i - \\bar{R})^2}\\sqrt{\\sum_{i=1}^n (S_i - \\bar{S})^2}} \\] where \\(\\bar{R}=\\dfrac{1}{n}\\sum_{i=1}^n R_i\\) conversely to S: \\(\\bar{S}=\\dfrac{1}{n}\\sum_{i=1}^n S_i\\). The distribution of the Spearman correlation coefficient is symmetric around 0 and can be approximated to a normal distribution as \\(\\sqrt{n-1}R_{s(X,Y)} \\sim N(0,1)\\) which can be used to calculate the p-value of a given estimation. 3.3.2.5 MCIA To find relationships between microorganisms and genes multiple co-inertia analysis, also known as MCIA, was used. It is a method to examine covariant gene expression patterns between two blocks [197] provided by the package omicade4. MCIA maximizes the following formula: \\[ \\sum_{k=1}^K w_k{cov}^2 (X_kQ_ku_k, v) \\] where \\(K\\) is the total number of matrices, \\(X_k\\) the transformed matrices and \\(Q_k\\) is a square matrix with \\(r_{ij}\\) in diagonal elements indicating the hyperspace of features metrics, \\(u_k\\) are auxiliary axes, \\(v\\) the reference structure and w the weights of the matrices. This can be used to obtain a dimension of \\(P_k^d=u_k^d(u_k^dQ_k{u_k^d}^T)^{-1}u_k^dQ_k\\) given that for each dimension the residuals are obtained following \\(X_k^{d-1}=X_1^d-X_1^dP_1^{d-1}\\) where \\(d\\) are the dimensions needed. It was used as a baseline method to compare the RGCCA integration. 3.3.2.6 STATegRa To explore how much do different blocks of a dataset have in common STATegRa was used [198]. It is a framework for integrating datasets with two data types using parametric and non-parametric methods. The methods used are omics component analysis based on singular value decomposition (SVD) of the data matrix. There are three different methods provided to this end: DISCO-SCA, JIVE and O2PLS. On DISCO-SCA it uses: \\[ X_k = TP_k ^ T + E_k \\] with \\(T\\) the \\(I \\times R\\) matrix of components scores that is shared between all blocks and \\(P_k\\) the \\(J_k \\times R\\) matrix of components loadings for block \\(k\\). Let \\(X_1,X_2, \\dots X_i\\) be blocks of data and \\(X=[X_1,X_2, \\dots X_i]\\) represent the joint data, then the JIVE decomposition is defined as: \\[X_i=J_i+A_i+\\epsilon_i \\text{, }i = 1,2, \\dots\\] where \\(J=[J_1,J_2, \\dots J_i]\\) is the \\(p \\times n\\) matrix of rank \\(r&lt;rank(X)\\) representing the joint structure, \\(A_i\\) is the \\(p_i\\times n\\) matrix of rank \\(r_i &lt; rank(X_i)\\) representing the individual structure of \\(X_i\\) and \\(\\epsilon_i\\) are \\(p_i \\times n\\) error matrices of independent entries. Finally, the O2PLS approach uses multiple linear regression to estimate the pure constituent profiles and divides the systematic part into two, one common to both blocks and one not. The O2PLS model can be written as a factor analysis where some factors are common between both blocks. \\[ \\begin{equation} \\(\\text{X model: } X = TW ^ T + T_{Y-ortho} P ^ T_{Y-ortho} + E) \\\\ \\(\\text{Y model: } Y = UC ^ T + U_{X-ortho} P ^ T_{X-ortho} + F) \\\\ \\(\\text{Inner relation: } U = T + H) \\tag{3.1} \\end{equation} \\] Were each model is build similarly by adding the subtraction of the projected values of the other component keeping the relationship between them as stated on the third line. 3.3.2.7 BaseSet When evaluating a model using bootstrap, BaseSet was developed and used to find which variables were really involved on the interaction and how likely were to be together. It is a package that uses fuzzy set logic for easy calculation of probability to belong to a group. A set \\(S\\) is a group of elements for which each element \\(e\\) has an \\(\\alpha\\) membership to that set. \\(\\alpha\\) is usually limited between 0 and 1: \\(\\alpha \\in [0, 1]\\) . A given element \\(e\\) can belong to more than one set. Assimilating the membership function to probability we can calculate the probability of a given element \\(e\\) to belong to a set \\(S\\) and not any other set: \\(P(e \\in S|e\\not \\in S^c)\\). Which applied to the data and case at hand is the probability that a given variable is associated with a given outcome and not with any other outcome. The membership function was derived from the bootstraps used for each model on the thousand iterations of the integrative method applied to give an estimation of how probable is a given gene and bacteria to be selected as relevant for the model. With the probabilities of the bootstraps it was used to calculate (via set_size) the genes and bacteria that are specific of the model that allows to separate the transcriptome by its location and the microbiome by the disease status. 3.3.2.8 experDesign experDesign was developed [199] to prevent and quantify if a given experiment has batch effect due to the batches used to measure the values or other known variable. It might help to detect a bad design of the experiment. On pseudo code the core of the program can be described as: for each index: for each batch pick size(batch) samples if samples are in another batch pick other samples for each batch calculate some summary statistics compare with the summary statistics of all the samples keep the index with less differences between the index and all the samples It can take into account spatial distribution provide which technical replicates4 are best given the number of samples that fit on a batch. experDesign was used avoid batch effect on the sequencing of the BARCELONA dataset and evaluate them after it was sequenced. 3.3.2.9 ROC- AUC To estimate if the selected features (genes or microorganism) by the integration methods have some biological meaningful contribution I measured if they can classify features, such as, which gastrointestinal segment is each sample from, or which type of disease does each patient have. To compare between different models using the receiver operating characteristic (ROC) curve and the area under the curve (AUC) of said curve was calculated with the pROC package [200]. It is based on the following formulas, where \\(FP\\) is false positive, \\(N\\) is a negative, \\(P\\) is positive, \\(TP\\) is true positive and \\(FN\\) is false negative: \\[ FPR = \\frac{FP}{N}=\\dfrac{FP}{FP+TN} \\] \\[ TPR = \\frac{TP}{P}=\\dfrac{FP}{FP+FN} \\] The ROC curve is that where the true positive rate (TPR) or sensitivity, recall or hit rate is represented against the false positive rate (FPR) on the x axis. The area under this curve is a measure of how good such classifications performs overall. The higher it is to 1 the better as it classifies incorrectly less samples and accurately classify them. 3.3.2.10 RGCCA The main method used on this thesis has been regularized generalized canonical correlation analysis (RGCCA) a method derived from the canonical correlation. Which is a method that uses data from the same sample but from different datasets. Regularized generalized canonical correlation analysis is implemented on the homonymous package RGCCA [201] which was used on this thesis. The method and implementation will be explained in detail on the next section. 3.4 Regularized generalized canonical correlation analysis The regularized generalized canonical correlation analysis is an extension to the canonical correlation analysis [202, 203] which is an old method to find agreement between two, or more, scorers (as it was first introduced on the literature). Over several years of progress on the field of canonical correlations [85, 86, 204–208] RGCCA emerged with a regularization step and an extension to more than two scorers. As previously methods did not have the hability to integrate more than 2 sources of data as does RGCCA [R-RGCCA?]. 3.4.1 Description The required data are numeric matrix, as big as you want, as it is designed for datasets with more variables than samples ( \\(p \\gg n\\)). It needs to have a complete case with no missing values (at the time of writing this thesis there is a new development version not released on CRAN5 yet that replaces any missing value by a 0) and time is not considered as a special variable. Data should be from the same samples and it is not specific to a certain technology or method used to obtain the data. It uses a dimensional reduction approach to relate the different data sets between them and produce specific factors for each dataset. The interpretation of the RGCCA results is complex and require expertise and familiarity with the technique used and is highly dependent on the model and options used. The authors realized that there is a special problem due to sparsity on biological data which could be handled using first another normalization to improve the stability and success of the canonical correlation methodology. To perform the dimensional reduction using the sparse method , the method consist on maximizing this function: \\[ \\underset{a_1,a_2, \\ldots,a_J}{\\text{maximize}} \\sum_{j, k = 1}^J c_{jk}g(\\mathrm{cov}(X_j a_j, X_k a_k)) \\mathrm{~~s.t.~~} \\Vert a_j \\Vert_2 = 1 \\text{ and } \\Vert a_j \\Vert_1 \\le s_j, j=1,\\ldots,J \\] Being \\(X_j\\) the values for sample \\(j\\), the weights of the variables of said sample are represented by \\(a\\). While \\(g\\) is a function that can take the form of \\(x\\), also known as Horst method, \\(\\Vert x \\Vert\\) known as centroid method, \\(x^2\\) known as factorial method, or any user-supplied function. The smaller the \\(s_j\\), the larger the degree of sparsity for \\(a_j\\). As \\(s_j\\) is closer to 0, more features are selected as it looks to optimize covariance, while if it is closer to 1 less features are selected and it resembles the correlation. The values of \\(s_j\\) were estimated using [209] when the block was from 16S data or RNaseq, otherwise 1 was used. The shrinkage is defined as: \\[ \\widehat{\\lambda}^{\\star} = \\dfrac{\\sum_{i\\neq j}\\widehat{Var}(r_{ij})}{\\sum_{i \\neq j}r_{ij}^2} \\] Where the \\(r_{ij}\\) are the correlation coefficients of the matrix between variables \\(i\\) and \\(j\\) . Where the variance is defined as: \\(\\widehat{Var}(S_{ij}) = \\dfrac{n^2}{{(n-1)}²}\\widehat{Var}({w}_{ij})=\\dfrac{n}{{(n-1)}^3}\\sum_{k=1}^n(w_{kij}-\\overline{w}_{ij})^2\\) And its components are: \\(w_{kij}=(x_{ki}-\\overline{x}_i)(x_{kj}-\\overline{x}_j)\\) and \\(\\overline{w}_{ij}=\\frac{1}{n}\\sum_{k=1}^nw_{kij}\\) representing \\(x_{ij}\\) the values of a sample \\(j\\) on a variable \\(i\\). There are different \\(g\\) functions that could be used but the centroid method was chosen to detect both positive and negative relationships without more computational costs. Categorical data was encoded as binary (dummy) variables for each factor except one to keep degrees of freedom, where 0 indicates not present and 1 indicates present omitting one level. Each block, regardless if it had continuous numeric variables or dummy variables was standardized to zero mean and unit variance. Later, it was divided by the square root of the number of variables of the block for an unbiased estimation. 3.4.2 Results RGCCA as other dimensional reduction techniques provides specific weights for each variable on each dimension and a sample score on each dimension, together with quality scores. To measure the quality of the model, the implementation provides indicators based on the Average Variance Explained (AVE): it returns an AVE score for each block, an AVE score which measures how each variable correlates with their block component, an AVE score of the inner model, which measures how each dimension accounts for the variance. The higher the score is to 1 the better does it work. However, that mathematically works better does not mean that it have more biological meaning or that it provides more insights on the biology. 3.4.3 Models There is no formal definition of what constitutes a block of data on multi-omics tools. Most multi-omics and integration tools assume one block for each type of data, such as an essay a survey or an experiment. We decided to split the block with data about the samples to separate independent variables from the same block. The hypothesis we made was that more blocks with highly related variables but independent from the other blocks would fit better the data and thus help to identify causal or dependent variables. To model what might be the relationships between the datasets current practices include using a pre-selected model of relations between blocks. However, this model might not be accurate and several models might need to be fitted. To help find the fitting model for the data I created an R package, named inteRmodel, which helps finding the right model and measure how fit it is for your data via a bootstrapping procedure and the provided AVE scores. This method was applied to the previously described datasets to find the relationship between microorganisms and the disease. Following this method: To provide a ground truth, a model with only the relationships between the two experimental obtained data is analyzed, on what it is called the model 0. The next model analyzed consists on the relationships between the two experimental blocks and them with the metadata of the samples, that is model 1. All models that have one block with metadata are from the same family of models 1. Later instead of a big metadata block, following our theory we split this metadata block on several ones, we have a block for time related variables, another one for location and the other about the people on the study. This allows to design a model with an expected relationships between these dimensions and help make more interpretable the relationships. These models are from the family 2. For each family of models we tested all possible models with weights between 0 and 1 (by 0.1 intervals) to find the best model on each datasets according to the AVE score. The final models were further validated using a bootstrap approach to measure their accuracy and likelihood on the data available. References "],["results-1.html", "Chapter 4 Results 4.1 Packages 4.2 Analysis", " Chapter 4 Results 4.1 Packages 4.1.1 experDesign experDesign package built in R was released for the first time on CRAN on 2020-09-08 after nearly a year after the initial release made on github. The package uses functional programming to create and modify objects and the features used. The package didn’t need a new class created to work and bases its performance on the large body of work made by the R core team. It simply adds the needed information on the columns of the introduced data.frame or returns an appropriate vector. experDesign functions are divided into several categories: Helper functions to aid on deciding how many batches are or how many samples per batch. There are some also that report how good a given distribution of the samples felt for a given dataset. Functions generating indexes. Functions distributing the samples on indexes Regarding time related variables experDesign will use them as factors, while issuing a warning to the user. Since its development it has been used on a couple of batch designs, one for organoids bulk RNA-seq and another one for biopsies bulk RNA-seq from the BARCELONA cohort. On both cases it worked well and no batch effect were created when sequencing samples. However on the organoids dataset, the change on the method of producing them introduced a batch effect that made it impposible to compare samples before and after that change. Since its release on CRAN it has had a median of ~400 downloads each month from RStudio package manager, so the total number of downloads is higher as not all the installations are from RStudio’s mirror. 4.1.2 BaseSet BaseSet package, built in R, was released for the first time on CRAN on 2020-11-11 after nearly two years after the initial work started on github. The package uses both functional programming and object oriented program to create and modify the TidySet S4 object defined6. Mixing it with S3 generic functions it provides a powerful interface compatible with the tidyverse principles, a group of packages following the same design. The package provides a new class to handle fuzzy sets and the associate information. BaseSet methods are divided into several categories: General functions to create sets of the TidySet class or convert from it to a list or about the package. Set operations like adjacency cartesian product, cardinality, complement, incidence, independence, intersection, union, subtract, power set or size. Functions to work with TidySets to add relationships, sets, elements or some complimentary data about them. Remove the same or simply move around data or calculate the number of elements, relations and sets. Functions to read files from formats where sets are usually stored in the bioinformatician world: GAF, GMT and OBO formats Last, some utility functions to set name conventions, use set_symbols and some other auxiliary functions The package had a long development process with initial iterations basing on GSEABase package which was later abandoned to include also some uncertainty on the relationship of a gene with a given gene set. The package also participated on an exploration on part of the Bioconductor community (project to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assay) for more modern and faster handling of sets. There were three different packages created as part of this process, BaseSet, BiocSet published on Bioconductor and unisets, available on github. The different approaches were presented at a birds of feather on BioC2019. The package passed the review on the rOpenSci organization (See review) and now is part of the packages hosted there too. Since its release on CRAN it has had a median of ~400 downloads each month from RStudio package manager, so the total number of downloads is higher. 4.1.3 inteRmodels The package was build once the method used to find accurate models of the relationships of the data available of a dataset using RGCCA was established. It simplifies the process and makes it easier to perform it. The package consists of functions that can be grouped in three categories: To look for models and evaluate them. There are functions to search for a model given some rules, that check them using leave-one-out methodology. Reporting: To make better reports by improving handling of names or simplifying the objects or how to calculate scores. Building: To easier build correct models on RGCCA, simplifying the process to create a symmetric matrix. Currently it is only available on github, so the number of downloads and usage is unknown but since its release a user has contacted to keep it up to date with development versions of RGCCA. Which now it is compatible with the next release of RGCCA being prepared7. Together with a copy of the RGCCA packages with more tests has enabled to identify changes on the RGCCA package that could affect users. 4.2 Analysis On the following sections the main results of analyzing each dataset are presented. 4.2.1 Puget’s dataset On this dataset the different parameters and capabilities of RGCCA were tested. The three different methods, centroid, factorial or horst were tested and compared. The main result of this comparison was that the differences of the selection of the variables mattered more than the number of variables selected with each method. The models were tested with different weights on all three schemes: horst, centroid and factorial. The horst and the centroid scheme were similar while the factorial resulted in the most different AVE values (see S1 Data of [168]). The centroid scheme was selected because it takes into account all the relationship regardless of the canonical correlation sign between the blocks and it is similarity to horst scheme. The effect of the sparsity value was measured by its effect on the inner AVE scores and the combination of the different values for each block. Figure 4.1: Effect of tau on the inner AVE. The suggested tau value is the column between the regular grid. Exploratory analysis with the superblock model was done. The first two components of the superblock didn’t help to explain the biology or classify the tumors: Figure 4.2: First components of the superblock The same data was used to look for a good model from the data itself including a model with a superblock but looking at the first component of the CGH and transcriptome block. This allowed to visually inspect if each model’s components helped to classify the samples: Figure 4.3: Different models tried with the same data showing the first components of the CGH data and the transcriptome. Showing the components of the CGH and the transcriptomics of the superblock show better classification than that of the superblock. However the other models show a better classification of the samples with much simpler models. To find these models the three blocks with the best tau and the centroid scheme were analyzed by changing the weights between 0 and 1 by 0.1 intervals. According to the inner AVE, the best model was the one in which the weights (1) between the host transcriptome and location, (2) the host transcriptome and the CGH, and (3) the CGH block were linked to variables related to the location with weights of 1, 0.1 and 0.1, respectively. When we added a superblock to the data, there was a slight increase of 0.01 on the inner AVE of the model. The model with the superblock that explained most of the variance was that in which the weights of the interaction within (1) the host transcriptome, (2) between the superblock and the CGH, (3) between the host transcriptome and the localization, and (4) between CGH and the host transcriptome were 1, 1, 1 and 1/3, respectively. To see if the superblock could classify the sample by location, we plotted the first two components of the superblock. We can clearly see that they do not classify the samples according to the location of the tumor, which is known to affect the tumor phenotype [169]. Adding one block containing the age of the patient and the severity of the tumor to the model, decreased the inner AVE. The best model with these blocks, according to the inner AVE, was that in which the interactions (1) within the host transcriptome, (2) between the host transcriptome and the localization, (3) between the host transcriptome and(4) the CGH and between the CGH and the other variables were 1, 1, 1/3 and 1/3, respectively. The first components of each model can be seen in the figure: Figure 4.4: Different models tried with the same data showing the first components of the CGH data and the transcriptome. We can observe on the figure, the strong dependency between gene expression and location since the first model while the weaker relationship with the CGH assay [169]. On the other hand, the major difference is the dispersion on the CGH component on each model. The effect of the superblock and weights on different models to the inner AVE. There are significant differences between having the superblock and not having it. Figure 4.5: Effect of superblock and weights on the inner AVE 4.2.2 HSCT dataset The permanova analysis was performed on this dataset to estimate which were the variables that are more relevant. From the many variables the location, sex, patient id and others were found to be related to the variability of the microbiome or the transcriptome on this dataset. With the permanova analysis we found that more of the 50% of the variance of normalized RNA-seq data and microbiome data respectively is explained by the variables of location, disease, sex, and the interaction between disease and sex. On the transcriptome the most important factor is location which is more than 15% of the variance, while on the microbiome data the most important factor is the patient id followed by location of the sample. With globaltest the results were similar. The resulting p-value was well below the 0,05 threshold defined for RNA-seq data on the models including the segment of the sample, sex and treatment. On the microbiome data the results were similar but the p-value was considerably higher but still below the threshold. Figure 4.6: Alpha diversity according to Shannon and Simpon effective measures Diversity indices of the samples were explored and compared for several subsets. Splitting by location of the sample and disease provided the highest differences and the diversity index along time did not change much. Weighted gene co-expression network analysis did not provide relevant links between bacteria and transcriptome as it failed to find an acceptable scale free degree. STATegRa didn’t show a value of, the assumption that there is a shared common factor without influence of other categorical variables was too much. In addition, the model is fixed, so it didn’t allow to find new or other relationships that are not one to one. With RGCCA we could select different models and use all the data available without much assumptions. The models with the highest inner AVE of the family 1 and the family 2 models were similar to those on the Häsler dataset. The weights of these models can be observed here: The best model of the family 2 confirmed a relationship between the host transcriptome and the location-related variables, while the microbiome was associated with the demographic and location-related variables (see Figure and S2 data of [168]). Overall, we see that the relationships in the model affected the distribution of samples on the components of both the host transcriptome and the microbiome. The different models selected different variables, some of which are shared between models. The most similar models are those that have split the metadata into 3 blocks, followed by those that have the metadata in a single block. In order to analyze the accuracy of the models, one thousand bootstraps were used to integrate the data from the HSCT CD dataset. Each bootstrap had its own dispersion on the variables according to the samples selected, the distribution of the bootstraps used are represented here: Figure 4.7: Dispersion of the bootstraps on the age and percentage of colon and controls samples. Evaluating the same model on each bootstrap lead to a dispersion on the inner AVE of the model. The lower the dispersion, the more robust the model was to different conditions than in the initial testing. Figure 4.8: The point with the black circle is the AVE of the original data. The dispersion is shown by the ellipses With the bootstrapped models we used BaseSet to estimate the probability that each variable to be relevant for the association with a disease. However, due to big amount of small probabilities when using the BaseSet package to calculate which variables are more relevant it couldn’t provide a good estimation on time. MCIA was applied as a baseline of the integration, the first two components were represented similarly to those of the blocks when using RGCCA. Figure 4.9: MCIA first two synthetic variables on the IBD related cohorts. The AUC of classifying the transcriptome in colon or ileum segments was compared between the two methods. Figure 4.10: ROC curve of the different datasets with the models from RGCCA and the result with MCIA The different models selected different variables as can be seen below: Figure 4.11: Upset plot of the variables selected on each model showing the intersection between the different models 4.2.3 Häsler’s dataset In this dataset, the parameter tau behaved slightly differently than with the previous dataset but the value from the Schäfer’s method for tau was close to the best value. In contrast to the HSCT’s dataset, the model with the highest inner AVE was model 1.2 (inner AVE value of ) but model 2.2 was close to it (inner AVE of ). Model 2.2 has a relationship of 0.1 between microbiome and the host transcriptome and of 1 between the location and the host transcriptome. The microbiome block is also related by a factor of 0.1 with the demographic block and of 1 with the time block. Lastly, the time and the demographic block are related by a factor of a 0.1. In either case the family 1 and family 2 models can correctly separate by sample location (colon or ileum) but not by disease type or inflammation status as can be seen below. Figure 4.12: Models on the Häsler’s dataset There is no observable cluster of IBD samples and the other samples, showing that on this dataset the differences of the microbiome between the different type of samples are less stark. MCIA was applied as a baseline of the integration and compared to the different models to know which one separates best colon and ileum samples. Figure 4.13: ROC curve of the different datasets with the models from RGCCA and the result with MCIA MCIA results was high but not as high as the model 2.2. 4.2.4 Morgan’s dataset We tested if results of inteRmodel were consistent on this dataset. The different models weren’t able to separate the samples neither by location or sex. Figure 4.14: First component of the transcriptome and microbiome of the Morgan’s dataset. Nevertheless we compared the classification with the MCIA algorithm and still resulted that model 2.2 provide a better classification than MCIA. When exploring the bootstraps of the data we found that model 1.2 is highly variable: Figure 4.15: Inner and outer AVE scores of the bootstrapped models. In addition the model 2.2 usually has a lower inner AVE compared to model 1.2. 4.2.5 BARCELONA dataset This dataset was processed but the results are not trustable as the microbiome diversity falls off the accepted values. Diversity plots 4.2.6 Howell’s dataset This dataset was processed to confirm the results on the previous datasets. Model 1.2 was the best according to the AVE score but perform worse when attempting to recreate known biological differences via classifying samples. Model 2.2 was selected. Figure 4.16: The three main models on the Howells dataset. Colored by section colon, ileum and shape according to the disease: square, ulcerative colitis; triangle, normal; circle, Crohn’s disease. Model 1.2 has a 0.1 relationship between the ASV and the transcriptome and 1 between transcriptome and metadata. While model 2.2 has a relationship of 1 between location and transcriptome and demographics and ASV but only of 0.1 between demographics and location. Figure 4.17: Bootstrap of the different models. The bootstrapping showed that model 1.2 has indeed higher inner AVE values than model 2.2 and is more stable than model 1.2. While model 0 shows a high variation according to which samples are selected. On this dataset we also focused on the most important ASV according to the model 2.2 that were present in more than 2 samples that in total were present in the whole dataset. These ASV were summarized to a single value and then used to calculate the AUC, which was 0.85. The dot product of the ASV and genes were also calculated and used to find out which ASV are related to which genes. 4.2.7 Hernández’ dataset This dataset was processed to confirm the results on the previous datasets. The models are. Model 1.2 is, model 2.2. References "],["discussion.html", "Chapter 5 Discussion 5.1 Relevance for other research 5.2 Implications of research", " Chapter 5 Discussion Häsler data data were obtained using the same sequencing techniques from endoscopic biopsies as our dataset HSCT and Barcelona and it is of the same disease/field. The confirmation that it works on it it helped to continue forward. Transciptome is related to localization of the samples. Microbiome is more related to the disease and other variables of the patients. Time is an important variable when modelling the disease, if multiple timepoints are taken they should be taken into account to identify the state of the disease for each patient. Comparing different dataset Shared selected variables BaseSet did not work, it is computationally expensive to calculate the likelihood of 1500 variables, there are too many combinations and the numeric precision of said calculations must be considered carefully. In this chapter we will summarise the main findings in relation to the broad research community and other work as well as the impact of the results to improve further research in the future or be used on clinical environments. If we had to summarise all the work in one sentence we would say that… Many work focus on finding some genes or bacteria to answer a question they have in mind. On the thesis the focus was more on finding a good representation of the disease that allowed me to identify genes and bacteria that were relevant to our questions. There are many methods for integration developed, each with their shortcomings and strengths. There are frequent new tools and methods released, and the most up to date list of tools is a collaborative list. With the increase of tools there have been more pressure on comparing different tools. There have been already some articles comparing the different tools, which the previous mentioned list also has. There are reviews of different tools on the same datasets [211], some are more theoretical, others are focused on a field. WGCNA did not work. adonis worked to identify which variables were important. MCIA works well as baseline. STATegRa not customizable the interaction between the datasets. RGCCA some improvements. Multi-omics seems to focus on metagenomics, metatranscriptomics and metaproteomics and abandon plain 16S sequencing. [212] How to move the transition from bench to hospital [213]. New technologies like “organ-on-chip” or more specifically “gut-on-chip” will be of great help in identifying causality by introducing the gut microbiota in this system and then study the interaction of the gut microbiota and the intestinal epithelium. BaseSet did not work out, too many combinations and problem with numeric precision of multiplying &gt; 1000 float numbers. 5.0.0.1 Designing models The model of the generalized canonical correlation is highly dependent of the blocks present. If one has preexisting theories about the data, a specific model can be used stating these known or hypothetical relationships. However, if new relationships are being explored or no prior beliefs on the data are held the models should be created with random links between blocks, and evaluate which model is better. 5.0.0.2 Evaluating models To evaluate a model RGCCA provides the average variance explained (AVE), inner and outer. Inner AVE is for how well do all the canonical dimensions correlate with the design of the experiment, so it a measure of how good the model is. While outer AVE is a measure of how well do the variables of each block correlate with the canonical dimension, so it measures the agreement between the variables and the canonical dimensions. Depending on the goals of the research one or the other should be used. If we are more interested on the model of the relationships the inner AVE makes more sense. Furthermore, to evaluate a design bootstraping can be used to know how well the design does apply to a variety of data. Another option is to use an external cohort to validate the same model, or using a different method to see if it finds the same relationships or explains the data as accurately. Of the multiple methods available we used MCIA [214]. Which was compared by looking at the area under the curve for classifying the samples according to their location. Besides a way to compare methods, these models do need to be evaluated by the insights they provide on the biological system they are being applied to, in our case the Crohn’s disease. In this article we did not look in depth to the biological relevance of the microorganisms an genes found. The procedure of separating independent variables in their own block of data and later search the best model that fits the data provides a good strategy that should be consider for integration efforts. The procedural method of searching a model and testing them is implemented on inteRmodel. But the most important thing is to consider which variables are independent of which and if they can be separated into a block for later usage on the modeling. 5.0.1 Other Quality of RNAseq analysis and 16S sequencing is important. Avoid contamination, same primers and read lengths (if using Illumina machines). Comparing 16S taxa ASV or OTUs is hard and selectying the right tool to compare them is important [215]. Some batch effects are avoidable due to the experDesign package. This might help to improve the quality of the datasets and to expand previously sequenced datasets. 5.1 Relevance for other research It is hard to provide more information that can be later used by researchers on the wet lab. Paper about using transcriptomics to infer inflammation without colonoscopy \\[@ungar2022\\] There is a disconnect between the computational side and the experimental side, driven by the difficulty to think an experiment to test the new information that multi-omic experiments provide. Many combinations possible that are hard to explore, further research on how to reduce the space of possible combinations of bacteria or evaluate which combinations are more important might be useful on the future. As this might help focus on the most promising bacteria. 5.2 Implications of research Translational research? How to apply this results ? Sequence just the ASV on patients to classify them on disease might be an option that would require further clinical validation. Selecting which genes are related to the microorganisms is will require also further validation (for which I did not have the creativity to think of). The several environmental factors highly affect the disease, so analysis or comparisons without taking into account them might provide misleading or false results. References "],["conclusions.html", "Chapter 6 Conclusions", " Chapter 6 Conclusions Microbiome under some dataset can classify the patients according to their disease. Doing so reliable might take some time. It is hard to make it generalizable. "],["acknowledgments.html", "Chapter 7 Acknowledgments", " Chapter 7 Acknowledgments Juanjo Pau + grup Nuria, Elena, Helena, Aida. Metges: Julià, Helena… Ana, Alba, Isa, Marisol, Maica… Azu Familia Twitter #rStats and other people on Bioconductor (support.bioconductor.org) Biostars, Bioinformatics SE Dr. Häsler for kindly providing metadata from their cohort and Dr. Cristian Hernández for their collaboration and selfless sharing the samples of their cohort. "],["references.html", "References", " References "],["appendix.html", "Appendix 7.1 Online resources 7.2 Software", " Appendix 7.1 Online resources Some links that I found useful on the thesis and could be useful if you are interested on the multi-omics field. Awesome multi-omics: An online repository of references to multi-omics methods. Bookdown: The book about how to write this type of books. Bioconductor: The project about bioinformatics on R mostly related to sequencing technologies. CRAN: The main archive of R extensions/packages for R. GitHub: Company which allows to freely host remote git repositories of many projects, including some used or developed on this thesis. 7.2 Software Along the years of this thesis several pieces of software have been generated as well as packages. Here they are listed for easier retrieval. They are listed on two ways, one with a brief explanation and another one ordered by what software piece is used on each analysis. 7.2.1 Listed An improved/tested version of RGCCA, some modifications on the internal functions to ease the maintenance as well as adding tests and sometimes improving the documentation. Also modified so that it is possible to provide a vector of models so that the model of the first dimension is not the same as the model on the second dimension (not sure if mathematically speaking makes sense but from a biological one I think it might be interesting to have it). Designed to be used with RGCCA I wrote the package inteRmodel to ease the bootstrapping and model selection. A package to design batches to avoid batch effect experDesign and its website on GitHub. Explore the effects of the hyperparameters on RGCCA on the provided dataset of gliomaData (Originally provided here) there is this repository sgcca_hyperparameters. We used a pouchitis cohort published in this article[216] that was used to compare how performs our method in other’s dataset. The code used can be found in this repository. Some functions used to explore the TRIM dataset ended up in the integration package.This include functions for correlation, network analysis, enrichment, normalization of metadata… I developed a package to analyze sets and fuzzy sets BaseSet (based on what I learned from a previous iteration of the package). This package was meant to be used with the probabilities that arise from bootstrapping the models. However, due to the long times of calculation that it would require it was not used. To analyze the antiTNF cohort (also named BARCELONA) a different repository was created to analyze the data using the previously developed packages. 7.2.2 By project All code of the analysis of the publications is available (in his messed state and complicated history) and a brief description as to why they were used: Multi-omic modelling of inflammatory bowel disease with regularized canonical correlation analysis: TRIM: Mangle with the sample, dataset, explore several methods… sgcca_hyperparameters: Explore the effects of the hyperparameters on RGCCA on the provided dataset. inteRmodel: Package for easy repeating the methodology developed on TRIM. pouchitis: Work with the pouchitis cohort used in this article. uncoupling: Work with the UC/CD dataset used in this article. integration: Package with functions that I wrote or used on different parts of exploring the TRIM dataset ended up here. BaseSet: BaseSet: Fuzzy logic implementation, available on rOpenSci too experDesign: experDesign: Help design experiments in batches, available on CRAN too. References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
