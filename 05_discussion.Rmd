# Discussion

In this chapter we will summarize the main findings in relation to the broad research community and other work as well as the impact of the results to improve further research in the future or be used on clinical environments.

## Preliminary steps

Quality of the initial data for integration is important for a valid integration analysis.
On RNAseq and 16S analysis the sequencing data quality is important.
On sequencing data it is very important to avoid contamination and have enough amount of data for the analysis.
To avoid contamination it is very important the protocol how are processed the samples before the sequencing.
In addition control samples can be added, a blank to control for extraneous material and a sample with known content to confirm that the sequencing worked correctly.

On bulk RNAseq the extraction procedure and sequencing protocol has been setup many years ago and works very well.
Rarely there are problems with bulk RNAseq sequencing and its quality.
On the contrary DNA extraction and 16S sequencing was a new procedure.
Initially it was done with the guidelines of collaborators.
Later, when we did ourselves we did not include control samples but after some problems we started adding at least blank samples to identify those bacteria that might come with the solvent or be due to contamination.

Once the 16S data was obtained we initially processed it to obtain OTUs.
OTUs were the standard some years back.
However, OTUs are not comparable between studies even those that use the same primers.
For this reason, and after a suggestion from a reviewer, we moved to process later 16S sequencing datasets to obtain ASV.
ASV allow to compare the taxonomic imputation between studies using the same primers.
However, comparing 16S taxa ASV or OTUs is hard and selecting the right tool to compare them is important [@nearing2022].

It is also worth to keep in mind that several species have variable number of 16S rRNA genes.
Higher sequences counts of certain 16S might not mean higher abundance of that species compared to another with lower 16S counts.
However, the exact copy number status may change even within the same bacterial species, making the correction difficult [@louca2018].
It is possible to accurately correct for copy number variation on mock populations where the species and the genomes are known (or at least the 16S rRNA), but harder on samples which content is unknown.

<!--# In addition, there are differences on the sequencing reads depending on the  regions selected to amplify the 16S. It would be better to fully sequence the 16S. -->

Furthermore, the precision of 16S sequencing for classification of taxa is not enough to understand its role on the gut.
Adherent invasive cells, are cells with same genetic content that show different behavior [@nadalian2021].
These microbiome is known to be present on patients with IBD.
Thus even with the complete sequence play a different role on the health of the patients.

Most of these steps were out of my control as they were performed by a lab teammate or by collaborators.
Other concerns are currently unavoidable or could not have been found beforehand.

### The datasets

There is not a methodology to calculate the size of cohorts for interaction or multiomic studies.
There are no clear rules about which alpha power or which kind of relations are tested.
This might be due to a lack of mathematical background and modelization of the relations on the biological field.
Further research on this area might help finding which kind of relationships can be found given certain dataset size.
In addition to the highly local and not generalizable interactions that happen on biological samples.
Usually size of the datasets is determined from practical reasons: either costs or patient recruitment on protocols.

There are some efforts to simulate multi-omic datasets [@martínez-mira2018].
They usually focus on RNA-seq, ATAC-seq (DNase-seq), ChIP-seq, small RNA-seq, Methyl-seq or proteomics.
To my knowledge there is no accurate generator of 16S datasets or other microbiome datasets.
This made impossible to simulate and compare different tools on a dataset with known relationships.
Current approaches focus on comparing tools on (several) previously published datasets [@cantini2021], an approach that we have taken too.
This approach was not used to compare different tools, that we also do it, but to validate findings of one dataset in other datasets.

There is not a golden dataset for integration.
There are many possible relationships between variables and different ways these relationships might be: regulated DNA, protein recruitment, transcription factors, siRNA regulation.
Not a single one can be measured on a sufficiently detailed timescale that it can matter and as a result we are left with trying to guess what are the the relationships between variables.
In addition, this is compound by the lack of explanations about how this relationships might come, despite other indications.

Comparing different datasets is complicated because each is collected with different goals and processed differently.
In addition, there is not a resource were datasets of publications are collected or the relationships between different datasets is not shared.
Authors upload their data to different, not centralized, data repositories, such as, gene expression omnibus ([GEO](https://www.ncbi.nlm.nih.gov/geo/)), European Genome-Phenome Archive ([EGA](https://ega-archive.org/)), European Nucleotide Archive ([ENA](https://www.ebi.ac.uk/ena/browser/home)), among others.
Authors might provide the processed data as supplementary material on their articles.
Some projects whose primary purpose is providing data for the community establishing their own dedicated sites to store the data [@humanmicrobiomeprojectconsortium2012].

Finding different datasets for very similarly purposes is very difficult.
And once it is found there might be batch effects.
There are tools to overcome this, but currently only work if they share some features in common [@ugidos2020].
This usually means having undergone similar RNA sequencing procedures.

All this made finding annotated, appropriate and similar datasets difficult.
We looked up for the most similar datasets in order to be able to confirm our results outside our own cohorts.
On this thesis there were two datasets collected and sequenced: the HSCT and the BARCELONA dataset.

As explained on [methods](#methods-hsct), the HSCT dataset is a very special cohort of patients undergoing hematopoietic stem cell transplant.
This treatment is reserved to patients whose all the other treatment failed and this is the only way to reach remission.
This is common on patients with long time CD.
These patients are highly characterized and there is follow up data for several years.
However, it does not include UC patients and they are not representative of the IBD, or even CD, patients.\
On the HSCT dataset microbiome features were OTUs.
OTUs can not be imputed afterwards to ASV.
As such, the annotation of the 16S data is not the same as in the others datasets.

The BARCELONA dataset includes samples from CD and UC of an ongoing study.
Similar to the HSCT dataset, patients on this study were followed up for up to a year.
The patients were younger and had less years with the disease, which could provide with more insights on the initial relationships between the microbiome and the mucosa of the IBD.

However, the data quality of BARCELONA was not good enough to make reliable analysis and confidently extract hypothesis or relationships.
It is not clear what happened and as a consequence it is not clear how to avoid such problems on the future.
It could be a problem of the original DNA extraction and/or of its sequencing process which did not include both positive and negative controls.

What happened with the BARCELONA dataset -and the organoids cohort not described here- highlights the importance of data quality checks.
On the BARCELONA cohort despite no batch effect or other quality issues with the data as checked with `experDesign`, the diversity indices made impossible using it.
The organoids cohort is a dataset that was sequenced following the `experDesign` recommendations for distribution of samples on different batches.
However, a batch effect was found.
The batch effect was created on the lab process before the samples' sequencing.
Caution and a good relationship with the data sample and data generator is important to discover these kind of problems and ensure the quality of the data.

Other datasets used from published sourced were assumed they had already passed enough controls and had a good quality.
Nevertheless, they were screened to avoid quality issues.

The Puget's dataset dataset provided a good benchmark to test the methods and understand how RGCCA works.
It is a completely different type of data (except for the microarray that is similar to the RNAseq).
But we were not as much interested on the biology as on learning about the methods and possibilities of integration.

Häsler data were obtained using the same sequencing techniques from endoscopic biopsies as our dataset HSCT and Barcelona and it is of the same disease/field.
The 16S data was very similar to the The confirmation that the inteRmodel approach works on it it helped to continue forward.

The taxonomy of the different datasets was done differently.
After the first two datasets it was used the SILVA database to annotate the ASV.

Morgan's dataset is related to IBD but it is from a cohort of patients that underwent colectomy and samples are specific of the pouch or pre-pouch ileum, there are no healthy samples and there is no follow up.
It made very unlikely that a classification of the location could be achieved and the classification of the samples according to the microbiota could not be based on the disease (as all of them had undergone the same procedure with the same disease).
There seems to be gradation on the models on \@ref(fig:morgan-models) that could partially be explained by different degrees of inflammation.

Howell's dataset is very similar to our BARCELONA dataset but on pediatric cases.
It includes both CD and UC (and controls).
The time with the disease of these patients is much lower, and they have might followed less treatments and probably have not had any surgery

The Cristian's dataset was also imputed with SILVA but using a different database version.

### The methods

As seen on the introduction there are many methods are available.
There are frequent releases of new tools and methods.
The most up to date list of tools might be added to a [collaborative list](https://github.com/mikelove/awesome-multi-omics "Awesome multi-omics list").
Between them methods are very different in how they can be used and how they can be classified.
The quality of the software and methods also differs.
Some of them were tested several datasets but their most important validation process undergone is the mathematical validation they have.

With the increase of tools there have been increasingly important to compare different tools.
There are reviews of different tools on the same datasets [@cantini2020], some are more theoretical [ref:needed], others are focused on a different field [@cavill2016].

Of all these methods very few have been applied on IBD datasets.
Recently there has been an article focused on integration on IBD [@sudhakar2022].
On this publication the authors suggest that one must be mindful of the gap between the experimental conditions and the real world.
It also encourages to collect more data about the exposome (the stimuli patients have).
It ends up advising to set up guidelines for multi-omic studies tailored to the field, coordinate in a global framework to prevent redundant studies and ensure efficient funding and resources and disseminate training and education on computational approaches to analyze multi-omic datasets.

On IBD, many studies focus on finding some genes or bacteria to answer a narrow question they have in mind, like which bacteria is related to inflammation.
On this thesis, the focus was on finding a good representation of the disease that allowed to identify genes and bacteria that were relevant to the disease.
We made the assumptions that the microbiome and the transcriptome are related.
This assumption is backed up by several other previous studies supporting this relationship or others [@holmes2012; @brand2021; @cornish2008; @thomas1998] [ref:needed about relationship between microbiome and mucosa].

However, it is also known the role of other factors that are outside the omics dataset.
To guide the search of which of these variables are important tools that relate the variance of a block with other variables, both numeric and categoric, are needed.
On this case, `adonis` and `globaltest` worked well to identify which variables that were recorded were important.
Following, the previous article about being conscious of the gap, we could miss some other important variable.
However, even if we missed it, we probably haven't collected it or it is not available, there is nothing at this point that can be done.
This also enters in conflict with the general idea of data protection, that the minimum sufficient information should be collected.
In this case we might have miss a variable but to identify it we would need to collect too much information from the patients.

We tried to find which genes and bacteria are correlated between them using `WGCNA`.
It is a tool designed to find common patterns based on correlations requiring homogeneous samples, with a minimum of 12 samples required for condition.
When applied to the whole dataset there might bee too much variance in order of WGCNA to find the proper signal.
In addition, having microbiome and RNASeq on the same matrix, would probably be hard for the process to find good relationships if we applied the same normalization process to both of them without scalation.\
We could have tried to make smaller groups and then compare the modules between them but it groups would be too small because samples were from multiple segments and conditions.

We selected `MCIA` as a baseline to compare our method because it works well and has a good documentation as well as it is fairly used on the computational world.

We briefly considered using `STATegRa`.
But it is not possible to select which interactions between the blocks exists and by that time we already knew it was important to consider several of the variables collected from the patients.

`BaseSet` did not work for its intended usage on this thesis.
It failed because it is computationally expensive to calculate the likelihood of 1500 variables, there are too many combinations.
In addition, the numeric precision of said calculations suffers from the flotating point problems and must be considered carefully.
To support multiplying more than 1000 float numbers a different strategy such as using log values might be better.
We could not come up with a better strategy to find all the combinations needed, perhaps a better method exists that could be used to find which are the terms more influential to the end result.

Some batch effects are avoidable due to the experDesign package.
This might help to improve the quality of the datasets and to expand previously sequenced datasets.

## Designing models

Previous publications using `RGCCA` on IBD focused on validating inflammation markers genes DUOX2 and APOA1 as inflammation predictors ([@tang2017]) from previously published articles [@haberman2014].
Some publication tried to summarize the existing relationships on IBD [@hasler_uncoupling_2016].

Modeling the relationships on `RGCCA` the connections between blocks had to be modified.
Also what belongs to a block should be modified.
The features selected by `RGCCA` are highly dependent on which connections are made.

If one has preexisting theories about the data, a specific model can be used stating these known or hypothetical relationships.
However, if new relationships are being explored or no prior beliefs on the data are held the models should be created with random links between blocks, and evaluate which model is better.

The connections tested required that all models should be connected and no isolated blocks were left, all blocks should be indirectly connected to other blocks.
This avoids optimizing two different networks of blocks that are not connected between them.
Thus, forcing the model to represent all the information.

Typically blocks are defined

## Evaluating models

To evaluate a model `RGCCA` provides the average variance explained (AVE), inner and outer.
Inner AVE is for how well do all the canonical dimensions correlate with the design of the experiment, so it a measure of how good the model is.
While outer AVE is a measure of how well do the variables of each block correlate with the canonical dimension, so it measures the agreement between the variables and the canonical dimensions.
Depending on the goals of the research one or the other should be used.
If we are more interested on the model of the relationships the inner AVE makes more sense.

Furthermore, to evaluate a design bootstraping can be used to know how well the design does apply to a variety of data.
Another option is to use an external cohort to validate the same model, or using a different method to see if it finds the same relationships or explains the data as accurately.
Of the multiple methods available we used [MCIA](https://bioconductor.org/packages/omicade4 "Omicade4 package") [@mengMultivariateApproachIntegration2014].
Which was compared by looking at the area under the curve for classifying the samples according to their location.

Besides a way to compare methods, these models do need to be evaluated by the insights they provide on the biological system they are being applied to, in our case the Crohn's disease.
In this article we did not look in depth to the biological relevance of the microorganisms an genes found.

The procedure of separating independent variables in their own block of data and later search the best model that fits the data provides a good strategy that should be consider for integration efforts.

The procedural method of searching a model and testing them is implemented on [`inteRmodel`](https://llrs.github.io/inteRmodel/ "inteRmodel").
But the most important thing is to consider which variables are independent of which and if they can be separated into a block for later usage on the modeling.

Transciptome is related to localization of the samples [ref:needed].

Microbiome is more related to the disease and other variables of the patients.

Time is an important variable when modelling the disease, if multiple timepoints are taken they should be taken into account to identify the state of the disease for each patient.

[@jiang2019] A paper about multi-omics and microbiome...

[@krassowski2020] advice and recommendations on software engineering and reproducibility practices to share a comprehensive awareness with new researchers in multi-omics for end-to-end workflow.

## Relevance for other research

It is hard to provide more information that can be later used by researchers on the wet lab.

Paper about using transcriptomics to infer inflammation without colonoscopy [@ungar2022]

There is a disconnect between the computational side and the experimental side, driven by the difficulty to think an experiment to test the new information that multi-omic experiments provide.

Many combinations possible that are hard to explore, further research on how to reduce the space of possible combinations of bacteria or evaluate which combinations are more important might be useful on the future.
As this might help focus on the most promising bacteria.

## Implications of research

Translational research?
How to apply this results ?

Sequence just the ASV on patients to classify them on disease might be an option that would require further clinical validation.

Selecting which genes are related to the microorganisms is will require also further validation (for which I did not have the creativity to think of).

The several environmental factors highly affect the disease, so analysis or comparisons without taking into account them might provide misleading or false results.

Multi-omics seems to focus on metagenomics, metatranscriptomics and metaproteomics and abandon plain 16S sequencing [@zhang2019].

<!--# From @collij2021 add references/adapt-->

To study complex systems, new technologies like "organ-on-chip" or more specifically "gut-on-chip" are being developed [@collij2021].
By introducing gut microbiota, these system will be of great help in the stud of the interaction of the gut microbiota dn the intestinal epithelium.
